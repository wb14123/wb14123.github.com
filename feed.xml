<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://www.binwang.me/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.binwang.me/" rel="alternate" type="text/html" /><updated>2023-11-28T20:05:21-05:00</updated><id>https://www.binwang.me/feed.xml</id><title type="html">Bin Wang - My Personal Blog</title><subtitle>This is my personal blog about computer science, technology and my life.</subtitle><entry><title type="html">Introduce K3s, CephFS and MetalLB to My High Avaliable Cluster</title><link href="https://www.binwang.me/2023-11-28-Introduce-K3s-CephFS-and-MetalLB-to-My-High-Avaliable-Cluster.html" rel="alternate" type="text/html" title="Introduce K3s, CephFS and MetalLB to My High Avaliable Cluster" /><published>2023-11-28T00:00:00-05:00</published><updated>2023-11-28T00:00:00-05:00</updated><id>https://www.binwang.me/Introduce-K3s-CephFS-and-MetalLB-to-My-High-Avaliable-Cluster</id><content type="html" xml:base="https://www.binwang.me/2023-11-28-Introduce-K3s-CephFS-and-MetalLB-to-My-High-Avaliable-Cluster.html"><![CDATA[<p>In a previous blog <a href="/2023-03-13-Infrastructure-Setup-for-High-Availability.html">Infrastructure Setup for High Availability</a>, I talked about how I setup a cluster infrastructure for high availability applications. I have made a few changes since then. This blog is to talk about them in details.</p>

<h2 id="updated-architecture-overview">Updated Architecture Overview</h2>

<p><img src="/static/images/2023-11-28-Introduce-K3s-CephFS-and-MetalLB-to-My-High-Avaliable-Cluster/ha-cluster-infrastructure-k3s.png" alt="arch-diagram" /></p>

<p>Comparing the diagram with the one in <a href="/2023-03-13-Infrastructure-Setup-for-High-Availability.html">Infrastructure Setup for High Availability</a>, the overall structure remains the same, with a few modifications:</p>

<ul>
  <li>Not shown in the graph, but replaced official Kubernetes with K3s.</li>
  <li>Replaced GlusterFS with CephFS.</li>
  <li>Included cert-manager to get SSL certificates.</li>
  <li>Replaced Keepalived on each node with MetalLB.</li>
</ul>

<h2 id="replace-kubernetes-with-k3s">Replace Kubernetes with K3s</h2>

<p>I didn’t know <a href="https://k3s.io/">K3s</a> back when I setup my Kubernetes cluster for the first time. But since then I heard a lot of good things about it at various places. However, the complexity of migration and its installation method through a script from Internet instead of an OS package made me think twice before adopt it. But after I watched the video <a href="https://www.youtube.com/watch?v=k58WnbKmjdA">Talk About K3s Internals from Darren Shepherd</a>, I realized how simple k3s is compared to Kubernetes. I highly recommend everyone who is interested in K3s watch this video.</p>

<p>In short, K3s is a distribution of Kubernetes instead of a fork. It does these things with a few patches: combined the components of Kubernetes into one binary and process, and removed some components not needed in a bare metal environment. By doing so, it makes its binary size and memory footprint smaller than Kubernetes, and makes it easier to deploy and manage. It only needs a binary <code>k3s</code> and a configuration file under <code>/etc/rancher/k3s/config.yaml</code> to start, and all of its content is under <code>/var/lib/rancher/k3s</code>. The official install script adds a little bit more than just the binary file: it has a few scripts to kill and uninstall k3s. It also includes systemd file to start/stop k3s through systemd. So even though it’s not packaged into a standard OS package, I think the complexity is manageable so I started to experiment with it.</p>

<p>It’s very easy to config K3s since all it needs is a configuration file on each machine. I created a virtual machine cluster with Vagrant in the project <a href="https://github.com/wb14123/k3s-vm-cluster">k3s-vm-cluster</a> to experiment with it. Feel free to play with it to get a feel with it before go all in. The setup is based on the official guide for <a href="https://docs.k3s.io/datastore/ha-embedded">High Availability Embedded etcd</a>. It’s the easiest way to setup a high available K3s cluster.</p>

<p>No load balancer setup is needed if no external Kubernetes API server HA is needed. That means, you can access to Kubernetes API server within the cluster if any of the machine fails. But if you still want to access it outside of the cluster during a failure, check <a href="https://docs.k3s.io/datastore/cluster-loadbalancer">this doc</a>. Alternatively, I think load balancer like MentalLB can also do it, but I don’t need it so I didn’t experiment with it.</p>

<h2 id="distributed-storage-system-glusterfs-to-cephfs">Distributed Storage System: GlusterFS to CephFS</h2>

<p>The biggest motivation drives this migration is the deprecation of GlusterFS. I’m using distributed file system for a few use cases:</p>

<ul>
  <li>Configuration files: this can be migrated to <a href="https://kubernetes.io/docs/concepts/configuration/configmap/">Kubernetes ConfigMaps</a>.</li>
  <li>Logs: this can be migrated to a centralized log management system like ElasticSearch. But some of them like <a href="https://grafana.com/oss/loki/">Loki</a> in turn depends on another distributed storage.</li>
  <li>Data files: this is most complex one. Some of the services support saving files into S3 compatible systems. But some of them don’t. (I cannot control the services since I only self host them instead of developed them). One option is to not having HA and just bind those services into a specific host and use local storage.</li>
  <li>Docker registry: this belongs to the point “Data files” above, but this is very import so I separate into another point. I’m using <a href="https://www.sonatype.com/products/sonatype-nexus-repository">Sonatype Nexus</a> as the docker registry. It supports to put packages into S3 but still pretty tricky to get rid of all the local files. This is a service that absolutely needs HA if I want to have a HA cluster. Or I can change to another Docker registry implementation, but I feel pretty comfortable using it so I don’t want to change it.</li>
</ul>

<p>So it basically comes down to these 2 options:</p>

<ol>
  <li>Use a S3 compatible storage like <a href="https://min.io/">MinIO</a> but do a lot of work to configure services to store files into that, and make services cannot do that not HA anymore.</li>
  <li>Go ahead and uses a real distributed file system like CephFs or <a href="https://longhorn.io/">Longhorn</a>.</li>
</ol>

<p>Option 1 sounds appealing to me at first since I really don’t want to deal with the complexity of setting up CephFS. But as I go into the rabbit hole, I found configuring the services to use S3 may be a more complex process and less portable than just setup CephFS. So at the end I decide to go option 2.</p>

<p>I’ve heard of CephFS long time ago but decided to use GlusterFS at previous setups because of the level of user friendly. So CephFS seems like a nature choice after GlusterFs is deprecated. Especially when I found other than the distributed block device, it also supports file system and S3 compatible storage system. It’s also easier to install than before because of <a href="https://rook.io/">Rook</a>. Longhorn is another choice I looked a little bit but because of wider adoption of CephFS and more features of it, I decide to use CephFS at the end.</p>

<p>The way I use it is mainly <a href="https://rook.io/docs/rook/v1.11/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage/">Ceph Filesystem</a>, so it’s easier to share volumes between pods. Again, the project <a href="https://github.com/wb14123/k3s-vm-cluster">k3s-vm-cluster</a> has an example about it. Try to play it if you are interested in it. Along the way I actually contributed to Rook project by improving doc (<a href="https://github.com/rook/rook/pull/13045">#13045</a>) and its error message (<a href="https://github.com/rook/rook/pull/13046">#13046</a>).</p>

<h2 id="network-gateway">Network Gateway</h2>

<p>In the previous article, I talked about using Cloudflare tunnel, or NodePort and Keepalived to expose services to the Internet. But there are some other things a network gateway can do other than just expose the service: it can also do things like terminate SSL encryption and so on. Cloudflare tunnel support terminate SSL at their end so I don’t need to worry about that. But for some services, I don’t want Cloudflare to see the traffic, so I need to terminate SSL and expose service by myself.</p>

<p>As I said, expose service part was done by NodePort and Keepalived, which is not very elegant but works. For the terminate SSL part, I was using Nginx as reverse proxy. But updating SSL certificates is a little bit more complex. I don’t want to talk it in details here because the setup is pretty complex and explaining it will be very lengthy. The point is, with this migration, I want to revisit this part to make it simpler and more elegant.</p>

<p>Kubernetes has a concept of <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress</a>, and newer but less mature, <a href="https://gateway-api.sigs.k8s.io/">Gateway</a>. What they are doing is essentially reverse proxy like Nginx. In fact, Nginx Ingress is a thing. The advantage is that you don’t need to configure all the services in a single place like Nginx’s configuration files. You can create Kubernetes resources for each of the service. So that the deployment and configuration of each service is totally self contained. This is a very good feature, especially for a company: when I first started to use Kubernetes at 2015 in a previous company, I felt the pain of not having it. But the feature of Ingress is pretty limited. For example, it can only bind to 443. It cannot modify the http content, and so on. So that I may still need a layer of Nginx for my use cases. The design of gateway is too complex and the features don’t really meet all my requirements as well.</p>

<p>There are some players like <a href="https://traefik.io/">Traefik</a>(shipped with K3s by default) and <a href="https://istio.io/">Istio</a> which overcome the limitations by having their own custom resources. But Traefik cannot get new certificates from Let’s Encrypt with a HA setup. Istio is just too complex and include features like service mesh that I don’t need. I can see how service mesh can be useful in big companies, but I prefer not to have another layer on my own service. At the end, I don’t think the complexity worth it.</p>

<p>But while I exploring Traefik and Istio, I found <a href="https://cert-manager.io/">cert-manager</a>, which can be deployed into Kubernetes. It can get certificates from Let’s Encrypt and put them into Kubernetes secrets, which then can be mount into each pods. It supports Cloudflare DNS API for <a href="https://letsencrypt.org/docs/challenge-types/#dns-01-challenge">ACME DNS challenge</a>, so I don’t need to export a http service for Let’s Encrypt to verify the ownership of the domain name. With all of this features, I decided to use it and mount the certificates into Nginx pods. It resolves the problem of update certificates from Let’s Encrypt.</p>

<p>For the other problem of exposing the services to Internet in a HA way, I want to use a more Kubernetes native way instead of setup Keepalived outside of the Kubernetes cluster. Kubernetes supports <a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/">external load balancers</a>. But most of the load balancers it supports are from cloud. Then I found <a href="https://metallb.org/">MetalLB</a>, which supports creating a HA load balancer without special hardware in a bare metal cluster. I use it with <a href="https://metallb.org/concepts/layer2/">layer 2 mode</a>, which creates a virtual IP like keepalived and can failover to another node.</p>

<h2 id="deploy-services-with-code">Deploy Services with Code</h2>

<p>What I didn’t talk in the previous blog is, I define the deployment of my services as code instead yaml files. It gives lots of advantages: first, you can create models for your own deployment pattern so that you can avoid lots of redundant code. Traditionally it’s hard to define the deployment as code. There are lots of frameworks to do it but none of them is easy to use. But with Kubernetes, all you need is generating a resource object for Kubernetes to use at the end. You can construct it in any way with your favorite language, and either output a YAML or call Kubernetes API directly. It’s using a high level language instead of writing machine code directly. It’s much more elegant and the maintenance is much easier. Be aware: use a real language instead of some template language. Why limit your power to do things?</p>

<p>This approach works so well especially during this migration. For example, I abstracted all the storage layer for my services, so that when I migrated from GlusterFS to CephFS, I just need to change the storage class to define the CephFS volume, and the code for services don’t need to change much.</p>

<p>Hope you enjoy my experience of setting up a HA cluster. Happy hacking and have fun with your own cluster!</p>]]></content><author><name></name></author><category term="Kubernetes" /><category term="CephFS" /><category term="MetalLB" /><category term="k3s" /><category term="Infrastructure" /><category term="devops" /><summary type="html"><![CDATA[In a previous blog Infrastructure Setup for High Availability, I talked about how I setup a cluster infrastructure for high availability applications. I have made a few changes since then. This blog is to talk about them in details.]]></summary></entry><entry><title type="html">Update on RSS Brain to Find Related Articles with Machine Learning</title><link href="https://www.binwang.me/2023-11-14-Update-On-RSS-Brain-to-Find-Related-Articles-with-Machine-Learning.html" rel="alternate" type="text/html" title="Update on RSS Brain to Find Related Articles with Machine Learning" /><published>2023-11-14T00:00:00-05:00</published><updated>2023-11-14T00:00:00-05:00</updated><id>https://www.binwang.me/Update-On-RSS-Brain-to-Find-Related-Articles-with-Machine-Learning</id><content type="html" xml:base="https://www.binwang.me/2023-11-14-Update-On-RSS-Brain-to-Find-Related-Articles-with-Machine-Learning.html"><![CDATA[<p>In the previous blog about RSS, <a href="/2022-12-03-How-RSS-Brain-Shows-Related-Articles.html">How RSS Brain Shows Related Articles</a>, I talked about how RSS Brain finds the related articles. I’ve updated the algorithm recently. This blog is about the details about the update. The basic idea is to replace tf-idf algorithm with text embeddings to represent the articles as vectors, and use ElastcSearch to store and query those vectors.</p>

<h2 id="the-disadvantages-of-previous-algorithm">The Disadvantages of Previous Algorithm</h2>

<p>First let’s do a quick revisit on the algorithm before the update: it’s using tf-idf algorithm. Which is basically an algorithm to represent each document as a vector by using the words’ frequency in it. It’s an algorithm that is easy to understand, and works well enough in practice to power lots of searching engines for a long time. However, it has a few shortcomings:</p>

<p>First, it doesn’t understand the meaning of the word. A word can mean different things based on context, order, combinations and so on. Different words can also have the same meaning. Word frequency along doesn’t catch that.</p>

<p>Second, “word” needs to be defined. Which is a relatively easy task for languages like English, since it has a built-in word separator character (space). However, for languages like Chinese, there is no obvious way to separate the words. The performance of tf-idf algorithm largely depends on the performance of word separating algorithm, which itself is much more complex than tf-idf and often involves machine learning as well. Even for languages like English, in order to minimize the first disadvantage above, the words are usually broke down so that some similar words can be matched.</p>

<p>Last, which is an extension of the first disadvantage: it’s hard to do multi language matches. Word frequency along doesn’t know that different words in different languages can mean the same thing. Of course you can translate the document to other languages and index the translated documents, but it doesn’t scale well when you need to support more and more languages. And translation algorithms are usually much more complex than tf-idf, and mostly use machine learning too.</p>

<h2 id="word-and-document-embeddings">Word and Document Embeddings</h2>

<p>With the advancement of machine learning, a new method to represent words as vectors has been developed in the paper <a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a>. The vector is called word embedding. Then based on the idea, <a href="https://arxiv.org/abs/1405.4053">Distributed Representations of Sentences and Documents</a> explores representing paragraphs as a vectors. Without go into the details, the basic idea is to get a layer from neural network for a NLP task.</p>

<p>For example, if we have a neural network to predict the nth word given previous words, then we may have a neural network like this:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>word[1]   --&gt; vector[1]
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>word[2]   --&gt; vector[2]    --&gt; layer2 --&gt; ... -&gt; classifier -&gt; output
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>...
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>word[n-1] --&gt; vector[n-1]
</pre></div>
</div>
</div>

<p>Words are mapped to vectors at the first layer, with something like</p>

\[v = w * W + b\]

<p>Which \(v\) is the vector, \(w\) is the one-hot encoded word. And matrix \(W\) and \(b\) is the trained parameters. There are many other parameters in the later layers of the neural network but we don’t care. We only take \(W\) and \(b\) so that we can compute the vector for any word. With this method, the represented vectors can measure similarities between words by computing similarity of the vectors. Also surprisingly, quoted from the paper <a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a>: “To find a word that is similar to small in the same sense as biggest is similar to big, we can simply compute vector \(X = vector(biggest) − vector(big) + vector(small)\).” What a beautiful result!</p>

<p>I was aware of this research not long after it came out. I believe some commercial search engines started to use it since then. But the ecosystem like models, tools, databases really picked up since GPT3 came out. So recently, I decided to use it in RSS Brain because how easy to do it nowadays.</p>

<h2 id="select-a-model-to-use">Select a Model to Use</h2>

<p>The first step is to select a model to use. I think OpenAI may have the best model that is available to public. You cannot access the real model but there are APIs you can call to use the model. But I don’t like it for 2 reasons: First, I don’t like OpenAI as a company: it presents itself as a non-profit organization first with the goal to make AI accessible to everyone, then stopped publish models or even the algorithm details. Second, I don’t want vendor lock-in.</p>

<p>There is also Llama. But it’s not really a multilingual model. I see some attempts to train it on some other languages, but the result are not that good in my experience. The license of the model is not commercial friendly as well. And there is no easy to use API to get the embeddings.</p>

<p>At the end I found <a href="https://www.sbert.net/index.html">SentenceTransformers</a>. There are lots of <a href="https://www.sbert.net/docs/pretrained_models.html in the project">pretrained models</a>. After all I selected the model <code>paraphrase-multilingual-mpnet-base-v2</code> since it’s a multilingual model. But it’s called “sentence” transformers for a reason: there is a size limit on the length of document that you can feed in to the models. I ended up to just get the embeddings for the article title. I think it’s a good enough for my use case.</p>

<h2 id="implementation-details-for-model-server">Implementation Details for Model Server</h2>

<p>The library SentenceTransformer is very easy to use. However it’s implemented in Python so it needs a way to communicate with RSS Brain server, which is written in Scala. Since this is a computation heavy task, the first though is to have a buffer queue in between so that the Python program can process the articles in a speed it can handle. Kafka is a good choice for external task queue but I don’t think it worth the complexity to import another component into the system. So I created buffer queue at both end to avoid creating too many requests while maintain some parallelism. Here is what the whole architecture looks like:</p>

<p><img src="/static/images/2023-11-14-Update-On-RSS-Brain-to-Find-Related-Articles-with-Machine-Learning/article-embedding-arch.png" alt="embedding-arch" /></p>

<p>The green parts in the diagram means the workers in them can work concurrently. On the Scala side, it follows the pattern I experimented in <a href="/2023-08-27-Compare-Task-Processing-Approaches-in-Scala.html">Compare Task Processing Approaches in Scala</a>. On the Python side, it’s more tricky since Python’s async handling is far worth than Scala’s plain old Future, not to mention effect systems like Cats Effect. I may write another blog in the future about it.</p>

<p>The reason I go great detail into this relatively simple problem is that it represents a category of problems: problems that need Python to do some async work because of the library supports. For example, in the future, Python server may have more features like fetching Youtube transcriptions. The architecture to integrate it into RSS Brain would be the same.</p>

<h2 id="database-to-store-and-query-embeddings">Database to Store and Query Embeddings</h2>

<p>There are a few vector databases that can store vectors and query nearest vectors if given one. ElasticSearch added vector fields support at 7.0 and approximate nearest neighbor search (ANN) at 8.0. Since RSS Brain is already using ElasticSearch heavily for searching, I can just use it without add another database into the dependency. It also supports machine learning models so that you don’t need to insert the embedding vectors from the outside world, but I find it’s not as flexible.</p>

<p>Once the vectors are inserted into ElastiSearch, it’s just an API call to get the most similar documents. The details of vector insert and query are in the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html">ElasticSearch KNN search document</a>. One tricky part is that even though ElasticSearch supports <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html#_combine_approximate_knn_with_other_features">combining ANN search with other features like term searches (tf-idf algorithm)</a> by using a boost factor, it doesn’t work well unless you are willing to tune it. That’s because the embedding vector and term vector mean different things, and the similarity score is not really comparable. So I ended up enable vector search only for finding related articles, instead of combining with term searches.</p>

<h2 id="result">Result</h2>

<p>It’s actually hard to have some metrics for the performance of finding related articles. I don’t believe metrics like click rate, since it doesn’t necessarily show the articles are related. I think the only way for me is to review the results manually and compute the score based on it. But I don’t think it has much value since supporting multiple language along would make it much better than the previous algorithm. But if you are using RSS Brain, you can see the results yourself and let me know what you think about the new algorithm!</p>]]></content><author><name></name></author><category term="RSS Brain" /><category term="Machine Learning" /><category term="Nerual Network" /><category term="Embeddings" /><category term="Python" /><summary type="html"><![CDATA[In the previous blog about RSS, How RSS Brain Shows Related Articles, I talked about how RSS Brain finds the related articles. I’ve updated the algorithm recently. This blog is about the details about the update. The basic idea is to replace tf-idf algorithm with text embeddings to represent the articles as vectors, and use ElastcSearch to store and query those vectors.]]></summary></entry><entry><title type="html">Add Index Sidebar to My Blog</title><link href="https://www.binwang.me/2023-11-10-Index-Sidebar-on-My-Blog.html" rel="alternate" type="text/html" title="Add Index Sidebar to My Blog" /><published>2023-11-10T00:00:00-05:00</published><updated>2023-11-10T00:00:00-05:00</updated><id>https://www.binwang.me/Index-Sidebar-on-My-Blog</id><content type="html" xml:base="https://www.binwang.me/2023-11-10-Index-Sidebar-on-My-Blog.html"><![CDATA[<p>In a previous blog <a href="/2021-10-31-Add-Index-to-My-Blog.html">Add Index to My Blog</a>, I talked about how I added an index page to my blog that put all the articles into categories. I always wanted the index to be a sidebar instead of a single page, but I guess I didn’t wrap my head around about how to implement so I gave up at last. But recently, when I started to use <a href="https://obsidian.md/">Obsidian</a> and checked some demos of <a href="https://obsidian.md/publish">Obsidian Publish</a>, I found having a sidebar is so useful and beautiful so I decide I should implement it.</p>

<p>You can see the result right now: if you are on a big screen device, the index is on the left side of the page. If you are on a small screen device like a mobile phone, it will show a menu button at the top left corner instead. Clicking it will take you to the index.</p>

<p>When I implement it, I want to keep it simple and stupid. That means:</p>

<ul>
  <li>I want to be as simple as possible as long as it has the function: show articles in nested categories.</li>
  <li>I want to use as little Javascript as possible so people can still use it with Javascript disabled.</li>
</ul>

<p>I found the design of Obsidian Publish is very good. So I copied lots of details from them with some modifications: I didn’t implement showing/hiding sub items when click on the index entry since I think it’s not necessary, and I like how it looks when all the articles are listed there: feels like I’ve written lots of things. The categories are sorted by alphabet order and the posts are ordered by publish date. I also added the publish year for each article entry: some articles can look outdated but if people noticed the published year they can understand the context.</p>

<p>Since I’m using Jeykyll, I can generate plain HTML when possible to avoid the usage of Javascript. So the sidebar is generated for each page instead of using Javascript to keep the sidebar and replace the article content on the fly. Javascript is only used for 2 features:</p>

<ol>
  <li>Remember the position of the sidebar when jump pages.</li>
  <li>Scroll the sidebar to show the entry for the current page if it’s not in the viewpoint.</li>
</ol>

<p>Both of the features are not that important so the sidebar is still usable without Javascript. Even for the menu button on small screens, it’s not popping up a dialog. It just jumps to a new static page that has all the index so no Javascript is needed.</p>

<p>The previous implementation of the index page uses recursive templates: Since the nested index is a tree, rendering the content in a recursive manner is a nature thought. However, I made that mistake to put the complex logic into the template engine. So this time, I traverse the tree with Ruby code and generates a list for the template to render. It has all the information like entry type, the depth of the entry and so on. It makes the template code much simpler so it’s easier to implement other features on top of it.</p>

<p>If you want to checkout the detailed implementation, go to my <a href="https://github.com/wb14123/blog">Github repo for the blog</a> and check <a href="https://github.com/wb14123/blog/blob/master/jekyll/_plugins/Index.rb"><code>jekyll/_plugins/Index.rb</code></a> and <a href="https://github.com/wb14123/blog/blob/master/jekyll/_includes/index_menu.html"><code>jekyll/_includes/index_menu.html</code></a>.</p>]]></content><author><name></name></author><category term="blog" /><category term="Jekyll" /><category term="Javascript" /><category term="desgin" /><summary type="html"><![CDATA[In a previous blog Add Index to My Blog, I talked about how I added an index page to my blog that put all the articles into categories. I always wanted the index to be a sidebar instead of a single page, but I guess I didn’t wrap my head around about how to implement so I gave up at last. But recently, when I started to use Obsidian and checked some demos of Obsidian Publish, I found having a sidebar is so useful and beautiful so I decide I should implement it.]]></summary></entry><entry><title type="html">How to Cleanup Ceph Filesystem for Deleted Kubernetes Persistent Volume</title><link href="https://www.binwang.me/2023-11-04-How-to-Cleanup-Ceph-Filesystem-for-Deleted-Kubernetes-Persistent-Volume.html" rel="alternate" type="text/html" title="How to Cleanup Ceph Filesystem for Deleted Kubernetes Persistent Volume" /><published>2023-11-04T00:00:00-04:00</published><updated>2023-11-04T00:00:00-04:00</updated><id>https://www.binwang.me/How-to-Cleanup-Ceph-Filesystem-for-Deleted-Kubernetes-Persistent-Volume</id><content type="html" xml:base="https://www.binwang.me/2023-11-04-How-to-Cleanup-Ceph-Filesystem-for-Deleted-Kubernetes-Persistent-Volume.html"><![CDATA[<p><a href="https://docs.ceph.com">Ceph</a> is a distributed file system. <a href="https://rook.io/">Rook</a> is a project to deploy it with Kubernetes. I recently replaced GlusterFS in my Kubernetes cluster with Ceph. I will write a blog (or a series of blogs) for the migration. But in this article, I will just talk about a problem I encountered, just in case I forget it.</p>

<p>Once Rook is deployed in Kubernetes, you can create a <a href="https://rook.io/docs/rook/v1.11/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage/">Ceph Filesystem</a> and use it to <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes">persistent volume (PV)</a>. Each PV’s data will be stored in a folder in the filesystem. If the PV’s reclaiming policy is set to <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retain">retain</a>, the data will not be deleted after the persistent volume is manually deleted. It’s safer in this way. But what could you do if you want to cleanup the data? Normally you should change the PV’s reclaim policy before you delete the PV, then Rook’s operator will auto reclaim the storage in Ceph. But what if you forget or didn’t know that (like me), and want to cleanup the data after?</p>

<p>First, we need to the folder/subvolume names in Ceph that store’s each PV’s data. We an get that by using <code>kubectl describe pv &lt;pv-name&gt;</code> and look for the field <code>subvolumeName</code>. But since the PV is deleted, we need to find the mappings for existing PVs and compare that with the folders/subvolumes in Ceph. This is the command to show all of the existing ones:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>kubectl get pv -o yaml | grep subvolumeName  | sort
</pre></div>
</div>
</div>

<p>Then we need to find all the existing folders/subvolumes in Ceph’s filesystem: Start a Ceph toolbox pod based on the <a href="https://rook.github.io/docs/rook/v1.11/Troubleshooting/ceph-toolbox/?h=toolbox">doc</a>. Then go into the pod and find the filesystem’s name first:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>ceph fs ls
</pre></div>
</div>
</div>

<p>After getting the filesystem’s name, get all the subvolumegroup from it:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>ceph fs subvolume ls &lt;fs-name&gt; csi | grep 'name' | sort
</pre></div>
</div>
</div>

<p>Compare this list with the list above, you should be able to find a subvolume that exists in Ceph but not shown in Kubernetes’ PV mapping. Use this command to check its info:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>ceph fs subvolume info &lt;fs-name&gt; &lt;subvolume-name&gt; csi
</pre></div>
</div>
</div>

<p>If you are sure this is the folder you want to delete, use this command to delete it:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>ceph fs subvolume rm &lt;fs-name&gt; &lt;subvolume-name&gt; csi
</pre></div>
</div>
</div>]]></content><author><name></name></author><category term="Kubernetes" /><category term="Ceph" /><category term="Distributed file system" /><summary type="html"><![CDATA[Ceph is a distributed file system. Rook is a project to deploy it with Kubernetes. I recently replaced GlusterFS in my Kubernetes cluster with Ceph. I will write a blog (or a series of blogs) for the migration. But in this article, I will just talk about a problem I encountered, just in case I forget it.]]></summary></entry><entry><title type="html">Linux Full Disk Encryption with Yubikey</title><link href="https://www.binwang.me/2023-10-22-Full-Disk-Encryption-with-Yubikey.html" rel="alternate" type="text/html" title="Linux Full Disk Encryption with Yubikey" /><published>2023-10-22T00:00:00-04:00</published><updated>2023-10-22T00:00:00-04:00</updated><id>https://www.binwang.me/Full-Disk-Encryption-with-Yubikey</id><content type="html" xml:base="https://www.binwang.me/2023-10-22-Full-Disk-Encryption-with-Yubikey.html"><![CDATA[<h2 id="background">Background</h2>

<p>As mentioned in a previous blog <a href="/2023-03-13-Infrastructure-Setup-for-High-Availability.html">Infrastructure Setup for High Availability</a>, I’ve setup a high available cluster that has 3 machines. But one of them is on my laptop. I feel like I need a dedicated machine for my personal usage, especially I’m planning some travels. So I need to remove the laptop from the cluster. Its disk space is also very limited. With migrating Gluster to Ceph (more blogs to come on that) and not be able to use a disk partition with Ceph’s encryption, I need another machine with more disks. So I repurposed another small form factor machine to join the cluster.</p>

<p>I want full disk encryption on it but I don’t want to input password every time it boots: this machine is put into a closet and it’s very inconvenient to plug/unplug keyboard and monitor. In another blog <a href="/2021-09-19-Personal-ZFS-Offsite-Online-Backup-Solution.html">Personal ZFS Offsite Backup Solution</a>, I talked about a solution to boot encrypted Linxu without input password by setting up TPM. However, old machines only have TPM 1.x chips instead of newer TPM 2.0 chips, which is very tricky to setup and  with very limited support from Linux distros. I don’t want to do it again if not necessary. The thread model is also different since this machine is supposed to be at my home all the time. So this time, I found a new solution to use Yubikey to decrypt disks: I just need to keep Yubikey plugged in during the boot process and press it at the proper time. I can also fallback to the password method if there is anything wrong with Yubikey decryption.</p>

<p>There are some great tutorials and wiki pages describe how to do it. I must give the credit to <a href="https://0pointer.net/blog/unlocking-luks2-volumes-with-tpm2-fido2-pkcs11-security-hardware-on-systemd-248.html">this article</a> that helped me a lot. But all of them are missing some details so I though it would be great to write down my setup so that it may help someone else. My setup is on Arch Linux but the steps should be portable to other Linux distros.</p>

<p><strong>Warning: the steps may make your system not be able to boot if not setup properly. Make sure to back it up or have a recovery CD available to fix it if things went south.</strong></p>

<h2 id="install-linux-with-luks2">Install Linux with LUKS2</h2>

<p>First we need to install Linux with our root partition encrypted. If you are using an installer, most likely there is an option to encryption the disk. If so, select that option and input a passphrase for it. Even though we are using Yubikey to decrypt the disks, it’s always good to have a passphrase to decrypt it in case something goes wrong. However, if your threat model needs a solution that doesn’t involve a passphrase, I believe you can remove it later after setup Yubikey, though I’ve never tried it myself.</p>

<p>Some installers will use LUKS1 instead of LUKS2 to encrypt the disk. Don’t worry, use <code>cryptsetup convert --type=LUKS2 &lt;device&gt;</code> to convert it to a LUKS2 setup after the OS is installed.</p>

<p>Note: do not encrypt boot partition. It usually doesn’t have sensitive information and encrypting it doesn’t prevent evil maid attack anyway. If you want it to be more secure, considering setup secure boot, which is also mentioned in my previous blog <a href="/2021-09-19-Personal-ZFS-Offsite-Online-Backup-Solution.html">Personal ZFS Offsite Backup Solution</a>.</p>

<h2 id="enroll-yubikey-to-key-slot">Enroll Yubikey to Key Slot</h2>

<p>We can enroll a FIDO2 (which is a protocol Yubikey supports) device by using <code>systemd-cryptenroll</code>.</p>

<p>Plug in the Yubikey. You can use this command first to list all the FIDO2 devices to make sure the Yubikey is recognized:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>systemd-cryptenroll --fido2-device=auto list
</pre></div>
</div>
</div>

<p>Note: You may need to install <code>libfido2</code>.</p>

<p>After confirm the Yubikey is recognized, use this command to enroll it:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>systemd-cryptenroll --fido2-device=auto &lt;disk-device&gt;
</pre></div>
</div>
</div>

<p>It will show hint about you may need to press the Yubikey during the process. So <strong>watch the Yubikey: when its LED flashes, press it to continue.</strong></p>

<h2 id="setup-crypttab">Setup crypttab</h2>

<p>Put a line like this into <code>/etc/crypttab.initramfs</code>. It will be copied to initramfs by mkinitcpio as <code>/etc/crypttab</code> so that your root partition can be decrypted before it is mounted:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>myvolume &lt;disk-device&gt; - fido2-device=auto
</pre></div>
</div>
</div>

<p><code>&lt;disk-device&gt;</code> can be something like <code>/dev/sda1</code> or using UUID format <code>UUID=&lt;disk-uuid&gt;</code>.</p>

<p>If it’s not root partition, you can put it in <code>/etc/crypttab</code> so it will be used after root partition is mounted.</p>

<h2 id="setup-mkinitcpio">Setup mkinitcpio</h2>

<p><code>mkinitcpio</code> is a tool to generate initramfs. <code>/etc/crypttab.initramfs</code> only works with it. So if your distro comes with other tools like <code>dracut</code>, you may need to uninstall it and install <code>mkinitcpio</code> instead.</p>

<p>Once making sure <code>mkinitcpio</code> is installed, we need to configure the hooks to make it read <code>crypttab</code> to decrypt the disks. We also need to make sure we are using systemd init instead of busybox init.</p>

<p>Open <code>/etc/mkinitcpio.conf</code>, and find the line with <code>HOOKS=(...)</code>. Refer to <a href="https://wiki.archlinux.org/title/Mkinitcpio#Common_hooks">this wiki page</a> about the common hooks and replace busybox hooks with systemd ones. For example, in my setup, I replaced <code>udev</code> with <code>systemd</code> and <code>keymap</code> with <code>sd-vconsole</code>. Then add <code>sd-encrypt</code> to the hooks. The order matters: usually it comes after <code>sd-vconsole</code>.</p>

<p>Then use <code>mkinitcpio -P</code> to regenerate initramfs images.</p>

<h2 id="test-and-finish">Test and Finish!</h2>

<p>Okay, now we have already setup everything. We can boot the system and test. Make sure Yubikey is plugged in before the boot. And watch for its LED light to flash and press it when it does! This little detail spent me lots of time to figure it out.</p>

<p>You can also use password to decrypt the disk <strong>without Yubikey plugged in</strong>. Wait it for 30 seconds and it will prompt you to input the password. The time can be configured in <code>/etc/crypttab</code> (or <code>/etc/crypttab.initramfs</code>) by setting up <a href="https://man.archlinux.org/man/crypttab.5">token-timeout=</a>.</p>]]></content><author><name></name></author><category term="Yubikey" /><category term="Linux" /><category term="Encryption" /><summary type="html"><![CDATA[Background]]></summary></entry><entry><title type="html">A Boring JVM Memory Profiling Story</title><link href="https://www.binwang.me/2023-09-30-A-Boring-JVM-Memory-Profiling-Story.html" rel="alternate" type="text/html" title="A Boring JVM Memory Profiling Story" /><published>2023-09-30T00:00:00-04:00</published><updated>2023-09-30T00:00:00-04:00</updated><id>https://www.binwang.me/A-Boring-JVM-Memory-Profiling-Story</id><content type="html" xml:base="https://www.binwang.me/2023-09-30-A-Boring-JVM-Memory-Profiling-Story.html"><![CDATA[<h2 id="background-of-memory-leakings">Background of Memory Leakings</h2>

<p>I encountered another memory leak problem recently. I’ve debugged a few of memory leak problems in the past, including <a href="https://github.com/splicemachine/spliceengine/pull/2260">the one</a> in Splice Machine, an open source distributed SQL engine based on HBase but was sadly discontinued. The memory leak problems are interesting because it’s challenging to find the root cause. However, I’ve never written a blog about it. Memory leak problems are not so usual, so when I encountered a new one, I kind of need to remember what tools I’ve used. So this time, even though not as interesting as some other memory leak problems I’ve debugged in the past, I decide to write it down as a note for my own reference in the future. The tool set I used this time is relatively simple. I guess I can write more when I use others in the future. This is more like a dev log instead of a tutorial. The “boring” in the title means it’s a pretty standard process and the problem is not that hard to find this time.</p>

<p>Most of the memory leak bugs are very easy to fix once found the root cause, but the part of finding the root cause is tricky. First of all, it’s hard to reproduce: sometime it only happens on production environment. Without knowing the cause, it’s hard to reproduce locally. Even it can be reproduced consistently, it may take some time to let the memory accumulate so the debugging loop can be time consuming sometimes. Last of all, unlike some other bugs that you have an exception and a nice stack trace to help you identify which code causes the problem, it’s almost impossible to find the root cause without the help of a profiler, which itself has challenging parts depending on the platform.</p>

<p>Luckily, JVM has good profilers. That’s one of the reasons Scala, a JVM based language, is my favorite language. (The criteria of a good production language for me is not only the language itself, but also the ecosystem like library, IDE and profilers. JVM based language makes lots of the criteria easy to meet.) This time I uses a very popular profiler <a href="https://www.ej-technologies.com/products/jprofiler/overview.html">JProfiler</a>. Other popular choices that I have used are <a href="https://visualvm.github.io/">VisulaVM</a> and <a href="https://www.oracle.com/java/technologies/jdk-mission-control.html">Java Msission Control</a>. But I found JProfiler is both powerful and easy to use. The only downside is you need to buy a license. It has free trail and open source license. So if you have an open source project or just need to use it for a few days, you can still use it for free.</p>

<h2 id="the-problem">The Problem</h2>

<p>Okay, enough of the background. Let’s dive into the memory leaking problem I encountered this time. As mentioned in the previous blog <a href="http://localhost:4001/2023-09-23-Migrate-Scala2Grpc-to-Cats-Effect-3.html">Migrate Scala2grpc to Cats Effect 3</a>, I migrated one of my side projects to Cats Effect 3 as well, with a lot of other dependencies. This side project is <a href="https://www.rssbrain.com/">RSS Brain</a>. There are two parts on the backend: one for serving client requests with gRPC and gRPC web, another one for fetching RSS feeds. The fetcher gets the RSS feeds that haven’t been fetched for a while from the database with the help of <a href="https://tpolecat.github.io/doobie/">doobie</a> and <a href="https://zio.dev/zio-quill/">quill</a>, and fetch them in parallel with the help of fs2 stream and Cats Effect.</p>

<p>After the mass upgrades, I looked into the metrics to make sure everything is okay. Then I found the fetcher’s memory starts to increase slowly. Looks like a memory leak problem to me. Here is the memory usage graph:</p>

<p><img src="/static/images/2023-09-30-A-Boring-JVM-Memory-Profiling-Story/memory-usage.png" alt="memory usage" /></p>

<p>Seems eventually the JVM will run out of memory but I didn’t wait for it. It’s good to try if force a full gc will reclaim the memory or not, in my case full gc doesn’t help much.</p>

<p>Another metrics to look at is the GC metrics. Only after I shipped the fix, I realized the GC didn’t look normal when there was this memory problem:</p>

<p><img src="/static/images/2023-09-30-A-Boring-JVM-Memory-Profiling-Story/gc.png" alt="gc" /></p>

<p>Before the blue line starts, GCs are all “Copy” and “MarkSweepCompact”, which means the memory are mostly being moved around instead of reclaimed. After the blue line starts, which was when the fix was shipped, we starts to see normal young and old generation GC.</p>

<p>So these metrics indicates that we may have a memory leak issue. Let’s starts to debug it.</p>

<h2 id="setup-profiler">Setup Profiler</h2>

<p>In this case I’m using JProfiler. But as I mentioned above, VisualVM or Java Mission Control should also be able to do the job.</p>

<p>JProfiler has a nice wizard to let you setup the profiler. In my case, since I run the service in Kubernetes, I need to select remote server profiling and go through the wizard. We are going to use <code>kubectl</code> to forward the debugging port to local, so that we can just use <code>localhost:8849</code> as the remote address. At the end of the setup wizard, it will prompt you to download the profiler agent and include it with a Java command line argument. Since the service is running in container, I added the following lines to the Dockerfile in order to include the agent in it:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>RUN apt update -y &amp;&amp; apt install -y wget
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>RUN cd /opt &amp;&amp; \
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>        wget -c 'https://download.ej-technologies.com/jprofiler/jprofiler_agent_linux-x86_14_0.tar.gz' &amp;&amp; \
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>        tar -xf jprofiler_agent_linux-x86_14_0.tar.gz
</pre></div>
</div>
</div>

<p>Also add this flag to Java command line when starting the service:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>-agentpath:/opt/jprofiler14/bin/linux-x64/libjprofilerti.so=port=8849,nowait
</pre></div>
</div>
</div>

<p>After the new container is deployed, we can port forward 8849 from the service to our localhost with kubectl:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>kubectl port-forward &lt;service-pod-name&gt; 8849:8849
</pre></div>
</div>
</div>

<h2 id="memory-comparison">Memory Comparison</h2>

<p>Since it’s a memory leaking problem, we want to find out what objects are increasing. First let’s restart the JVM, connect JProfiler to it and take a snapshot of all objects in live memory:</p>

<p><img src="/static/images/2023-09-30-A-Boring-JVM-Memory-Profiling-Story/objects-start.png" alt="objects-start" /></p>

<p>We can see <code>byte[]</code> takes the most memory but it doesn’t mean it’s responsible for memory leak, since we need to look at the increase of the memory.</p>

<p>So we need to wait for a while for the memory problem starts to happen. In my case, obvious memory increase can be occurred after the JVM run for about 12 hours. Normally if this is a work related thing, I may want to make it faster by increasing the work load. In this case, the code is fetching RSS feeds, so I could make the interval shorter so that it makes more requests. But since this is only a side project, I don’t need to continues working on it, and I also don’t quite like the idea to increase the requests to target RSS websites to increase their load. So I decide just let the JVM run during the night and take another look next day:</p>

<p><img src="/static/images/2023-09-30-A-Boring-JVM-Memory-Profiling-Story/objects-end.png" alt="objects-end" /></p>

<p>Okay, obviously <code>scala.collection.mutable.LinkedHashMap$LinkedEntry</code> increased a lot. But is there anything else? Conveniently, JProfiler has the feature to compare 2 snapshots. Just go to “Session” -&gt; “Start Center” -&gt; “Open Snapshots” -&gt; “Compare Multiple Snapshots”. After open those 2 snapshots, select both of them on the left and then compare memory:</p>

<p><img src="/static/images/2023-09-30-A-Boring-JVM-Memory-Profiling-Story/objects-compare.png" alt="objects-compare" /></p>

<p>We can see <code>LinkedEntry</code> indeed increased the most by instance count. However, if we sort by size, we fill find <code>byte[]</code> increased the most by memory size.</p>

<h2 id="a-false-root-cause">A False Root Cause</h2>

<p>Since <code>byte[]</code> increased the most by memory size, I’d like to start there. By using “Allocation Call Tree”, we can check which code allocates <code>byte[]</code> the most. After profiling for a while, we get the following result:</p>

<p><img src="/static/images/2023-09-30-A-Boring-JVM-Memory-Profiling-Story/allocation-tree-bytes.png" alt="allocation-tree-bytes" /></p>

<p>Okay, the top allocation goes to my own code <code>me.binwang.rss.parser.SourceParser</code>. It’s the class that parse the xml from RSS feeds. So I looked into it if it has any code that can cause memory leak and I found this:</p>

<div class="language-Scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>object SourceParser {
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>  def parse(url: String, content: Resource[IO, InputStream]): IO[(Source, Seq[Try[FullArticle]])] = {
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>    content.use { c =&gt;
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>      // ...
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>      throw new RuntimeException(s&quot;Error to parse source xml, unrecognized label $label&quot;)
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>      // ...
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>    }
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>  }
</pre></div>
</div>
</div>

<p>So there is an exception thrown in a <code>Resource.use</code>. <code>Resource.use</code> makes sure to cleanup the resource when the <code>use</code> scope is over. But what will happen if it throws an example in there? I thought it will cause <code>use</code> to not handle the cleanup properly. So I changed it to use <code>IO.raiseError</code> instead of throw it directly.</p>

<p>However, while I deploying the code, I thought I should really test it. So I wrote a piece of simple code to see whether <code>Resource</code> will still be cleaned up if there is any exception thrown in <code>use</code>, and the answer is yes. So this shouldn’t be the root cause. And the deployment result also confirms that: the memory kept increasing with this fix.</p>

<h2 id="the-real-root-cause">The Real Root Cause</h2>

<p>Maybe <code>byte[]</code> just happened to uses more memory because it’s parsing a large xml at that time. It’s okay that it isn’t the real root cause since we have another lead: <code>scala.collection.mutable.LinkedHashMap$LinkedEntry</code>. From the profiling, its allocation tree looks like this:</p>

<p><img src="/static/images/2023-09-30-A-Boring-JVM-Memory-Profiling-Story/allocation-tree-linkedlist.png" alt="allocation-tree-linkedlist" /></p>

<p>Okay, so seems most of them come from quill. quill is a library that compiles Scala DSL to SQL queries. It is fairly complex since it uses macros. I checked the code in the allocation tree and couldn’t find out what is wrong.</p>

<p>Then I tried to check the object reference to see which instances are pointed to the these LinkedEntry:</p>

<p><img src="/static/images/2023-09-30-A-Boring-JVM-Memory-Profiling-Story/object-refer.png" alt="object-refer" /></p>

<p>No surprise, they are basically all from quill as well. However, I couldn’t understand the internal AST represents of quill and not sure where are they coming from.</p>

<p>It’s time to search the Internet to see if there is any known issue in quill about memory leak. Maybe I didn’t have the right query, I didn’t find proper results from Internet.</p>

<p>After struggle for a while, I went to its Github repo to search “Memory leak” directly and found 3 issues. That’s good! And there is <a href="https://github.com/zio/zio-quill/issues/2484">one</a> describes the exact problem we have. If we see the allocation tree above, we can find there is a call from <code>NormalizeCaching</code> (at the bottom of the tree in the picture), which is the class that the issue describes. I guess I didn’t go that far enough to check this class. I’m glad someone else did and found the issue! Basically the root cause is there is a map in the caching doesn’t have any bound. So the cache triggered by dynamic queries never got expired and is growing more and more:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>private val cache = new ConcurrentHashMap[Ast, Ast] 
</pre></div>
</div>
</div>

<h2 id="fix-the-memory-leak">Fix the Memory Leak</h2>

<p>The issue is pretty old and is related to a core feature. I’m surprised it’s not fixed yet. As I said, once we found the root cause, the fix should be easy. We just need a way to make the cache expire. I replaced the cache implementation with Guava’s cache, and after the suggestion of maintainer changed it to <a href="https://github.com/ben-manes/caffeine">Caffeine</a>’s cache implementation. <a href="https://github.com/zio/zio-quill/pull/2878">Here is the PR</a>.</p>

<p>I built quill with the fix locally and tested with RSS Brain. The memory leak is indeed fixed! How exciting it is!</p>

<h2 id="conclusion">Conclusion</h2>

<p>Let’s review the process of fixing the memory leak in this case:</p>

<ul>
  <li>Setup profiler.</li>
  <li>Run full GC cannot resolve the memory issue.</li>
  <li>Compare the snapshots between when JVM first started and when the memory increases. See which classes increased most.</li>
  <li>Using allocation tree to find out which part of the code is creating the instances.</li>
  <li>Using references in heap walker to check which classes holds references of those instances.</li>
  <li>Check the identified code and classes.</li>
  <li>If it’s a third party library and we cannot find the root cause, check if the issue is reported. Otherwise report the issue.</li>
  <li>Fix the memory leak based on the root cause.</li>
</ul>]]></content><author><name></name></author><category term="Java" /><category term="JVM" /><category term="Memory Leak" /><category term="Scala" /><category term="JProfiler" /><category term="Profiling" /><summary type="html"><![CDATA[Background of Memory Leakings]]></summary></entry><entry><title type="html">Jekyll Plugin to Load Asciinema Recordings Locally</title><link href="https://www.binwang.me/2023-09-24-Jekyll-Plugin-to-Load-Asciicast-Locally.html" rel="alternate" type="text/html" title="Jekyll Plugin to Load Asciinema Recordings Locally" /><published>2023-09-24T00:00:00-04:00</published><updated>2023-09-24T00:00:00-04:00</updated><id>https://www.binwang.me/Jekyll-Plugin-to-Load-Asciicast-Locally</id><content type="html" xml:base="https://www.binwang.me/2023-09-24-Jekyll-Plugin-to-Load-Asciicast-Locally.html"><![CDATA[<p><a href="https://asciinema.org/">Asciinema</a> is a wonderful tool to record Linux terminal. It saves the records as a text format called Asciicast. However, it has a strong integration with its website. Especially if you want to embed the recordings into the web page use some simple JS code like this:</p>

<div class="language-html highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span><span style="color:#070;font-weight:bold">&lt;script</span> <span style="color:#b48">src</span>=<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">https://asciinema.org/a/14.js</span><span style="color:#710">&quot;</span></span> <span style="color:#b48">id</span>=<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">asciicast-14</span><span style="color:#710">&quot;</span></span> <span style="color:#b48">async</span><span style="color:#070;font-weight:bold">&gt;</span><span style="color:#070;font-weight:bold">&lt;/script&gt;</span>
</pre></div>
</div>
</div>

<p>You need to share the recordings to Asciinema’s website and need to link an account with the recordings, otherwise they will be deleted after 7 days, which I just found out yesterday. I don’t want my blog to rely on some third party website for core content, so I need a way to load the recordings from my website itself.</p>

<p>Lucky, the <a href="https://github.com/asciinema/asciinema-player">Asciinema Javascript player</a> is open source and support to load recordings from a url out of box. First you need to import the CSS:</p>

<div class="language-html highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span><span style="color:#070;font-weight:bold">&lt;link</span> <span style="color:#b48">rel</span>=<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">stylesheet</span><span style="color:#710">&quot;</span></span> <span style="color:#b48">type</span>=<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">text/css</span><span style="color:#710">&quot;</span></span> <span style="color:#b48">href</span>=<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">/asciinema-player.css</span><span style="color:#710">&quot;</span></span> <span style="color:#070;font-weight:bold">/&gt;</span>
</pre></div>
</div>
</div>

<p>This is no big deal since this can be put in Jekyll’s template. Then you need some JS code like this:</p>

<div class="language-html highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span><span style="color:#070;font-weight:bold">&lt;div</span> <span style="color:#b48">id</span>=<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">demo</span><span style="color:#710">&quot;</span></span><span style="color:#070;font-weight:bold">&gt;</span><span style="color:#070;font-weight:bold">&lt;/div&gt;</span>
<span class="line-numbers"><a href="#n2" name="n2">2</a></span> ...
<span class="line-numbers"><a href="#n3" name="n3">3</a></span><span style="color:#070;font-weight:bold">&lt;script</span> <span style="color:#b48">src</span>=<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">/asciinema-player.min.js</span><span style="color:#710">&quot;</span></span><span style="color:#070;font-weight:bold">&gt;</span><span style="color:#070;font-weight:bold">&lt;/script&gt;</span>
<span class="line-numbers"><a href="#n4" name="n4">4</a></span><span style="color:#070;font-weight:bold">&lt;script&gt;</span>
<span class="line-numbers"><a href="#n5" name="n5">5</a></span><span style="background-color:hsla(0,0%,0%,0.07);color:black">  AsciinemaPlayer.create(<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">/demo.cast</span><span style="color:#710">'</span></span>, document.getElementById(<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">demo</span><span style="color:#710">'</span></span>));</span>
<span class="line-numbers"><a href="#n6" name="n6">6</a></span><span style="color:#070;font-weight:bold">&lt;/script&gt;</span>
</pre></div>
</div>
</div>

<p>It’s a little bit too much for embedding a terminal recording in a blog. However, with the powerful Jekyll plugin system, We can write a plugin to make it simpler so that we can just use a tag to include it:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>{% asciicast &lt;id&gt; %}
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>
</pre></div>
</div>
</div>

<p>Here is the implementation, it’s also in <a href="https://github.com/wb14123/blog/blob/master/jekyll/_plugins/Asciicast.rb">my blog’s Github repo</a>:</p>

<div class="language-ruby highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span><span style="color:#080;font-weight:bold">module</span> <span style="color:#B06;font-weight:bold">Jekyll</span>
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>  <span style="color:#080;font-weight:bold">class</span> <span style="color:#B06;font-weight:bold">RenderAsciicastTag</span> &lt; <span style="color:#036;font-weight:bold">Liquid</span>::<span style="color:#036;font-weight:bold">Tag</span>
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>    <span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">initialize</span>(tag_name, text, tokens)
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>      <span style="color:#080;font-weight:bold">super</span>
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>      <span style="color:#33B">@text</span> = text.strip
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>    <span style="color:#080;font-weight:bold">end</span>
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>    <span style="color:#080;font-weight:bold">def</span> <span style="color:#06B;font-weight:bold">render</span>(context)
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>      <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">&lt;div id=</span><span style="color:#b0b">\&quot;</span><span style="color:#D20">cast-</span><span style="background-color:hsla(0,0%,0%,0.07);color:black"><span style="font-weight:bold;color:#666">#{</span><span style="color:#33B">@text</span><span style="font-weight:bold;color:#666">}</span></span><span style="color:#b0b">\&quot;</span><span style="color:#D20">&gt;&lt;/div&gt;</span><span style="color:#710">&quot;</span></span> \
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>      <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">&lt;script src=&quot;/static/js/asciinema-player.min.js&quot;&gt;&lt;/script&gt;</span><span style="color:#710">'</span></span> \
<span class="line-numbers"><a href="#n12" name="n12">12</a></span>      <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">&quot;</span><span style="color:#D20">&lt;script&gt;AsciinemaPlayer.create('/static/asciicasts/</span><span style="background-color:hsla(0,0%,0%,0.07);color:black"><span style="font-weight:bold;color:#666">#{</span><span style="color:#33B">@text</span><span style="font-weight:bold;color:#666">}</span></span><span style="color:#D20">.cast', document.getElementById('cast-</span><span style="background-color:hsla(0,0%,0%,0.07);color:black"><span style="font-weight:bold;color:#666">#{</span><span style="color:#33B">@text</span><span style="font-weight:bold;color:#666">}</span></span><span style="color:#D20">'), {rows: 10, autoPlay: true});&lt;/script&gt;</span><span style="color:#710">&quot;</span></span>
<span class="line-numbers"><a href="#n13" name="n13">13</a></span>    <span style="color:#080;font-weight:bold">end</span>
<span class="line-numbers"><a href="#n14" name="n14">14</a></span>  <span style="color:#080;font-weight:bold">end</span>
<span class="line-numbers"><a href="#n15" name="n15">15</a></span><span style="color:#080;font-weight:bold">end</span>
<span class="line-numbers"><a href="#n16" name="n16">16</a></span>
<span class="line-numbers"><a href="#n17" name="n17">17</a></span><span style="color:#036;font-weight:bold">Liquid</span>::<span style="color:#036;font-weight:bold">Template</span>.register_tag(<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">asciicast</span><span style="color:#710">'</span></span>, <span style="color:#036;font-weight:bold">Jekyll</span>::<span style="color:#036;font-weight:bold">RenderAsciicastTag</span>)
</pre></div>
</div>
</div>

<p>It will find the recordings under <code>/static/asciicasts/{id}.cast</code> and load from there.</p>

<p>Put this file under <code>_plugins</code> and happy hacking!</p>]]></content><author><name></name></author><category term="Jekyll" /><category term="Blog" /><category term="command line" /><category term="Asciicast" /><summary type="html"><![CDATA[Asciinema is a wonderful tool to record Linux terminal. It saves the records as a text format called Asciicast. However, it has a strong integration with its website. Especially if you want to embed the recordings into the web page use some simple JS code like this:]]></summary></entry><entry><title type="html">Migrate Scala2grpc to Cats Effect 3</title><link href="https://www.binwang.me/2023-09-23-Migrate-Scala2Grpc-to-Cats-Effect-3.html" rel="alternate" type="text/html" title="Migrate Scala2grpc to Cats Effect 3" /><published>2023-09-23T00:00:00-04:00</published><updated>2023-09-23T00:00:00-04:00</updated><id>https://www.binwang.me/Migrate-Scala2Grpc-to-Cats-Effect-3</id><content type="html" xml:base="https://www.binwang.me/2023-09-23-Migrate-Scala2Grpc-to-Cats-Effect-3.html"><![CDATA[<p><a href="https://github.com/wb14123/scala2grpc">Scala2grpc</a> is a library and SBT plugin I wrote so that you can integrate gRPC to Scala code in a non-invasive way. In a previous <a href="/2022-05-02-A-Library-to-Make-It-Easier-to-Use-Scala-with-GRPC.html">blog post</a>, I talked about the motivation behind it.</p>

<p>The library requires each service method to return Cats Effect’s <code>IO</code> or fs2 stream. However, it’s still using Cats Effect 2.x version. There are big changes in Cats Effect 3 and almost all up to date libraries already support it. So it’s time to migrate it to Cats Effect 3 as well.</p>

<h2 id="replace-akka-grpc-with-fs2-grpc">Replace Akka gRPC with fs2-grpc</h2>

<p>This library was using <a href="https://doc.akka.io/docs/akka-grpc/current/index.html">akka-grpc</a>. But I want to replace it for a few reasons:</p>

<ul>
  <li>It uses <a href="https://github.com/krasserm/streamz">streamz</a> to convert between Akka streams and fs2 streams. This library doesn’t support Cats Effect 3 and hasn’t been updated for a while. This is the biggest reason that I need to migrate from akka immediately.</li>
  <li>Akka changed its open source license to a very expensive one which I didn’t know at the time writing this library. Even the license doesn’t cost anything if the revenue doesn’t reach a certain point, I don’t want it to be a liability.</li>
  <li>It’s good to have a gRPC library that supports for Cats Effect and fs2 streams natively.</li>
</ul>

<p>So in the newer version, I replaced Akka gRPC with <a href="https://github.com/typelevel/fs2-grpc">fs2-grpc</a>, a library under Typelevel umbrella and natively supports Cats Effect and fs2. The document is not as good and I spent quite some time to figure out how to actually use it, but I’m happy it finally worked out.</p>

<h2 id="add-hooks-to-grpc-calls">Add Hooks to gRPC Calls</h2>

<p>In the previous version of Scala2grpc, there is a feature to log every request. But it is a pretty hacky implementation: I just throw the logging logic into the generated code. Since this version is a breaking change, it’s a good opportunity to revisit the approach and see how to add generic hooks before and after each gRPC calls.</p>

<p>My implementation wraps the gRPC response into a context in the generated code. The context includes the response with type of <code>IO</code> or <code>fs2.Stream</code>. Because of the referential transparency, you can take the response and add hooks before or after it. More detailed document is in <a href="https://github.com/wb14123/scala2grpc#4-optional-define-custom-grpc-hook">this section of readme</a>.</p>

<h2 id="non-invasive-nature-of-the-library">Non-invasive Nature of the Library</h2>

<p>The migration brings lots of breaking changes, so it is a good test for the non-invasive nature of the library. I have 2 side projects that are using this library and I migrated one of them recently. Since it’s also using Cats Effect, there are some migration steps unrelated to this library. But regarding of the related parts, the migration process is very smooth: I don’t need to change any implementation code of the services. The generated gPRC protocol files are also not changed either. The only thing I need to change is the single object that implements <code>GRPCGenerator</code> to <a href="https://github.com/wb14123/scala2grpc#2-create-an-object-to-implement-grpcgenerator">pass in some new parameters</a>. It is impossible if I used akka gRPC directly and changed it to fs2-grpc since their interface are all different.</p>

<p>I’m so happy with the result: it adds gRPC to pure Scala code so easily without ever touch it. It saves me so much time to build a service and keeps the code clean at the same time. It continue to be a must have for my future Scala gRPC projects.</p>]]></content><author><name></name></author><category term="Programming" /><category term="Scala" /><category term="gRPC" /><category term="Cats" /><category term="Functional Programming" /><summary type="html"><![CDATA[Scala2grpc is a library and SBT plugin I wrote so that you can integrate gRPC to Scala code in a non-invasive way. In a previous blog post, I talked about the motivation behind it.]]></summary></entry><entry><title type="html">Build a Linux Virtual Machine for Windows Apps</title><link href="https://www.binwang.me/2023-09-01-Build-a-Linux-Virtual-Machine-for-Windows-Apps.html" rel="alternate" type="text/html" title="Build a Linux Virtual Machine for Windows Apps" /><published>2023-09-01T00:00:00-04:00</published><updated>2023-09-01T00:00:00-04:00</updated><id>https://www.binwang.me/Build-a-Linux-Virtual-Machine-for-Windows-Apps</id><content type="html" xml:base="https://www.binwang.me/2023-09-01-Build-a-Linux-Virtual-Machine-for-Windows-Apps.html"><![CDATA[<h2 id="background">Background</h2>

<p>Linux is great. However sometimes you just need to run some Windows only applications to collaborate with other people, especially if it’s impossible to let the other party to change the software. Luckily I rarely run into that situation in the past 10+ years. The only recent exceptions I can remember are filling some government forms (which uses pdf with XFA form. Yes Firefox can fill that now but it’s still incompatible with Adobe Reader from time to time), use IM and video meeting software with a previous Chinese client for some consultant work.</p>

<p>Even it’s rarely used, it’s handy to have a VM to run Windows applications. But Windows is becoming more and more bloat, adding more and more tracking and ads, basically more holistic to users. The only good parts in Windows XP and Windows 7 have long gone. Currently I have a Windows 10 VM, but I’m not sure if I ever want to login to Windows 11 if Windows 10’s life is end. So it’s good to have a backup plan to run Windows apps without Windows. Practise reasons aside, it’s just fun to play with Linux distros. Linux and its desktop environments are so diverse and configurable, I spent such a great time to explore what are the possibilities.</p>

<p>There is <a href="https://www.winehq.org/">Wine</a> to run Windows applications on Linux. It’s not perfect. Some apps need to be tweaked a lot in order to run with wine and some are just nearly impossible. Even the software can be run with wine, I don’t want to run it on my OS since wine is just a compatibility layer, not a sandbox. Which means the typical malware like behaviours in Windows applications are still effective under Wine. So I need to run it in a virtual machine. It also gives us an opportunity to select a distro to focus more on this specific task.</p>

<p>I have tried a few distros in the last few days. At the end I find <a href="https://www.deepin.org">Deepin Linux</a> is the best one for this use case. Especially you want to run Chinese Windows apps.</p>

<h2 id="a-little-history-of-deepin">A Little History of Deepin</h2>

<p>Deepin’s root is in Windows. It first started as a Windows online forum and then started to customize and piracy Windows XP. The year was 2006. Almost no one bought Windows in China back in the days. I don’t know if business or even Universities ever bought Windows licenses or not, but even they do, it’s a very common practise to install piracy Windows in those environment because the popular ones are so user friendly. Computer sellers would ask the buyers if they want to change the stock Windows to a piracy one, and most of the time they do. Ironically, the only time I’ve inputted a (legitimately obtained) Windows key is when I was working at Redhat and setting up a Windows server for testing Samba and nfs. The popular piracy versions are really impressive: the installation is fast and easy, they are more beautiful, they include things like system backup and recovery, they have common drivers pre-installed and application to find drivers (not like the driver finder on Windows, this one actually works), the system was cut down to a very small size and so on. However, if it sounds sketchy to you, you are not wrong. Even though the user experience maybe superior, there is no shortage of back doors and things like that. The popular versions even have their own more sketchy piracy versions. However, that era is wild west for computer security and just one more vulnerability didn’t really matter that much in my mind.</p>

<p>Anyway, Deepin was one of the most popular among them. But things didn’t last for long. Around 2008, the year China hosted its first Olympic Game, the person behind a popular piracy version got arrested. Even the common practise of using piracy Windows in China lasted a long time after that, the big ones felt the risk and stopped making them. Some of them started to make Linux distros instead. Again, Deepin became one of the most popular. <a href="https://en.wikipedia.org/wiki/YLMF_Computer_Technology_Co.,_Ltd.">雨林沐风 (YLMF)</a> is another very popular one which is in famous of its clean and beautify theme in the Windows piracy era. It started to make Linux distro (<a href="https://en.wikipedia.org/wiki/StartOS">YLMF OS</a>) around the same time. It is the first Linux distro I’ve ever used and introduced the whole world of Linux to me.</p>

<p>It’s no wonder Deepin Linux has good support for Windows applications: Windows users are its earliest user base. It’s still true nowadays: even with some failed attempts, Chinese government never stopped exploring to use Linux instead of Windows on government devices. After these years, because of factors like applications are more web based instead of native, and the better experience of Linux desktop, Chinese government actually replaced a large amount of their devices with Linux. From what I know, they are using <a href="https://en.wikipedia.org/wiki/Ubuntu_Kylin">Ubuntu Kylin</a> instead of Deepin or UOS (commercial version of Deepin), but the market is large enough to motivate Deepin to continue maintaining Windows app supports.</p>

<h2 id="deepin-linux-101">Deepin Linux 101</h2>

<p>We’ve talked enough history. Let’s look at Deepin Linux at nowadays. It has it’s own desktop environment called DDE and includes lots of its own apps like browser, video player, mail client and so on. But it’s not my taste and the DE is pretty resource hungry on my machine. Luckily, Deepin is based on Debian stable, so you can basically customize to whatever you like using Debian packages, which we will do later.</p>

<p>The main thing we want in Deepin Linux is its app store. It has lots of Windows applications supported by default. Deepin actually has its own wine version deepin-wine to support those Windows apps better. No matter what tweaks we are going to do with the system, make sure app store works after that.</p>

<p>It also ships with Android support with UEngine, which is a fork of Anbox. There are some officially supported Android apps in the package repo but seems they are not findable in app store. You can use <code>apt search uengine</code> to find them in terminal. I’ve never had good experience with Android in VM and this time it’s no exception: I tried to install an app from apt and it couldn’t start because of uengine startup timeout. I’m not sure if it will be better on a physical machine but I don’t bother to try it.</p>

<p>Overall, I have double feelings about Deepin. On one hand, it’s pretty impressive on the technical side about what they have done and the community they’ve built. On the other hand, it always feels a little bit sketchy. Even after the Windows piracy era, the Linux distro is still less trustworthy in my mind because of some telemetry its app store collects, not straightforward removable stock apps, the connection (or the intention to connect) with Chinese government and so on. In addition of all the Windows apps I’m going to install, I will not have any personal or important data on it.</p>

<p>After understanding the basics of Deepin Linux, let’s go ahead to install and tweak it. To have a taste of what it will look like, let me show a screenshot of my setup:</p>

<p><img src="/static/images/2023-08-31-Build-a-Linux-VM-for-Windows-Apps/screenshot.png" alt="screnshot" /></p>

<h2 id="installation">Installation</h2>

<p>The installation is pretty straightforward, but make sure to make these tweaks:</p>

<ul>
  <li>During the installation, it will detect that you are in a VM and prompt to use “performance” mode. Make sure to select it so that it will be faster. Even though we will replace the DE later so I don’t think it matters that much, but it doesn’t hurt anyway.</li>
  <li>By default it will create a recovery partition, which is a waste of storage since we are using a VM. We can take snapshots through the VM software if we backup and recovery. So make sure to manually partition the file system and not using recovery partition.</li>
</ul>

<h2 id="replace-dde-with-xfce">Replace DDE with XFCE</h2>

<p>As I said, I don’t like the default DDE Deepin ships. And with limited time I don’t find it’s very configurable as well. So we will replace it with the less resource hungry and highly customizable XFCE. In order to install XFCE, run this in the terminal:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>sudo apt install xfce4 xfce4-goodies
</pre></div>
</div>
</div>

<p>Then logout and select <code>xfce</code> in the login screen.</p>

<h2 id="configure-lightdm">Configure lightdm</h2>

<p>Deepin is using <code>lightdm</code> as its display manager. To match our simple xfce feeling, I’d like to change the login screen to a simpler and reto look. Open <code>/etc/ligthdm/lightdm.conf</code> and apply these changes:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>- greeter-session=lightdm-deepin-greeter
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>+ greeter-session=lightdm-gtk-greeter
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>- user-session=deepin
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>+ user-sesion=xfce
</pre></div>
</div>
</div>

<h2 id="install-windows-95-theme">Install Windows 95 theme</h2>

<p>Another important reason of choosing XFCE the <a href="https://github.com/grassmunk/Chicago95">Chicago95 theme</a>, which makes XFCE looks like Windows 95. If we are having an OS for running Windows programs, what’s a better look than Windows 95/98/2000 era theme?</p>

<p>Go to <a href="https://github.com/grassmunk/Chicago95">Chicago95’s Github repo</a> and follow the instructions to install it. Since it’s made for XUbuntu, which is based on Ubuntu which is in turn based on Debian, the installation script works perfectly. After installation, read the popped up txt file so tweak remaining things to make it more Windows 95 like if you want.</p>

<p>If you also want to make the login screen Windows 95 like, you need to use <code>lightdm-webkit-greeter</code> instead of the <code>lightdm-gtk-greeter</code> above and change the theme. See <a href="https://github.com/grassmunk/Chicago95/tree/5670fde8ce33b33d37622b888278aa9cdbe5eea2/Lightdm/Chicago95">the doc</a> for more details. However, <code>lightdm-webkit-greeter</code> is not in Debian or Deepin’s package repo by default and I find the trouble to install it manually doesn’t worth it, so I didn’t make the change.</p>

<p>For Firefox, there is a <a href="https://addons.mozilla.org/en-US/firefox/addon/windows-98-se/?utm_source=addons.mozilla.org&amp;utm_medium=referral&amp;utm_content=search">Windows 98 SE</a> theme I find fit into the system theme the best.</p>

<h2 id="other-xfce-tweaks">Other XFCE tweaks</h2>

<ul>
  <li>Disable compositor in Settings -&gt; Window Manager Tweaks -&gt; Compositor. Some windows will have a black frame after this, so enable/disable it based on your preference.</li>
  <li>I’d also like to disable auto session save:
    <ul>
      <li>Uncheck all the boxes in Settings -&gt; Session and Startup -&gt; General (I find it doesn’t prompt and auto save if <code>prompt on logout</code> is checked)</li>
      <li>Remove all the existing sessions: <code>rm -r ~/.cache/sessions/*</code></li>
    </ul>
  </li>
</ul>

<h2 id="remove-stock-deepin-apps">Remove Stock Deepin Apps</h2>

<p>There are lots of stock apps made by Deepin. Even though I like the effort, I still prefer the familiar ones and the ones XFCE has. So I need to uninstall the stock apps. However, you cannot uninstall them through Deepin’s app store, so we need to use <code>apt</code> to find and remove them.</p>

<p>Use <code>apt search deepin | grep installed</code> to find installed deepin packages and remove the ones you don’t want. Then use <code>sudo apt autoremove</code> and <code>sudo apt autoclean</code> to cleanup the not needed dependencies. Make sure app store is still working after this since it’s the whole point of using Deepin.</p>

<h2 id="disable-deepin-services">Disable Deepin Services</h2>

<p>I didn’t remove all the deepin related packages since some of them are needed by the App Store. However, I don’t think some of them are needed to run as deamon even if I left them on the machine. So type <code>systemctl status deepin-</code> and press tab for autocomplete to see the systemd services related to deepin, and use <code>systemctl disable &lt;service&gt;</code> to disable the ones you don’t need.</p>]]></content><author><name></name></author><category term="Linux" /><category term="Windows" /><category term="Virtual Machine" /><category term="Deepin" /><category term="Wine" /><summary type="html"><![CDATA[Background]]></summary></entry><entry><title type="html">Compare Task Processing Approaches in Scala</title><link href="https://www.binwang.me/2023-08-27-Compare-Task-Processing-Approaches-in-Scala.html" rel="alternate" type="text/html" title="Compare Task Processing Approaches in Scala" /><published>2023-08-27T00:00:00-04:00</published><updated>2023-08-27T00:00:00-04:00</updated><id>https://www.binwang.me/Compare-Task-Processing-Approaches-in-Scala</id><content type="html" xml:base="https://www.binwang.me/2023-08-27-Compare-Task-Processing-Approaches-in-Scala.html"><![CDATA[<p><em>All the source code mentioned in this blog can be found in <a href="https://github.com/wb14123/scala-stream-demo">my Github repo</a>.</em></p>

<h2 id="task-processing">Task Processing</h2>

<p>There is a common problem in computer science and I’ve met it again recently: how to generate and process tasks efficiently? Use my recent project <a href="https://www.rssbrain.com">RSS Brain</a> as an example: it needs to find the RSS feeds that haven’t been updated for a while in a database, and fetch the newest data from network.</p>

<p>The easiest way to do it is producing and consuming the tasks in a sequence, for example:</p>

<div class="language-Scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>val feeds = getPendingFeeds() // produce the tasks
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>feeds.foreach(fetchFromNetwork) // consume the dtasks
</pre></div>
</div>
</div>

<p>However, it is unnecessarily slow. Network request doesn’t take lots of CPU and we can send multiple requests at the same time. Even if <code>fetchFromNetwork</code> is a CPU bound task, it can be parallelized if there are multiple CPU cores on a machine.</p>

<p>In this article, we will explore ways to do it more efficiently with <a href="https://typelevel.org/cats-effect/">Cats Effect</a> and <a href="https://fs2.io">FS2</a> in a functional programming fashion.</p>

<p><em>You may wonder why not using AKKA stream? Other than it’s using a different programming paradigm (not functional programming), it’s also because <a href="https://www.lightbend.com/blog/why-we-are-changing-the-license-for-akka">AKKA has changed its license</a> with a ridiculous price.</em></p>

<h2 id="introducing-cats-effect-and-fs2">Introducing Cats Effect and FS2</h2>

<p>To make <code>processTask</code> async, there is <code>Future</code> in Scala’s standard library. However, the side effect will happen when you create a <code>Future</code> instance. For example:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>def processTask(task: Task): Future[Unit] = Future(println(task))
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>val runTask1 = processTask(task1) // this will start the async task
</pre></div>
</div>
</div>

<p>I assume the readers have a basic understanding of functional programming, so I’ll not explain why we want to avoid side effects. While Scala is not a pure functional language, a popular Scala library <a href="https://typelevel.org/cats-effect/">Cats Effect</a> provides convenient ways to wrap side effects. With the help of its <code>IO</code> type, we can define an async task like this:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>def processTask(task: Task): IO[Unit] = IO(println(task))
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>// this will not start the task, so no side effect
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>val runTask1 = processTask(task1)
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>
<span class="line-numbers"><a href="#n6" name="n6">6</a></span>// out of pure functional world and starts the side effect
<span class="line-numbers"><a href="#n7" name="n7">7</a></span>runTask1.unsafeRunSync()
</pre></div>
</div>
</div>

<p>Then there is <a href="https://fs2.io">fs2</a> that is a stream library that can be used with cats effect. It will be very handy when resolving our problem as we can see later.</p>

<p><em>Cat Effect has some big changes in version 3.x. In this article, we are using version 2.x. But I may upgrade the version in the future.</em></p>

<h2 id="testing-setup">Testing Setup</h2>

<p>In order to test which approach is the best under different scenarios, we need some basic setup. In <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/TestRunner.scala">TestRunner.scala</a>, I defined some functions to generate tasks. Here are their signatures:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>// Produce a sequence of tasks represented by `Int`
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>def produce(start: Int, end: Int): IO[Seq[Int]]
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>// Process a task
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>def consume(x: Int): IO[Unit]
<span class="line-numbers"><a href="#n6" name="n6">6</a></span>
<span class="line-numbers"><a href="#n7" name="n7">7</a></span>// Produce tasks as a stream
<span class="line-numbers"><a href="#n8" name="n8">8</a></span>def def produceStream(start: Int, end: Double): fs2.Stream[IO, Int]
</pre></div>
</div>
</div>

<p><code>produce</code> simply produces tasks as <code>int</code>, and <code>consume</code> just print characters. In each of the functions, I use <code>IO.sleep</code> to create some delay to simulate the real world non-blocking IO. They also print characters <code>P</code> (produce) or <code>C</code> (consume) (based on the width of terminal, some of the <code>C</code> outputs may be skipped to fit the width) when being invoked, so that we can have an intuitive view of how quick tasks are produced and consumed.</p>

<p>Then there is <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/TestConfig.scala">TestConfig.scala</a> for configuring the test:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>trait TestConfig {
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>  val testName: String
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>  val produceDelay: FiniteDuration
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>  val minConsumeDelayMillis: Long
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>  val maxConsumeDelayMillis: Long
<span class="line-numbers"><a href="#n6" name="n6">6</a></span>  val batchSize = 100  // consume batch size
<span class="line-numbers"><a href="#n7" name="n7">7</a></span>  val totalSize = 1000 // how many tasks to generate
<span class="line-numbers"><a href="#n8" name="n8">8</a></span>}
</pre></div>
</div>
</div>

<p>By setting up produce and consume delays, we can test scenarios when producer is slower, consumer is slower, or producer and consumer speed is almost the same. Here are the configurations we are going to use in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/Main.scala">Main.scala</a></p>

<div class="language-Scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>val configs = Seq(
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>  new TestConfig {
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>    override val testName: String = &quot;slow-producer&quot;
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>    override val produceDelay: FiniteDuration = 1000.millis
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>    override val minConsumeDelayMillis: Long = 10
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>    override val maxConsumeDelayMillis: Long = 100
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>  },
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>  new TestConfig {
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>    override val testName: String = &quot;balanced&quot;
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>    override val produceDelay: FiniteDuration = 1005.millis
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>    override val minConsumeDelayMillis: Long = 10
<span class="line-numbers"><a href="#n12" name="n12">12</a></span>    override val maxConsumeDelayMillis: Long = 2000
<span class="line-numbers"><a href="#n13" name="n13">13</a></span>  },
<span class="line-numbers"><a href="#n14" name="n14">14</a></span>  new TestConfig {
<span class="line-numbers"><a href="#n15" name="n15">15</a></span>    override val testName: String = &quot;slow-consumer&quot;
<span class="line-numbers"><a href="#n16" name="n16">16</a></span>    override val produceDelay: FiniteDuration = 10.millis
<span class="line-numbers"><a href="#n17" name="n17">17</a></span>    override val minConsumeDelayMillis: Long = 10
<span class="line-numbers"><a href="#n18" name="n18">18</a></span>    override val maxConsumeDelayMillis: Long = 1000
<span class="line-numbers"><a href="#n19" name="n19">19</a></span>  }
<span class="line-numbers"><strong><a href="#n20" name="n20">20</a></strong></span>)
</pre></div>
</div>
</div>

<h2 id="approach-1-batch-consuming">Approach 1: Batch Consuming</h2>

<p>The first approach is to make the consuming side parallel. We can consume a batch of tasks concurrently, like in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/BatchIOApp.scala">BatchIOApp.scala</a>.</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>def loop(start: Int): IO[Unit] = {
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>  if (start &gt;= config.totalSize) {
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>    IO.unit
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>  } else {
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>    produce(start, start + config.batchSize)
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>      .flatMap{_.map(consume).parSequence}
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>      .flatMap(_ =&gt; loop(start + config.batchSize))
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>  }
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>}
</pre></div>
</div>
</div>

<p>However, this only makes a batch of tasks run in parallel. It needs to wait the whole batch to be finished in order to start next batch. This is very obvious when we run this approach and see the output of characters (download <a href="https://github.com/wb14123/scala-stream-demo">Github repo</a> and run <code>sbt "run -n BatchIOApp"</code>). See how it paused after each batch even when consumer is slower than producer:</p>

<div id="cast-tmpg4x5_bn7-ascii"></div>
<script src="/static/js/asciinema-player.min.js"></script>
<script>AsciinemaPlayer.create('/static/asciicasts/tmpg4x5_bn7-ascii.cast', document.getElementById('cast-tmpg4x5_bn7-ascii'), {rows: 10, autoPlay: true});</script>

<h2 id="approach-2-use-blocking-queue-to-buffer-tasks">Approach 2: Use Blocking Queue to Buffer Tasks</h2>

<p>We need a way to let producers not waiting for consumers, and also let consumers not wait for a batch to finish in order to start next batch. A very common solution is to use a queue between producers and consumers. Producers put tasks into the queue, and consumers get tasks for the queue. If the queue is thread safe, then both producers and consumers can work on their own without care about each other. In order to not let producer put unlimited tasks into the queue to blowup the memory, we need the queue to have a capacity. When the queue is full, the producer should be blocked. And when the queue is empty, the consumers should be blocked as well.</p>

<p>In Java, <code>BlockingQueue</code> meets our requirements. We can use an implementation <code>LinkedBlockingQueue</code>. However, <code>BlockingQueue</code> will block the whole thread instead of a single <code>IO</code>. Let’s not worry about it for now and see how to use a queue to implement producing and consuming in parallel. The implementation is in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/BlockingQueueApp.scala">BlockingQueueApp.scala</a>:</p>

<div class="language-scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>val queue = new LinkedBlockingQueue[Option[Int]](config.batchSize * 2)
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>override def work(): IO[Unit] = {
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>  Seq(
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>    (produceStream(0).map(Some(_)) ++ fs2.Stream.emit(None))
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>                        .evalMap(x =&gt; IO(queue.put(x))).compile.drain,
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>    dequeueStream().unNoneTerminate.parEvalMap(config.batchSize)(consume).compile.drain,
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>  ).parSequence.map(_ =&gt; ())
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>}
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>
<span class="line-numbers"><a href="#n12" name="n12">12</a></span>private def dequeueStream(): fs2.Stream[IO, Option[Int]] = {
<span class="line-numbers"><a href="#n13" name="n13">13</a></span>  fs2.Stream.eval(IO(queue.take())) ++ dequeueStream()
<span class="line-numbers"><a href="#n14" name="n14">14</a></span>}
</pre></div>
</div>
</div>

<p>Here we have two IOs run in parallel with <code>parSequence</code>: the first one creates a task stream by <code>produceStream</code>, and append <code>None</code> at the end so that the consumer knows it should end processing. Another stream <code>dequeueStream</code> gets the tasks from the queue then consumes it in parallel with <code>parEvalmap(config.batchSize)(consume)</code>.</p>

<p>When run it with <code>sbt "run -n BlockingQueueApp"</code>, we can see it’s much faster when the consumer is faster or has the same speed as the producer. Especially when the consumer is slow, it prints multiple <code>P</code> at first, which means the producers doesn’t wait all the consumers to finish in order to produce tasks.</p>

<div id="cast-tmp6dx2heu_-ascii"></div>
<script src="/static/js/asciinema-player.min.js"></script>
<script>AsciinemaPlayer.create('/static/asciicasts/tmp6dx2heu_-ascii.cast', document.getElementById('cast-tmp6dx2heu_-ascii'), {rows: 10, autoPlay: true});</script>

<p>Back to the blocking the whole thread problem: it doesn’t seem to be a problem in this case, right? It’s only because we are lucky! In this setup, we are using two fixed threads as the thread pool of running IO in <code>Main.scala</code>:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>private val executor = Executors.newFixedThreadPool(2, (r: Runnable) =&gt; {
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>  val back = new Thread(r)
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>  back.setDaemon(true)
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>  back
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>})
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>implicit override def executionContext: ExecutionContext = ExecutionContext.fromExecutor(executor)
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>implicit override def timer: Timer[IO] = IO.timer(executionContext)
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>implicit override def contextShift: ContextShift[IO] = IO.contextShift(executionContext)
</pre></div>
</div>
</div>

<p>If 2 consumers with empty queue happens to be scheduled on these 2 threads separately, it will block. If we change our <code>BlockingQueueApp</code> to the code in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/RealBlockingQueueApp.scala">RealBlockingQueueApp</a>:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>override def work(): IO[Unit] = {
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>  Seq(
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>    dequeueStream().unNoneTerminate.parEvalMap(config.batchSize)(consume).compile.drain,
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>    dequeueStream().unNoneTerminate.parEvalMap(config.batchSize)(consume).compile.drain,
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>    (produceStream(0).map(Some(_)) ++ fs2.Stream.emit(None)).evalMap(x =&gt; IO(queue.put(x))).compile.drain,
<span class="line-numbers"><a href="#n6" name="n6">6</a></span>  ).parSequence.map(_ =&gt; ())
<span class="line-numbers"><a href="#n7" name="n7">7</a></span>}
</pre></div>
</div>
</div>

<p>Here we started two dequeue stream at first. Now the whole program will block when run it with <code>sbt "run -b"</code> .</p>

<p>The lesson learned here is that there is a big risk if any operation blocks the whole thread in cats effect. Even it doesn’t block the whole program, it may make a whole thread unavailable.</p>

<p>Actually in <a href="https://typelevel.org/cats-effect/docs/thread-model">Cats Effect’s thread model</a>, there is another thread pool for blocking tasks if we mark it explicitly. In <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/AsyncConsole.scala">AsyncConsole.scala</a>, I use this exact block mode to run console output so that it won’t effect other non blocking IO operations:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>def asyncPrintln(s: String)(
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>    implicit cs: ContextShift[IO], blocker: Blocker): IO[Unit] = blocker.blockOn(IO(println(s)))
</pre></div>
</div>
</div>

<p>However, if a thread is blocked in this pool, it will start another thread for the next operation. Based on the document, there is no limit on how many threads will be created. So if the producer is much slower than consumer, there will be more and more consume operations blocked on dequeue, so it will generate a large amount of threads, which is not ideal and eventually even will blow up the memory.</p>

<h2 id="approach-3-use-cats-effect-friendly-queue">Approach 3: Use Cats Effect Friendly Queue</h2>

<p>What if we have a queue that only block the dequeue <code>IO</code> when empty instead of blocking the whole thread? Luckily, FS2 provides such a queue. (Cats Effect 3.x also provides such a queue). The implementation is basically the same as above (code in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/StreamQueueApp.scala">StreamQueueApp.scala</a>):</p>

<div class="language-Scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>import fs2.concurrent.Queue
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>def work(): IO[Unit] = {
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>  for {
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>    queue &lt;- Queue.bounded[IO, Option[Int]](config.batchSize * 2)
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>    _ &lt;- Seq(
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>      (produceStream(0).map(Some(_)) ++ fs2.Stream.emit(None)).through(queue.enqueue).compile.drain,
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>      queue.dequeue.unNoneTerminate.parEvalMap(config.batchSize)(consume).compile.drain,
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>    ).parSequence
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>  } yield ()
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>}
</pre></div>
</div>
</div>

<p>Run <code>sbt "run -n StreamAppQueue"</code> to see how it performs.</p>

<h2 id="approach-4-use-fs2-stream-directly">Approach 4: Use FS2 Stream Directly</h2>

<p>FS2 actually provides some advanced stream operations that makes it possible to combine the producing stream and consume stream, like the code in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/StreamApp.scala">StreamApp.scala</a>:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>produceStream(0).parEvalMap(config.batchSize)(consume).compile.drain
</pre></div>
</div>
</div>

<p>Here we map <code>consume</code> in parallel on <code>produce</code> stream. However, if you try to run <code>sbt "run -n StreamApp"</code> vs <code>sbt "run -n StreamQueueApp"</code>, you will find this is slower than before. This is because <code>produceStream</code> will give the next batch when the downstream asks. If we can prepare at least one batch before the downstream is free, we can save more time. Luckily, it’s very easy to do in fs2. As we can see in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/PrefetchStreamApp.scala">PrefetchStreamApp.scala</a>, we can add <code>prefetch</code> after the <code>produceStream</code>:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>produceStream(0).prefetch.parEvalMap(config.batchSize)(consume).compile.drain
</pre></div>
</div>
</div>

<p>It will prefetch a <a href="https://fs2.io/#/guide?id=chunks">chunk</a> of elements. Use <code>prefetchN</code> if you want to prefetch N chunks.</p>

<p>Then run this with <code>sbt "run -n PrefetchStreamApp"</code>, you will find the performance is similar as the queued approach.</p>

<p>Actually if you check the source code of <code>prefetch</code>, you will find the implementation is almost the same as ours:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>def prefetch[F2[x] &gt;: F[x]: Concurrent]: Stream[F2, O] = prefetchN[F2](1)
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>def prefetchN[F2[x] &gt;: F[x]: Concurrent](n: Int): Stream[F2, O] =
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>  Stream.eval(Queue.bounded[F2, Option[Chunk[O]]](n)).flatMap { queue =&gt;
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>    queue.dequeue.unNoneTerminate
<span class="line-numbers"><a href="#n6" name="n6">6</a></span>      .flatMap(Stream.chunk(_))
<span class="line-numbers"><a href="#n7" name="n7">7</a></span>      .concurrently(chunks.noneTerminate.covary[F2].through(queue.enqueue))
<span class="line-numbers"><a href="#n8" name="n8">8</a></span>  }
</pre></div>
</div>
</div>

<h2 id="approach-5-make-producers-run-in-parallel">Approach 5: Make Producers Run in Parallel</h2>

<p>We’ve made it runs in parallel between consumers, also between consumers and producers. But we haven’t made producers run in parallel yet. With the queue, its very easy to do, just start multiple <code>IO</code>s for <code>produceStream.through(queue.enqueue)</code>. <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/ConcurrentProducerQueueApp.scala">ConcurrentProduceQueueApp.scala</a> is an example:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>private val counter = new AtomicInteger(0)
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>override def work(): IO[Unit] = {
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>  for {
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>    queue &lt;- Queue.bounded[IO, Int](config.batchSize * 2)
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>    _ &lt;- Seq(
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>      produceStream(0, config.totalSize / 2).through(queue.enqueue).compile.drain,
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>      produceStream(config.totalSize / 2, config.totalSize).through(queue.enqueue).compile.drain,
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>      queue.dequeue.parEvalMap(config.batchSize) { x =&gt;
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>        consume(x).map { _ =&gt;
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>          if (counter.incrementAndGet() &gt;= config.totalSize) {
<span class="line-numbers"><a href="#n12" name="n12">12</a></span>            None
<span class="line-numbers"><a href="#n13" name="n13">13</a></span>          } else {
<span class="line-numbers"><a href="#n14" name="n14">14</a></span>            Some()
<span class="line-numbers"><a href="#n15" name="n15">15</a></span>          }
<span class="line-numbers"><a href="#n16" name="n16">16</a></span>        }
<span class="line-numbers"><a href="#n17" name="n17">17</a></span>      }.unNoneTerminate.compile.drain,
<span class="line-numbers"><a href="#n18" name="n18">18</a></span>    ).parSequence
<span class="line-numbers"><a href="#n19" name="n19">19</a></span>  } yield ()
<span class="line-numbers"><strong><a href="#n20" name="n20">20</a></strong></span>}
</pre></div>
</div>
</div>

<p>It has 2 concurrent producers but in theory you can create as many as you want, just be careful with the parameters of <code>produceStream</code>.</p>

<p>If you run this with <code>sbt "run -n ConcurrentProduceQueueApp"</code>, you can find the performance is much better with slower producer. However, with the help of fs2 library, we can make the code cleaner without depends on any queue explicitly. Here is what I did in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/ConcurrentProducerApp.scala">ConcurrentProducerApp.scala</a>:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>def work(): IO[Unit] = {
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>  fs2.Stream.emits(Range(0, produceParallelism))
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>    .map(batch =&gt; produceStream(
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>      batch * config.totalSize / produceParallelism,
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>      (batch + 1) * config.totalSize / produceParallelism.toDouble))
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>    .parJoin(produceParallelism)
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>    .prefetch
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>    .parEvalMap(config.batchSize)(consume).compile.drain
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>}
</pre></div>
</div>
</div>

<p>Here we use <code>parJoin</code> to join multiple producer stream at the same time.</p>

<h2 id="more">More</h2>

<p>All the approaches above other than the first one uses a queue either implicitly or explicitly. However, under high parallelism and load, every job operating on a single queue may makes this queue a bottleneck. In this case, there is a <a href="https://en.wikipedia.org/wiki/Work_stealing">work stealing</a> algorithm that each consumers can has its own queue, and whenever a consumer’s queue is empty, it steal some tasks from another one. But it’s a little bit complex and unnecessary if the load is not so high, so I will not cover it in this article.</p>

<h2 id="test-results">Test Results</h2>

<p>Now let’s run all the approaches and compare the performance with <code>sbt "run -n"</code>. Here are the results:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>slow producer</th>
      <th>balanced</th>
      <th>slow consumer</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BatchIO</td>
      <td>11086.078637 ms</td>
      <td>29912.377578 ms</td>
      <td>10015.51878 ms</td>
    </tr>
    <tr>
      <td>BlockingQueue</td>
      <td>10190.038753 ms</td>
      <td>14195.228189 ms</td>
      <td>6495.333179 ms</td>
    </tr>
    <tr>
      <td>StreamQueue</td>
      <td>10138.016643 ms</td>
      <td>14458.443122 ms</td>
      <td>6418.078377 ms</td>
    </tr>
    <tr>
      <td>Stream</td>
      <td>10356.178562 ms</td>
      <td>15655.697826 ms</td>
      <td>6560.111697 ms</td>
    </tr>
    <tr>
      <td>PrefetchStream</td>
      <td>10141.110634 ms</td>
      <td>14578.362136 ms</td>
      <td>6376.628036 ms</td>
    </tr>
    <tr>
      <td>ConcurrentProduceresQueue</td>
      <td>5187.442452 ms</td>
      <td>14395.321922 ms</td>
      <td>6576.538821 ms</td>
    </tr>
    <tr>
      <td>ConcurrentProducer</td>
      <td>5198.723825 ms</td>
      <td>14544.247312 ms</td>
      <td>6418.078377 ms</td>
    </tr>
  </tbody>
</table>

<p>We can see approaches that parallelize all the parts win the performance game.</p>]]></content><author><name></name></author><category term="Scala" /><category term="concurrent" /><category term="cats" /><category term="cats-effect" /><category term="fs2" /><category term="queue" /><category term="stream" /><summary type="html"><![CDATA[All the source code mentioned in this blog can be found in my Github repo.]]></summary></entry><entry><title type="html">Upgrade Kubernetes from 1.23 to 1.24</title><link href="https://www.binwang.me/2023-05-22-Upgrade-Kubernetes-from-1.23-to-1.24.html" rel="alternate" type="text/html" title="Upgrade Kubernetes from 1.23 to 1.24" /><published>2023-05-22T00:00:00-04:00</published><updated>2023-05-22T00:00:00-04:00</updated><id>https://www.binwang.me/Upgrade-Kubernetes-from-1.23-to-1.24</id><content type="html" xml:base="https://www.binwang.me/2023-05-22-Upgrade-Kubernetes-from-1.23-to-1.24.html"><![CDATA[<p>In the <a href="/2023-03-13-Infrastructure-Setup-for-High-Availability.html">last blog post</a>, I introduced using Kubernetes to setup high available infrastructure. I had that setup a long time ago. I did the long overdue upgrade for Kubernetes from 1.23 to 1.24 recently. Since GlusterFS is <a href="https://github.com/kubernetes/kubernetes/pull/111485">deprecated</a>(though not removed) in 1.25, I have no plans to continue the upgrade without exploring alternative storage options.</p>

<p>There is a big change from 1.23 to 1.24 as well, namely, <a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">Docker Engine support has been removed</a>. I migrated the container engine to containerd. But the process is not without pain. I need to search different sources to fix the issues. So I list my upgrade steps so that if anyone has the same issue, this may help.</p>

<p>My Kubernetes cluster is set up locally with <code>kubeadm</code>. There is an <a href="https://v1-24.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">official upgrade guide</a> for kubeadm to upgrade from 1.23 to 1.24, but it doesn’t mention any steps to remove Docker and setup containerd. So here are the steps I took:</p>

<ol>
  <li>Add <code>--container-runtime-endpoint</code> option to kubelet. The way I did it is adding <code>KUBELET_ARGS="--container-runtime-endpoint=/run/containerd/containerd.sock"</code> to <code>/etc/kubernetes/kublet.env</code>. Without this, Kubelet will fail to start.</li>
  <li>Remove <code>--network-plugin=cni</code> from  <code>/var/lib/kubelet/kubeadm-flags.env</code>.</li>
  <li>Add the following configuration in <code>/etc/crictl.yaml</code>, otherwise kubeadm will not be able to pull needed images:
    <div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>runtime-endpoint: unix:///run/containerd/containerd.sock
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>image-endpoint: unix:///run/containerd/containerd.sock
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>timeout: 10
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>debug: false
</pre></div>
</div>
    </div>
  </li>
  <li>Configure <code>SystemdCgroup</code> permission for containerd. Otherwise kube-apiserver will always be restarted because of “sandbox environment changes” (see more in <a href="https://github.com/kubernetes/kubernetes/issues/110177">Github issue</a>):
    <div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>sudo mkdir -p /etc/containerd/
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>containerd config default | sudo tee /etc/containerd/config.toml
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>sudo sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>sudo systemctl restart containerd
</pre></div>
</div>
    </div>
  </li>
  <li>Follow the <a href="https://v1-24.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">official upgrade guide</a>.</li>
  <li>After the upgrade, remember to restart Docker so that the old containers started by Docker will be stopped.</li>
</ol>]]></content><author><name></name></author><category term="Kubernetes" /><category term="container" /><category term="Docker" /><category term="Linux" /><summary type="html"><![CDATA[In the last blog post, I introduced using Kubernetes to setup high available infrastructure. I had that setup a long time ago. I did the long overdue upgrade for Kubernetes from 1.23 to 1.24 recently. Since GlusterFS is deprecated(though not removed) in 1.25, I have no plans to continue the upgrade without exploring alternative storage options.]]></summary></entry><entry><title type="html">Infrastructure Setup for High Availability</title><link href="https://www.binwang.me/2023-03-13-Infrastructure-Setup-for-High-Availability.html" rel="alternate" type="text/html" title="Infrastructure Setup for High Availability" /><published>2023-03-13T00:00:00-04:00</published><updated>2023-03-13T00:00:00-04:00</updated><id>https://www.binwang.me/Infrastructure-Setup-for-High-Availability</id><content type="html" xml:base="https://www.binwang.me/2023-03-13-Infrastructure-Setup-for-High-Availability.html"><![CDATA[<p><em>See <a href="/2023-11-28-Introduce-K3s-CephFS-and-MetalLB-to-My-High-Avaliable-Cluster.html">Introduce K3s, CephFS and MetalLB to My High Avaliable Cluster</a> for updates on this setup.</em></p>

<p>Cloud is popular these days. But sometimes we just want to host something small, maybe just an open source service for family and friends, or some self-built service that we are still experimenting on. In this case, the cloud can be expensive. We can just throw a few nodes at home and run it at a very low cost. But you don’t want the service down when some nodes failed, at least the service should be available when you upgrade and reboot the nodes because it can happen very frequently. In this article, I will talk about how to build high available infrastructure so that the service can be alive even when some nodes are down.</p>

<h2 id="what-is-high-availability">What is High Availability?</h2>

<p>Availability means a service is alive and can serve traffic. High availability (HA) means when some components of the system are down, the service is still alive. The failed components can be in different layers: it can be a region, a DC, a network, or some nodes. In this article, I’ll only talk about things including and above node level, since region and network are usually out of control for a small infrastructure setup. That means you can host the service on multiple machines, and it should still be alive even when some of the machines are down. This is the most useful case of HA in the case anyway since nodes can be down frequently because of OS or software updates.</p>

<p>Before I start with the real setup, I want to clear some myths about HA first. Maybe you’ve heard of the famous CAP theorem, which says only two of the three properties can be met at the same time: consistency, availability, and partition tolerance. Lots of people misinterpret it as a HA system that will sacrifice consistency during a failure. It is not true: the type of partition that makes you must choose between consistency and availability is very rare. In most well-designed HA systems, you can have both as long as more than half of the nodes are alive (alive also means reachable from clients). And HA doesn’t necessarily mean it prioritizes availability over consistency either: it just means it can handle more failure cases when keeping both consistency and availability. When there is a failure it cannot handle, it can choose to keep consistency and make the service unavailable. This is the type of HA I’m going to introduce in this setup.</p>

<p>So to make it clear, the HA goal in this setup is to make the services still alive without sacrificing consistency when we lose less than half of the nodes (either it’s partitioned from the network or actually dead).</p>

<h2 id="ha-setup">HA Setup</h2>

<p>As said before, the HA needs multiple nodes in case of some nodes are down. The setup in this article can tolerate less than half of the node loss. So if you want to have a HA that can handle 1 node loss, the whole system needs at least 3 nodes. There are lots of cheap used machines on the market that have enough power to host many open source services.</p>

<p>The HA setup has multiple layers and we will use different tools for each of the layers:</p>

<ul>
  <li>Compute: Kubernetes</li>
  <li>Storage: GlusterFk</li>
  <li>Database: Cockroach DB</li>
  <li>Network Ingress: Cloudflare Tunnel</li>
</ul>

<p>Here is an overview of the setup:</p>

<p><img src="/static/images/2023-03-13-Infrastructure-Setup-for-High-Availability/HA-self-hosted.png" alt="overview" /></p>

<p><em>Update at 2023-11-28: updated the diagram to include Keepalived.</em></p>

<p>Let’s talk about each of them in detail.</p>

<h2 id="compute-kubernetes">Compute: Kubernetes</h2>

<p><a href="https://kubernetes.io/">Kubernetes</a> is a container orchestration system. Think of it as Docker but across machines. You define what you want to run in the format of YAML or Json, including how much CPU, memory, and storage to use, then Kubernetes will find a node that fits your needs to run your container. It also tries to keep the current system state that meets your definition. For example, if there is a node failed and your service’s container is on it, Kubernetes will try to find another node to start the container so that the state meets the definition. So if the service itself doesn’t have any state between restarts, you get HA for free using Kubernetes.</p>

<p>I must have some warnings about using Kubernetes. It’s a complex project that is used by many big players. It’s not very easy to set up, maintain or upgrade. You need lots of knowledge to make it work. It has so many open issues that your particular needs are most likely not prioritized. While it’s open source so that you can modify the code to meet your use case, and I’ve had good experience contributing code in the early days, the recent experience is not so good anymore: you may need to attend some discussions to push your change instead of async online discussion. That is a lot for a causal contributor. So I end up maintaining a custom branch of Kubernetes with the changes I need locally, which is also a lot for average users.</p>

<p>Even though Kubernetes is heavy, I still think it’s a good tool even for small deployments since it’s already the industry standard. If you want to dedicate the maintenance of the Kubernetes cluster to a third party in the future, you can find lots of providers very easily. And you can just migrate your services to a different cluster without much effort since you’ve already defined the deployment in a language that any Kubernetes cluster can understand.</p>

<p>If you decide Kubernetes is the way to go, Kubernetes can be deployed with Kubeadm. Here is the <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/">official document</a> about how to deploy a Kubernetes cluster. Make sure to finish the <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/">HA setup</a> so that Kubernetes can survive even if some nodes are down.</p>

<p>Because of the nature of multiple nodes, every time the service restarts, the container can end on a different machine. So if your service needs to store anything on a disk, the data can get lost if you use the local disk of the machine. There are a few solutions for this:</p>

<ul>
  <li>Just don’t store data on a disk:
    <ul>
      <li>Store it in a database instead. But this is not an option for a service we don’t write and control.</li>
      <li>Send the files to another system. For example, you can send all the logs to something like Elastic Search so that it’s acceptable to lose logs after the container is restarted.</li>
      <li>Use <a href="https://kubernetes.io/docs/concepts/configuration/configmap/">ConfigMaps</a> for configuration files so that you can access them on any machine.</li>
    </ul>
  </li>
  <li>Bind the service to a specific node so that the container will run on that machine all the time. This needs the service itself to have HA built in so that when that node fails, the containers on the other nodes can still serve traffic. We will see an example of this in Cockroach DB setup.</li>
  <li>The last option is to use a distributed storage system that can be accessed from every machine. We will use GlusterFS for it in this setup.</li>
</ul>

<h2 id="storage-glusterfs">Storage: GlusterFS</h2>

<p><strong>WARNING: Gluster integration for Kubernetes has been removed since Kubernetes 1.26. You can use CephFS instead. Or check <a href="https://github.com/kadalu/kadalu/">Kadulu</a> if you still want to use GlusterFS.</strong></p>

<p><a href="https://www.gluster.org/">Gluster</a> is a distributed storage system. Once you created a GlusterFS volume, you can mount it to a machine just like NFS. The difference is the volume is backed by multiple machines so if even one of the machines fails, the volume is still usable. Kubernetes could mount a GlusterFS volume for containers as well. Sadly, Kubernetes has removed this support since version 1.26. But I’ve had this setup for a while and is still using an older version of Kubernetes, so I’ll still list GlusterFS as a solution here. The documents are still available for older versions. <a href="https://v1-24.docs.kubernetes.io/docs/concepts/storage/volumes/#glusterfs">Here is an example for Kubernetes 1.24</a>. You can select “versions” on the upper right to match your Kubernetes installation. CephFS is another distributed storage system, but it’s less user-friendly than GlusterFS in my opinion since the setup is more complex and it’s harder to mount it locally and explore it like a normal Linux file system. <a href="https://github.com/kadalu/kadalu/">Kadulu</a> seems to be another option if you still want to use GlusterFS, but I’ve never used it and I’m not sure if it’s production ready or not.</p>

<p>See <a href="https://docs.gluster.org/en/latest/Install-Guide/Overview/#what-is-gluster-without-making-me-learn-an-extra-glossary-of-terminology">the official install guide</a> for how to install Gluster and set it up. Most of the Linux distros already have the Gluster in the repo so you can install it by the package manager, and configure it based on the official document. Be aware you need to reserve a separate partition just for Gluster.</p>

<p>When creating a Gluster volume for use with Kubernetes, make sure to create it with at least 3 replicas so that you have HA for this volume. One of the replicas can be “<a href="https://docs.gluster.org/en/v3/Administrator%20Guide/arbiter-volumes-and-quorum/">arbiter</a>”, which means it’s only used for checking consistency and doesn’t store any actual data. So the data is only duplicated across 2 machines instead of 3 to save some space. Here is an example command to create such a volume:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>sudo gluster volume create &lt;volume-name&gt; replica 3 arbiter 1 &lt;host1&gt;:&lt;glusterfs-path&gt; &lt;host2&gt;:&lt;glusterfs-path&gt; &lt;host3&gt;:&lt;glusterfs-path&gt;
</pre></div>
</div>
</div>

<h2 id="database-cockroachdb">Database: CockroachDB</h2>

<p>Even though we can make persistent work with distributed storage, it’s better to avoid it if possible because of the setup complexity and performance impact. (This is more of the case of a self-hosted solution, distributed storage from cloud providers is very easy to use, and is also used by the VM so there is no difference in performance). We’ve listed some options above. In this section, we will look at how to create a database for the services to use so that they don’t need to store data on disks.</p>

<p>Here I will use CockroachDB as an example. But this introduction should help you to set up other similar systems like Elastic Search. Cockroach DB is a distributed database that is compatible with PostgreSQL. It’s built with HA in mind, so it has good guarantees and is easy to set up. I’ve checked lots of HA solutions for PostgreSQL and all of them have less guarantee (lots of them have no information about the consistency and availability level they provide, and I found them half-baked with a closer look) while are much harder to set up. I’ve written <a href="/2018-07-29-A-Review-on-Spanner-and-Open-Source-Implementations.html">a blog about Spanner that also talks about Cockroach DB</a> if you are interested in more details. Overall I have a good impression of it: the tech writings are solid, and the support is nice: when I have an issue and report it in the forum, the response is usually very quick and useful even though I’m just a free user.</p>

<p>CockroachDB has <a href="https://www.cockroachlabs.com/docs/stable/kubernetes-overview.html">an official document</a> about how to install it on Kubernetes. It’s using <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a>. Here is <a href="https://raw.githubusercontent.com/cockroachdb/cockroach/master/cloud/kubernetes/cockroachdb-statefulset.yaml">one of the configurations it uses</a>. However, I still find there are too many limitations in StatefulSets so I deployed it in my own way:</p>

<ul>
  <li>Each CockroachDB instance is in its own StatefulSets with only 1 replica.</li>
  <li>Each of the instances is bound to the physical node with <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">PodAffinity</a>. So that each instance will only ever run on a specific host. In this way, we can just use the local disk as the storage because it will never run on a different host.</li>
  <li>Each CockroachDB instance has its own <a href="https://kubernetes.io/docs/concepts/services-networking/service/">service</a> defined so that they can communicate with each other.</li>
  <li>Copy the parameters from the official configuration and adjust them based on your use case.</li>
</ul>

<p>With a setup like this, it’s like installing CockroachDB on physical nodes but managed by Kubernetes. You don’t need to worry about distributed storage. When a node fails, a CockroachDB instance will also fail. But since CockroachDB itself has HA enabled, the whole CockroachDB cluster is still alive. Here is an example of the Kubernetes resources in my setup:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span># Pods
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>NAME                READY   STATUS    RESTARTS   AGE
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>pod/cockroach01-0   1/1     Running   1          4d11h
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>pod/cockroach02-0   1/1     Running   5          4d11h
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>pod/cockroach03-0   1/1     Running   0          4d11h
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span># StatefulSet
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>NAME          READY   AGE
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>cockroach01   1/1     298d
<span class="line-numbers"><a href="#n12" name="n12">12</a></span>cockroach02   1/1     298d
<span class="line-numbers"><a href="#n13" name="n13">13</a></span>cockroach03   1/1     298d
<span class="line-numbers"><a href="#n14" name="n14">14</a></span>
<span class="line-numbers"><a href="#n15" name="n15">15</a></span># Service
<span class="line-numbers"><a href="#n16" name="n16">16</a></span>
<span class="line-numbers"><a href="#n17" name="n17">17</a></span>NAME                              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                          AGE
<span class="line-numbers"><a href="#n18" name="n18">18</a></span>service/cockroach01-cockroachdb   ClusterIP   None           &lt;none&gt;        26257/TCP,8080/TCP               298d
<span class="line-numbers"><a href="#n19" name="n19">19</a></span>service/cockroach02-cockroachdb   ClusterIP   None           &lt;none&gt;        26257/TCP,8080/TCP               298d
<span class="line-numbers"><strong><a href="#n20" name="n20">20</a></strong></span>service/cockroach03-cockroachdb   ClusterIP   None           &lt;none&gt;        26257/TCP,8080/TCP               298d
<span class="line-numbers"><a href="#n21" name="n21">21</a></span>service/cockroachdb               ClusterIP   10.96.70.142   &lt;none&gt;        26257/TCP,8080/TCP               269d
<span class="line-numbers"><a href="#n22" name="n22">22</a></span>service/cockroachdb-public        NodePort    10.108.23.98   &lt;none&gt;        26257:30005/TCP,8080:30006/TCP   269d
</pre></div>
</div>
</div>

<p>You can see there is a separate StatefulSet for each of the CockroachDB instances, and a service for each of them for internal communications (with the name pattern <code>cockroach**-cockroachdb</code>). Service <code>cockroachdb</code> is for the use in Kubernetes cluster, and service <code>cockroachdb-public</code> is used by the service outside of the Kubernetes cluster (can be disabled if not needed) so that you can see the dashboard from your browser.</p>

<p>It may seem to have more Kubernetes definitions to write with such a method. But remember, while Kubernetes accepts YAML or Json format, how to prepare the definition can be flexible: you can use your favorite programming language to construct the definition and pass it to Kubernetes with a <a href="https://kubernetes.io/docs/reference/using-api/client-libraries/">client library</a>.</p>

<p>The upgrade of CockroachDB is very easy as well. Make sure to check the official release notes and upgrade guides first, but normally the upgrade is just to patch each of the StatefulSet with a newer version of Docker image, for example:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span># run this command for every stateful set
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>kubectl patch statefulset cockroach01 \
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>--type='json' \
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>-p='[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/image&quot;, &quot;value&quot;:&quot;cockroachdb/cockroach:v22.2.5&quot;}]'
</pre></div>
</div>
</div>

<h2 id="network-ingress">Network Ingress</h2>

<p>Once we have everything deployed in the cluster, the last step is to expose our service to the public Internet so that everyone can use it. Here we list two options based on the use case.</p>

<h3 id="cloudflare-tunnel">Cloudflare Tunnel</h3>

<p><a href="https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/">Cloudflare Tunnel</a> is basically a reverse proxy that forwards the traffic from the public Internet to your service. There is a daemon called cloudflared running in Kubernetes. Cloudflare will forward the traffic from clients to cloudflared and cloudflared will forward the traffic to the actual service. Check <a href="https://developers.cloudflare.com/cloudflare-one/tutorials/many-cfd-one-tunnel/">this doc</a> to see how it works with Kubernetes.</p>

<p>The upside of Cloudflare tunnel is that you don’t need to open any port to the public Internet at all. So it’s safer because there is no way to access your service without going to Cloudflare first. Cloudflare also provide some tools to mediate attacks like DDoS.</p>

<p>The downside is it depends on a third-party provider. And it can see all the traffic. It only supports limited protocols. So if you want to avoid Cloudflare seeing your traffic or have a protocol that is not supported, you need a more generic way to do it.</p>

<h3 id="nodeport-with-virtual-ip-and-dynamic-dns">NodePort with Virtual IP and Dynamic DNS</h3>

<p>We need to really open a port to the Internet without something like Cloudflare Tunnel. First, we need to open a port on our nodes, this can be done by defining <a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport">NodePort</a> in Kubernetes’ service.</p>

<p>Once we have the port opened on the nodes, we need to open it to the Internet. How to do it depends on the Internet provider. Usually, you should be able to set up a port mapping from the router to an internal IP for a node. However, to make the setup HA, we shouldn’t map the port just to a single node since that single node can be down, we can set up Keepalived so that there is a virtual IP that always maps to a live node. If you’ve <a href="https://github.com/kubernetes/kubeadm/blob/main/docs/ha-considerations.md#options-for-software-load-balancing">set up HA for Kubernetes with Keepalived and HAProxy</a>, you should be already familiar with how to set it up.</p>

<p>When you open a NodePort, make sure you’ve configured all the protections like authentication and encryption since beyond that it’s public Internet and anyone can access it.
You may want to run Nginx or HAProxy in the Kubernetes cluster, use it as a reverse proxy and only expose it to the Internet so that it’s safer and you have more control over the public traffic.</p>

<p>The client also needs a way to find the IP address of your network. Depending on the Internet provider, the IP address can change from time to time. So we need dynamic DNS to bind the changing IP to a fixed DNS. <a href="https://github.com/ddclient/ddclient">ddclient</a> can do it automatically and supports lots of domain name providers.</p>

<p>After all of this, your service is open to the public Internet and can be accessed by anyone. But if desired, you can still use Cloudflare DNS with proxy enabled, so that the client will send requests to Cloudflare first and you can get protections from Cloudflare. In this case, since the SSL is terminated inside the Kubernetes cluster, Cloudflare will not be able to see the actual payload of the traffic.</p>]]></content><author><name></name></author><category term="Kubernetes" /><category term="GlusterFS" /><category term="CockroachDB" /><category term="tech" /><category term="high availability" /><summary type="html"><![CDATA[See Introduce K3s, CephFS and MetalLB to My High Avaliable Cluster for updates on this setup.]]></summary></entry><entry><title type="html">My 2022 in Review</title><link href="https://www.binwang.me/2023-01-26-My-2022-in-Review.html" rel="alternate" type="text/html" title="My 2022 in Review" /><published>2023-01-26T00:00:00-05:00</published><updated>2023-01-26T00:00:00-05:00</updated><id>https://www.binwang.me/My-2022-in-Review</id><content type="html" xml:base="https://www.binwang.me/2023-01-26-My-2022-in-Review.html"><![CDATA[<p>It’s 2023 now! 2022 has ended. It has been 2 years since I wrote <a href="/2021-01-26-My-2020-in-Review.html">the last yearly review</a>. So this article would be more like 2021 and 2022 in review.</p>

<p>There are lots of things that happened in the last two years, if not more than the two years of 2019 and 2020. That includes both my personal life and work life, both individual level and national level. So let’s talk about them one by one.</p>

<p>My personal life has changed a lot in the past two years. My wife and I obtained permanent resident status in Canada in mid-2021, and bought our first home in Toronto. Those two things are a big relief to me: I lived in Beijing for more than 7 years since graduating from university, but I never saw it as a long-term home. Needless to say, the apartments in Beijing are too expensive. There are also lots of restrictions because of the Hukou system: Hukou basically means “citizenship” for a region in China. Without it, the biggest problem is the education of the children. While the children can enroll in schools in Beijing and be educated there, they still need to take Gaokao (the national college entrance examination) as a resident of my hometown, where their Hukou belongs (no, being born in Beijing wouldn’t get them a Beijing Hukou). The eligible scores for colleges vary by region, since the number of students admitted from each region is different and each region’s population is also different. In this case, good colleges usually admit much fewer students from my hometown province, even though the province has a much larger population. As a result, a much higher score is needed to be accepted by the same college. In my opinion, this puts the children into an unfair game since the education they received is not prepared for such a level of competition. (It’s already an unfair game for the students from regions like my hometown province, but for better or for worse, the education system there is prepared for it). While Gaokao is still the way for most Chinese to enroll in a college, there are some other options like studying aboard, which requires studying in a private school for preparation. But all the other paths need much more financial support. This is only the biggest problem, not to mention other inconveniences without Hukou like retirement or even buying a home or a car. In general, without it, you won’t be treated equally by the government, even though you’ve paid lots of tax and social insurance. You may ask, why not get one if it’s so important? Because it’s even harder than immigrating to another country. The only practical way other than marriage is to join a state-owned company or a government department just after graduating from college. Once you miss that opportunity, it’s nearly impossible for average people to get it.</p>

<p>Anyway, even if I can solve all the problems above, Beijing has changed in the last few years when I was there, to the level that I don’t want to live there. The government started to focus on Beijing’s role as China’s capital, aiming for a place like Washington. There is even a “one thousand years” plan to move non-capital functions to a small town called Xiong’an. However, Beijing is not a city that is built for the pure purpose of political capital from scratch. It has a long history, multiple functions, and diverse residents. But the average people are not a priority in the government’s decision. It started to ban commercial usage at the lower level of resident buildings (such commercial usage was encouraged during Beijing’s 2008 Olympic games to make the city more vibrant), demolish restaurants and shopping malls, ban street foods, and so on. Lots of historical buildings were refurbished with shiny walls and unified shop signs. The policy reached an extreme when it began to banish “low-end” people (the phrase “clean up low-end people” was literally used in official publications and slogans). During the winter of 2017, using a fire incident as an excuse, the government started a movement to demolish “slums”, and ban some of the rental rooms which have the proper licenses. The movement caused lots of people didn’t have a place to live during the cold winter. As a result, the city as a whole seems to be dying. While the parks were grander, the streets were also quieter, and more people were leaving. It’s so sad to see a city I lived and loved transferred to a place like that. But my view doesn’t represent the view of big brother. Cai Qi, the leader of Beijing at the time (the leader of a Chinese city is not the mayor, instead, it’s the head of that city’s communist party), used to be seen as an open-minded official because of his interaction with people on social media (which is very rare in China), a close comrade of Xi, entered the standing committee last year and became one of the most powerful political figures in China.</p>

<p>Another bigger event also happened in the same winter. It made me not only want to leave Beijing but also China itself. It was an evening with a beautiful sunset, I went for a walk with my girlfriend to Dong Zhi Men – East Straight Gate of ancient Beijing, a place with a combination of historical buildings, vibrant shopping malls, and sky crawlers. In front of a shopping mall, I saw a large billboard displaying news of amendments to China’s Constitution. The news was pure text with a large font size, which seemed out of place compared to the modern buildings nearby. It’s only a draft of amendments, but there is an important one that caught my eye: it removes the presidential term limits! I’ve never had high hopes for China’s democracy. When some people talk about the ingenuity of Deng Xiaoping’s design of skip-generation appointments for power transfer, I always think it’s childish. However, I never thought the transfer of power would break so quickly. In my opinion, it’s like the last barrier that prevents China from becoming a pure dictatorship country again. Once that barrier is broken, it would lead to a downfall in the foreseeable future. Even if it’s not the last barrier, the change shows Xi’s ambition to become a dictator in the most obvious way, which will take China in the wrong direction.</p>

<p>It was a cold winter day when the draft was passed. Living in a rented apartment, which is close to a military compound and just less than a 30-minute drive to the power center of China, I wrote down the following sentence after a sleepless night:</p>

<blockquote>
  <p>京城一夜风吹雪，万籁无声国岁寒。</p>
</blockquote>

<blockquote>
  <p>Beijing at night, wind and snow blowing,</p>

  <p>All creatures silent, the nation’s cold in growing.</p>
</blockquote>

<p>It seems weird to write about so many things that didn’t happen in the last two years. But they are important contexts to understand the significance of being a permanent resident of Canada. By no means Canada or Toronto is perfect. It has its own problems, but I’d rather live in a sociality where people can discuss the problems and hold the government accountable. So after moving to Toronto, I feel like this is a place that I can live and call home, and finally made it happen. Now, we’re having a baby on the way, which makes me excited and nervous at the same time.</p>

<p>Also because of the PR status, I was able to change the job without worrying about the visa. I left Amazon mainly because the on-call was too stressful to the point where it affected my personal life. I moved to a smaller company and started to do database related stuff again. The workload is much lighter than at Amazon, and I feel I have recovered a lot of energy because of it.</p>

<p>Outside of work, I continued to work on my side project which I mentioned in 2020’s year-end review. It’s now a usable product called <a href="https://rssbrain.com">RSS Brain</a>. It’s an RSS reader. I’ve mentioned it in past blogs so I will not go into details again. But it has been the app that I use the most every day. And it feels so good to write software that meets my own needs and maybe able to help others at the same time. I will eventually open source it, but before that, I still have some plans that need to be finished first. So open source the code became a lower priority. The development has been slowed down after most of the features have been finished: that’s a pattern of my past projects. But for the past projects it often slowed down before they were actually usable. At least for RSS Brain, it’s a usable app now. Hopefully, I still have the passion to continue developing it and finish all of my planned features.</p>

<p>While developing RSS Brain, I also developed and open sourced a <a href="/2022-05-02-A-Library-to-Make-It-Easier-to-Use-Scala-with-GRPC.html">Scala library</a> that makes writing gRPC service with Scala much easier. I’m very happy about the library because of its non-invasive nature. Even though I can see it’s controversial in some projects, I feel like it’s a very useful library that can help a lot in my future projects.</p>

<p>Another thing I want to review, which I value a lot, is this blog. In my opinion, ideas and the interaction of ideas are the most important factor for the advancement of human beings. The most important platforms that I can use to share ideas are open source projects and articles. Work is valuable, but I don’t think the work I’m doing can inspire lots of people because not a lot of people can see it. So I always treat this blog as an important platform for me to contribute to the world, even though it’s small, it shares ideas directly. Who knows if someone can get some inspiration from it, had some ideas on top of it then inspires someone else, and eventually create something revolutionary. In the past two years, I have posted a few articles. The frequency is neither too high nor too low. Quality is similar: neither too high nor too low overall. On some level, I’ve happy I left something, but at the same time, I feel like there is still a lot of room for improvement. There are some topics I wanted to write about but ended up not doing it. One factor is the RSS Brain project I was working on, but I still think there is still a lot of time I’ve wasted which can be used on writing articles.</p>

<p>Other than the output of knowledge and ideas, there is also the input part. One important source is reading books. My readings decreased a lot in the past two years in terms of both quantity and quality. Part of it may be because of working from home: I’ve read lots of books during my commute. I wasted too much time watching videos that didn’t even bring me relaxation or happiness in the end. I should be more aware of that and remember to find some books to read when I’m bored.</p>

<p>Other than the things that happened to me or around me, the events that happened in China also greatly affected me: the Covid restrictions became more and more extreme, preventing me from visiting my family for more than 3 years and greatly impacting the daily life of Chinese people. People were controlled by massive testing and surveillance in the guise of monitoring Covid cases. Cities implemented absurd lockdowns from time to time. The level of lockdown is so strict that lots of people were not able to get enough food or basic medicine. Some people were dead because of the lack of health care. Some people killed themselves because of depression. People are transferred into quarantine centers. Parents and kids were forcibly separated. The better quarantine centers are built from sports centers that have 24-hour overhead lights, the worse ones can be just outdoor parking lots or even public washrooms. During the transfer, some of them were packed onto trains without enough capacity, leading to people sleeping on luggage racks. There was even an incident that happened on a transfer bus, which killed more than 20 people. Government workers went into people’s homes to sterilize, which often destroys furniture and kills pets. Because of the lack of food and infrastructure, people fled lockdown regions on foot, walking tens of miles to airports and train stations, and in some cases, even hundreds of miles on the highway to their hometowns. Eventually, a fire broke out in Xinjiang and killed ten people, because they were locked into the building and were unable to escape. This tragedy sparked protests all around China. People held blank papers to protest without a word, but everyone, including the police and the government, knew what they wanted to say. So some of them were beaten and arrested. It’s like a dark Soviet Union joke but happened in real life China. Then, suddenly, all the Covid restrictions were lifted without any preparation. This led to a medicine shortage and a surge in Covid cases. Everyone I knew in China got Covid, except for those who stayed at home all the time. mRNA vaccines are still not approved because it’s not produced domestically. The strong government which controlled every aspect of people’s life during lockdowns are missing now, the cases and deaths are not even properly counted anymore. Everyone is left in the dark to fight for themselves.</p>

<p>There are so many tragedies that happened in China in the past two years that it’s impossible to write all of them down in this short article. Almost everyone’s life is affected by the bad economy if not by the Covid policies directly. Most of my friends and family members are in China, and my culture is also from there. Seeing the events unfold during the past two years has made me heartbroken. It also makes me frustrated because there is nothing much I can do. As the new year comes, hope everything can be better. And I’ll think more about what I can do, even if it’s just the smallest help.</p>]]></content><author><name></name></author><category term="life" /><summary type="html"><![CDATA[It’s 2023 now! 2022 has ended. It has been 2 years since I wrote the last yearly review. So this article would be more like 2021 and 2022 in review.]]></summary></entry><entry><title type="html">How RSS Brain Shows Related Articles</title><link href="https://www.binwang.me/2022-12-03-How-RSS-Brain-Shows-Related-Articles.html" rel="alternate" type="text/html" title="How RSS Brain Shows Related Articles" /><published>2022-12-03T00:00:00-05:00</published><updated>2022-12-03T00:00:00-05:00</updated><id>https://www.binwang.me/How-RSS-Brain-Shows-Related-Articles</id><content type="html" xml:base="https://www.binwang.me/2022-12-03-How-RSS-Brain-Shows-Related-Articles.html"><![CDATA[<p>In the new version of <a href="https://rssbrain.com">RSS Brain</a>, I added a new feature to show related articles from folders or feeds of your choice, instead of only show related articles from all feeds.</p>

<p>I have mentioned this feature in the <a href="/2022-10-29-RSS-Brain-Yet-Another-RSS-Reader-With-More-Features.html">previous blog post</a>. I also mentioned the algorithms in RSS Brain will be transparent. So in this article, I will talk about the details of this feature, how it can be used, the algorithm that backs it and how RSS Brain implements it.</p>

<h2 id="what-is-the-feature">What is the Feature</h2>

<p>The feature is very straightforward: when you are reading an article from a feed, RSS Brain will show related articles at the end. It’s a very common feature. What makes RSS Brain different are two things:</p>

<ol>
  <li>You can configure where the related articles come from.</li>
  <li>The implementation is transparent. That includes both the algorithm, which I will introduce in this article, and the code, which I will open source it in the future.</li>
</ol>

<p>How do you configure the related articles? By default, there will be no recommended articles. But there is a button to let you add a recommendation section at the end of an article:</p>

<p><img src="/static/images/2022-11-27-How-RSS-Brain-Show-Related-Articles/screenshot_add_section.png" alt="screenshot-add-section" /></p>

<p>Once you click the “Add More” button, it will show all your folders and feeds, with another “All Subscriptions” option. If you select “All Subscriptions”, this recommendation section will find related articles from all your subscriptions. If you select a folder or feed, it will find them from the folder or feed of your choice.</p>

<p>You can add multiple recommendation sections. After that, each recommendation section will be shown after the article. The screenshot below is an example that has a section that shows related articles from a folder called “local-form”, and another section that shows related articles from all the user’s subscriptions.</p>

<p><img src="https://rssbrain.com/images/screenshot_multi_recommend.png" alt="screenshot-sections" /></p>

<p>The recommendation configuration is attached to the feed that this article belongs. So if you read another article from the same feed, it will show related articles from the same recommendation sections.</p>

<h2 id="why-the-feature-is-useful">Why the Feature is Useful</h2>

<p>The first way I use it is to find more discussions about this article. For example, you can configure it to show related articles from Hacker News, some Twitter account or from a sub Reddit. So that you know what other people think about this article, or about this topic.</p>

<p>Another way to use it is to check the coverage from different sources. For example, you can add a recommendation section that contains left wing media and another section that has right wing media, so that you can compare the coverage and get a whole picture.</p>

<p>Last but not least, the recommendation is useful in its traditional way: just show related articles about the same topic so that you can read more details about the same topic. I often just read folders that has high quality sources, and when I want to know more, I will add a recommendation section that has more sources and find articles to read from there.</p>

<h2 id="how-the-feature-is-implemented">How the Feature is Implemented</h2>

<p><em>Update: see <a href="/2023-11-14-Update-On-RSS-Brain-to-Find-Related-Articles-with-Machine-Learning.html">this blog</a> for updated algorithm.</em></p>

<p>The algorithm to find recommended feature is content based instead of user based. RSS Brain doesn’t collect any user’s information in order to make personalized recommendation. It just find the related articles by how similar they are.</p>

<p>Each of the article can be represented by a term vector. The values in this vector are scores of the terms. For example, if article A has the content of “apple boy cat” and article B has the content of “apple boy dog”, the term vectors for each of the articles can be:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>   apple, boy, cat, dog
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>A: [0.5,  0.5,   1,   0]
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>B: [0.5,  0.5,   0,   1]
</pre></div>
</div>
</div>

<p>The score is computed by <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">tf-idf</a>, which is basically a score considers both the frequent of the term in this article, and the frequent in all the articles: the more frequent it is in this article, the bigger the score since it can better represent the article. However the more frequent it is in all the articles, the score should be smaller since it’s not unique enough to represent the feature of this article. Once we have a term vector for each of the article, we can find the similarity by counting the distance between these vectors.</p>

<p>So we have the algorithm, how RSS Brain implements it in the code? We are using <a href="https://www.elastic.co/">ElasticSearch</a> under the hood. It’s widely used and open source. So for the APIs that RSS Brain is using, you can check the code for implementation details if you want. It has <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-termvectors.html">an API to find the term vector</a> for an article, and we use the scores in the term vector to do a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-boosting-query.html">boosting query</a>, which means do a search with different weights for each query term. For example, in the example above, if we want to find related articles for article A, we would find its term vector first, then convert the term vector into a boosting query that searches related articles by the query <code>apple^0.5 boy^0.5 cat^1</code>.</p>

<p>There are more details in this implementation, like adding filter on feed or folder in the query, limiting the term vector size and so on. The details can be found in the code once it’s open sourced. But the main idea doesn’t change.</p>

<p>With this simple and content based recommendation algorithm, instead of letting a black box AI decides what content are shown to you, I believe users can understand why an article is recommended, and judge whether the recommendation can benefit them or not.</p>]]></content><author><name></name></author><category term="RSS" /><category term="project" /><category term="RSS Brain" /><category term="digital life" /><summary type="html"><![CDATA[In the new version of RSS Brain, I added a new feature to show related articles from folders or feeds of your choice, instead of only show related articles from all feeds.]]></summary></entry><entry><title type="html">RSS Brain: Yet Another RSS Reader, With More Features</title><link href="https://www.binwang.me/2022-10-29-RSS-Brain-Yet-Another-RSS-Reader-With-More-Features.html" rel="alternate" type="text/html" title="RSS Brain: Yet Another RSS Reader, With More Features" /><published>2022-10-29T00:00:00-04:00</published><updated>2022-10-29T00:00:00-04:00</updated><id>https://www.binwang.me/RSS-Brain-Yet-Another-RSS-Reader-With-More-Features</id><content type="html" xml:base="https://www.binwang.me/2022-10-29-RSS-Brain-Yet-Another-RSS-Reader-With-More-Features.html"><![CDATA[<p>I’ve written yet another RSS Reader called RSS Brain recently. If you just want to take a quick look at the main features and try it, <a href="https://rssbrain.com/">the official website</a> is the best place to start. Though most of the main features are already finished and I’ve used it personally for a few months, be aware it’s still in beta stage and hasn’t been tested by a larger group yet. So please let me know if you run into any issues. I will open source it as well in the future (for non commercial usage) but it’s not ready yet for now.</p>

<p>In this article, I’ll give an introduction to the motivation behind it. You may have a better reason to try it after read this.</p>

<p>RSS is just a protocol to aggregate news. But how to organize them depends on the RSS reader.  While lots of RSS readers have the basic organization features like folders to manage the feeds, it is not enough. Not a lot of them have taken new technologies from fields like information retrieval and machine learning. Even a few them have done that, they are using some complex and black box algorithms which we don’t know what is going on behind the sense. I’ve written <a href="/2020-08-02-What-Is-Wrong-abount-Recommendation-System.html">an article</a> that talks about the harm of letting black box algorithm decide what we read. So in RSS Brain, I use transparent algorithms to help organize the feeds and articles without use clicks or read time as the optimization target, so that we can have a good understanding about why it behaves in that way, in that way we can decide whether it’s good for us or not.</p>

<p>Here are some pain points in the traditional RSS readers and how I solve them in RSS Brain with transparent algorithms.</p>

<h2 id="1-traditional-rss-readers-dont-rank-articles-by-weight">1. Traditional RSS Readers Don’t Rank Articles by Weight</h2>

<p>The mainstream RSS protocol don’t have a field to indicate how important an article is in a feed. It is very different from news papers or news websites, which have head line for the most important news. When it comes to RSS, there is no difference between the importance of the articles. It is a smaller problem when there are not so many articles in a feed, but for feeds that have lots of articles, or forums like Reddit and Hacker News, that is a very big problem.</p>

<p>Reddit and Hacker News sort the articles by both votes and timeline. The algorithm is relatively transparent (although less and less transparent in the case of Reddit). Some people don’t want the rank to be effected by other users at all, but I’m okay for other this kind of ranking for these reasons:</p>

<ol>
  <li>The posts are just too many to read them all without some kind of priority.</li>
  <li>The community is part of the forum. If I like a subreddit, it means I trust the community and mods to promote high quality posts. Otherwise I will just not subscribe to it.</li>
  <li>If you think about it, the traditional media also rank news for you, even khough it is selected by more professional people. But like I said in point 2, if you trust the community of a subreddit, then it doesn’t make a big difference.</li>
</ol>

<h3 id="11-ranking-algorithm">1.1 Ranking Algorithm</h3>

<p>Some readers have ranking algorithms to sort the articles for you, but those are mostly black box algorithms, which is harmful like I said before. In RSS Brain, we will take the votes from the source, and sort it with an algorithm that is similar to Reddit:</p>

\[S_{vote} = log_{10}v\]

\[S_{time} = { time \over C }\]

\[S = S_{vote} + S_{time}\]

<p>Which <code>v</code> means how many votes this article has. <code>time</code> means when this article is posted. <code>C</code> is a constant number that indicate how much wait time contributes to the whole score. I’m using 12.5 hours in my implementation.</p>

<p>For \(S_{vote}\), it means the first 10 votes will get the most weights, the next 100 votes has the same weight as the first 10 votes, and so on. It’s not a perfect algorithm but works good enough, and is easy to implement. Most importantly it’s very easy to be understood.</p>

<p>Once you have the score, you have the option to sort the articles in a folder or feed based on score instead of time.</p>

<p><img src="https://www.rssbrain.com/images/sort_post.png" alt="ranking" /></p>

<h3 id="12-data-source">1.2 Data Source</h3>

<p>Another obvious problem is how to get the votes. The traditional RSS protocols don’t have a specific field for vote count, luckily atom protocol has the ability to extend it with custom tags. So RSS Brain will parse these tags in <code>&lt;entry&gt;</code> tag if exists: <code>&lt;*:comments&gt;</code>, <code>&lt;*:upvotes&gt;</code>, <code>&lt;*:downvotes&gt;</code>. <code>*</code> can be any namespace. I’ve added <a href="https://docs.rsshub.app/en/joinus/quick-start.html#submit-new-rss-rule-code-the-script-produce-rss-feed-interactions">these fields</a> to a very popular RSS generator <a href="https://rsshub.app/">RSSHub</a> in namespace <code>RSSHub</code>. So if someone include these fields when implement a RSS, RSS Brain will be able to parse it and use <code>upvotes - downvotes</code> as the votes. If they are not available, RSS Brain will try to use <code>comments</code> instead.</p>

<p>Reddit and HackerNews are two of my main daily reading websites, and I believe it’s the same for many other people, so I also included an implementation to fetch the posts from Reddit and HackerNews JSON API. You can just input HackerNews or subreddit URL when add a new source, it will has an option to use the JSON feed when it tries to find the feeds. I know it’s not a very standard way, but it’s easier for me to implement rather than do it in RSSHub. Once there is a RSS implementation that contains the fields above, you have no trouble to use the RSS feed.</p>

<p>Since RSS Brain is parsing the tags in the RSS feed, for data sources other than forum, the RSS generator can also try to generate the votes in some way if necessary. For example, some score based on the article position of the website, the font size and so on. I haven’t done any experiment with it yet but I think it’s an interesting idea to explore.</p>

<h2 id="2-filter-articles-with-search-terms">2. Filter Articles With Search Terms</h2>

<p>Ranking the articles is one way to get the interested aritcles pop up. Another way is to filter the articles: sometimes we only care about a topic in a feed. In traditional RSS readers, there is no easy way to filter the topic out if there is no feed for that topic. In RSS Brain, you can define a search term on a folder, so that when you check the articles in this folder, it will only show the articles that matches the search term.</p>

<p>For example, you’ve got a few news feed, but only care reports about the war between Ukraine and Russia, then you can just set the search term as <code>"Ukraine" AND "Russia"</code> on the folder and enable search filter. After that, when you click on the folder to see articles, it will only show the articles about those news.</p>

<p><img src="https://www.rssbrain.com/images/filter_folder.png" alt="filter folder" /></p>

<p>By the way, you can also just search in a folder or source. This is very useful for me to search some news, since the search quality of search engineers for some news is very bad, especially for Chinese news, which very suspicious news agencies are always shown in the top results.</p>

<h2 id="3-show-related-articles">3. Show Related Articles</h2>

<p>This is a feature I found to be pretty interesting while Google News included it. But Google News selects the news source for you. In RSS Brain, you get get related articles from the feeds you subscribed, so that you can decide which sources are valuable to you instead of letting a big corp decide that.</p>

<p>Currently the implementation only has the ability to select from all your subscribed feeds. But even with this implementation, I find it is very useful in some ways:</p>

<ul>
  <li>When I check the news, the related articles will show a post related to this in forums like Reddit and HackerNews, so that I can check other people’s opinion.</li>
  <li>I can compare the report coverage from both left and right news agencies, to get a whole picture. I don’t use this feature a lot myself, but I know a lot of people like that and there are some popular apps do just that.</li>
  <li>I usually read news from a high quality source. But if I find some news to be interesting, I’d like to read more coverage on that. Those coverage doesn’t need to be high quality but might have more details.</li>
</ul>

<p>I have plan to extend this feature. (<em>Update: this feature has been implemented. Check the <a href="/2022-12-03-How-RSS-Brain-Shows-Related-Articles.html">blog post here</a>.</em>) It will be much better after that: RSS Brain will support related articles groups, so that you can show related articles not only from all the subscriptions, but also from the folders of your choice. So you can configure it to show related forum posts in one group, left/right news coverage on another group, and so on. I’ll also implement a feature to show related articles from a time range, so that it can filter the matches based on time, to make the news more related if you want.</p>

<h2 id="4-just-want-to-write-something">4. Just Want to Write Something</h2>

<p>Last but not least, I just want to write something and feel the happiness of coding. I really enjoy myself a lot while writing this project: coding, choosing the tech stack to use, setting up high availability cluster and database, and so on. Everything is so elegant because I can decide how to do it.</p>

<p>While enjoy myself during the development of the project, I find the product is pretty useful as well. So I decided to share it, and if it benefits more people I’m happier. As I said at the beginning of the article, I’ll open source the whole project and allow non commercial usages when it’s ready. But at the mean time, you can pay a monthly fee to use the software. The reason of this payment mode is two fold: I don’t want this software to be flooded with terrible ads, but I still need some revenue to keep the infrastructure running (for both hardware cost and my time). So I want to set some barriers at first to limit the user base, so that the users can have a better experience. Hope you enjoy this app and let me know if you run into any issues or have any question.</p>]]></content><author><name></name></author><category term="RSS" /><category term="project" /><category term="RSS Brain" /><category term="digital life" /><category term="news" /><summary type="html"><![CDATA[I’ve written yet another RSS Reader called RSS Brain recently. If you just want to take a quick look at the main features and try it, the official website is the best place to start. Though most of the main features are already finished and I’ve used it personally for a few months, be aware it’s still in beta stage and hasn’t been tested by a larger group yet. So please let me know if you run into any issues. I will open source it as well in the future (for non commercial usage) but it’s not ready yet for now.]]></summary></entry><entry><title type="html">Handle Apple In-App-Purchase Server Notification with Scala/Java</title><link href="https://www.binwang.me/2022-08-20-Apple-In-App-Purchase-Server-to-Server-Notification-Handling-with-Scala-Java.html" rel="alternate" type="text/html" title="Handle Apple In-App-Purchase Server Notification with Scala/Java" /><published>2022-08-20T00:00:00-04:00</published><updated>2022-08-20T00:00:00-04:00</updated><id>https://www.binwang.me/Apple-In-App-Purchase-Server-to-Server-Notification-Handling-with-Scala-Java</id><content type="html" xml:base="https://www.binwang.me/2022-08-20-Apple-In-App-Purchase-Server-to-Server-Notification-Handling-with-Scala-Java.html"><![CDATA[<p>When you write an app for iOS, publish it to Apple App Store and want to sell something within it, Apple makes it mandatory to use its own in app purchase framework for non consumable items and subscriptions. If the app has a server, it’s very usual that the server wants to know the payment events and have some followup logic with them. But how to do that? There is always an option to let the app send a request to server, but anyone can use the same endpoint to make false claims. To prevent this, Apple has a server to server notification mechanism: Instead the app itself, a server from Apple will send a request to your server to notify the payment events. Since the message is signed by Apple, you can make sure no one else can fake it by verifying the signature.</p>

<p>While this framework should in theory makes developer’s life easier, the lack of documentation makes it very painful to use. There is also little and often wrong information on the Internet about how to verify the signature, especially for a server written with Java related tech stack. So in this article, I will give an example about how to decode and verify the payment notification messages sent by Apple with Scala. The library we are using is <a href="https://connect2id.com/products/nimbus-jose-jwt">Nimbus JOSE + JWT</a> which is written in Java, so the method applies to other JVM languages as well. Hopefully this can help other developers who are facing the same problem.</p>

<h2 id="1-how-to-trigger-a-server-notification">1. How to Trigger a Server Notification</h2>

<p>Needless to say, to receive a payment notification you must initiate a payment from the app. There are various ways to do it and we will not discuss it in this article. But be aware there are two ways to test the in app payment: create a <a href="https://developer.apple.com/documentation/xcode/setting-up-storekit-testing-in-xcode">StoreKit configuration in Xcode</a> or <a href="https://developer.apple.com/documentation/storekit/in-app_purchase/testing_in-app_purchases_with_sandbox">use a sandbox environment</a>. Only the later one will trigger a server to server notification.</p>

<p>Another requirement is to set up the notification endpoint in Apple Connection settings. The endpoint is a https URL that Apple will send a http post request to. <a href="https://developer.apple.com/documentation/storekit/in-app_purchase/original_api_for_in-app_purchase/subscriptions_and_offers/enabling_app_store_server_notifications">Here</a> is the Apple document about how to do it.</p>

<h2 id="2-server-notification-workflow">2. Server Notification Workflow</h2>

<p>In order to better understand how to handle the notification message, let’s take a look at the server notification workflow first.</p>

<p><img src="/static/images/2022-08-20-Apple-In-App-Purchase-Server-to-Server-Notification-Handling-with-Scala-Java/iap-server-notification.png" alt="iap-server-notification" /></p>

<p>When Apple receives some payment information, whether it’s from the app, or from subscription renew, or subscription expiration or other events, it will try to send the event to your server by sending an http POST request. In order to make sure no other people can fake a request to the same http endpoint, Apple signs the request payload with a private key that only Apple has access, so that when your server received the message, you can verify it by Apple’s public key.</p>

<p>The way Apple signs the message is using a standard called <a href="https://www.rfc-editor.org/rfc/rfc7515.html">JWS</a>. This is a complex standard with multiple implementation options. To fully understand it you also need to know things like <a href="https://www.rfc-editor.org/rfc/rfc7518.html">JWA</a> and <a href="https://www.rfc-editor.org/rfc/rfc7517">JWK</a>. Apple has very little document about how to decode its own message other than throw this RFC page into the document. Even in support forums, their response is like “use your favourite crypto library”, which doesn’t really help anything.</p>

<h3 id="3-jws-overview">3. JWS Overview</h3>

<p>To make it easy, I will give a very simple overview of JWS, JWS has three parts: a header that contains metadata like keys and algorithm to use, the actual payload, and a signature:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>header (metadata)
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>-----------
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>payload (actual message we want, base64 encoded JSON)
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>-----------
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>signature (generated by applying crypto algrithm on payload with keys in header)
</pre></div>
</div>
</div>

<p>So in order to make sure the whole message is actually sent by Apple, we need to verify:</p>

<ul>
  <li>The signature is generated by the keys in header and the payload.</li>
  <li>The keys in header is generated by Apple.</li>
</ul>

<p>Since the payload is only base64 encoded, for a developer that is not familiar with JWS, even with the help of a JWS library, both verification steps can be easily missed since it only affects the verification, not the decoding of the message.</p>

<p>In the section next, we will have an example about how to decode the message while really verify the message is sent by Apple as well.</p>

<h2 id="4-decode-and-verify-notification">4. Decode and Verify Notification</h2>

<p>Here we are using <a href="https://developer.apple.com/documentation/appstoreservernotifications/app_store_server_notifications_v2">App Store server notifications v2</a>. Let’s say you’ve already got the http POST body from your configured endpoint:</p>

<div class="language-Scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>val responseBodyV2Str: String = ... // your code to get POST body from HTTP request
</pre></div>
</div>
</div>

<p><code>responseBodyV2Str</code> itself is not a JWS object but a JSON string like this:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>{&quot;signedPayload&quot;:&quot;eyJhbGciOiJFUzI1NiIsIng1YyI6WyJNSUlFTU....&quot;}
</pre></div>
</div>
</div>

<p>The value of <code>signedPayload</code> is the encoded JWS string we want to parse. So we need to get that value first. I’m using <a href="https://circe.github.io/circe/">circe</a> to parse the JSON string, but any method that can parse it and get the value is fine. In my case, I defined some classes based on the JSON structures so that we can parse them in a more type safe way.</p>

<div class="language-Scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>import io.circe.generic.extras._
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>import io.circe.parser
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>object ApplePaymentService {
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>  implicit val config: Configuration = Configuration.default.withDefaults
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>  @ConfiguredJsonCodec case class AppleResponseBodyV2(
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>    signedPayload: String,
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>  )
<span class="line-numbers"><a href="#n12" name="n12">12</a></span>
<span class="line-numbers"><a href="#n13" name="n13">13</a></span>  @ConfiguredJsonCodec case class AppleResponseBodyV2DecodedPayload(
<span class="line-numbers"><a href="#n14" name="n14">14</a></span>    notificationType: String,
<span class="line-numbers"><a href="#n15" name="n15">15</a></span>    subtype: Option[String],
<span class="line-numbers"><a href="#n16" name="n16">16</a></span>    data: ApplePayloadData,
<span class="line-numbers"><a href="#n17" name="n17">17</a></span>  )
<span class="line-numbers"><a href="#n18" name="n18">18</a></span>
<span class="line-numbers"><a href="#n19" name="n19">19</a></span>  @ConfiguredJsonCodec case class ApplePayloadData(
<span class="line-numbers"><strong><a href="#n20" name="n20">20</a></strong></span>    appAppleId: Option[String],
<span class="line-numbers"><a href="#n21" name="n21">21</a></span>    bundleId: String,
<span class="line-numbers"><a href="#n22" name="n22">22</a></span>    bundleVersion: String,
<span class="line-numbers"><a href="#n23" name="n23">23</a></span>    environment: String,
<span class="line-numbers"><a href="#n24" name="n24">24</a></span>    signedRenewalInfo: String,
<span class="line-numbers"><a href="#n25" name="n25">25</a></span>    signedTransactionInfo: String,
<span class="line-numbers"><a href="#n26" name="n26">26</a></span>  )
<span class="line-numbers"><a href="#n27" name="n27">27</a></span>
<span class="line-numbers"><a href="#n28" name="n28">28</a></span>  @ConfiguredJsonCodec case class AppleJWSTransactionDecodedPayload(
<span class="line-numbers"><a href="#n29" name="n29">29</a></span>    appAccountToken: String,
<span class="line-numbers"><strong><a href="#n30" name="n30">30</a></strong></span>    bundleId: String,
<span class="line-numbers"><a href="#n31" name="n31">31</a></span>    environment: String,
<span class="line-numbers"><a href="#n32" name="n32">32</a></span>    expiresDate: Long, // timestamp in ms
<span class="line-numbers"><a href="#n33" name="n33">33</a></span>    inAppOwnershipType: String,
<span class="line-numbers"><a href="#n34" name="n34">34</a></span>    originalPurchaseDate: Long, // timestamp in ms
<span class="line-numbers"><a href="#n35" name="n35">35</a></span>    originalTransactionId: String,
<span class="line-numbers"><a href="#n36" name="n36">36</a></span>    productId: String,
<span class="line-numbers"><a href="#n37" name="n37">37</a></span>    purchaseDate: Long, // timestamp in ms
<span class="line-numbers"><a href="#n38" name="n38">38</a></span>    quantity: Int,
<span class="line-numbers"><a href="#n39" name="n39">39</a></span>    transactionId: String,
<span class="line-numbers"><strong><a href="#n40" name="n40">40</a></strong></span>    `type`: String,
<span class="line-numbers"><a href="#n41" name="n41">41</a></span>    webOrderLineItemId: String,
<span class="line-numbers"><a href="#n42" name="n42">42</a></span>  )
<span class="line-numbers"><a href="#n43" name="n43">43</a></span>}
</pre></div>
</div>
</div>

<p>With the help with the classes and JSON parser, we can get the value of <code>signedPayload</code>. (I removed <code>\n</code> from the JSON string since it’s not valid to have newlines in JSON string, not sure why Apple’s request body has newline in it):</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>val responseBodyV2 = parser.parse(responseBodyV2Str.replace(&quot;\n&quot;, &quot;&quot;)).flatMap(_.as[AppleResponseBodyV2]).toTry.get
</pre></div>
</div>
</div>

<p>After get the JWS string, we can parse it with <a href="https://connect2id.com/products/nimbus-jose-jwt/examples">Numbus Jose + JWT</a> (follow the document to add this dependency into your project first):</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>import com.nimbusds.jose.JWSObject
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>import com.nimbusds.jose.crypto.ECDSAVerifier
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>import com.nimbusds.jose.crypto.bc.BouncyCastleProviderSingleton
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>import com.nimbusds.jose.jwk.ECKey
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>import com.nimbusds.jose.util.X509CertUtils
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>val jwsObject = JWSObject.parse(responseBodyV2.signedPayload)
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>val jwsCerts = jwsObject.getHeader.getX509CertChain.asScala.map(c =&gt; X509CertUtils.parse(c.decode()))
</pre></div>
</div>
</div>

<h3 id="41-verify-keys-in-jws-header-is-signed-by-apple">4.1 Verify keys in JWS header is signed by Apple</h3>

<p><code>jwsCerts</code> is a list of <code>X509Certificate</code>, which is the key chain in JWS header. A cert in the list can be verified by the cert behind it. And the last cert should be verified by Apple’s public key so that we can make sure the whole key chain is signed by Apple.</p>

<p>So let’s first get the root cert of Apple first: download <a href="https://www.apple.com/certificateauthority/AppleRootCA-G3.cer">Apple Root CA - G3 Root</a> from <a href="https://www.apple.com/certificateauthority/">Apple PKI website</a> and put it under your project’s <code>src/resources/certs</code> (or any where the program can read, we are just using it as an example here). Then we can read the Apple root cert with this code:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>val appleRootCa = X509CertUtils.parse(getClass.getResourceAsStream(&quot;/certs/AppleRootCA-G3.cer&quot;).readAllBytes())
</pre></div>
</div>
</div>

<p>With both the key chain and root cert, we can verify the key chain is both valid and signed by Apple:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>jwsCerts.sliding(2).foreach { x =&gt;
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>  x.head.verify(x.last.getPublicKey)
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>}
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>jwsCerts.last.verify(appleRootCa.getPublicKey)
</pre></div>
</div>
</div>

<p>It will throw exception if the verify doesn’t pass.</p>

<h3 id="42-verify-jws-is-signed-by-keys-in-jws-header">4.2 Verify JWS is signed by keys in JWS header</h3>

<p>Once we verified the keys in JWS header is signed by Apple, we need to verify JWS itself is signed by these keys. Since the <code>alg</code> field in this JWS header is <code>ES256</code>, we will use <code>ECDSAVerifier</code> to verify it:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>val jwk = ECKey.parse(jwsCerts.head)
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>val jwsVerifier = new ECDSAVerifier(jwk)
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>if (!jwsObject.verify(jwsVerifier)) {
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>  throw new RuntimeException(&quot;Apple JWS object cannot be verified&quot;)
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>}
</pre></div>
</div>
</div>

<h3 id="43-parse-the-payload">4.3 Parse the payload</h3>

<p>After verify the JWS is valid, we can start to parse the payload. I’m using the JSON parser and the structure I defined above. Please refer to Apple’s document about the actual fields in the payload:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>val responseBodyV2Payload = jwsObject.getPayload.toString
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>val responseBodyV2DecodedPayload = parser.parse(responseBodyV2Payload).flatMap(_.as[AppleResponseBodyV2DecodedPayload]).toTry.get
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>val transactionPayload = responseBodyV2DecodedPayload.data.signedTransactionInfo
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>val transactionDecodedPayloadStr = JWSObject.parse(transactionPayload).getPayload.toString
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>val transactionDecodedPayload = parser.parse(transactionDecodedPayloadStr).flatMap(_.as[AppleJWSTransactionDecodedPayload]).toTry.get
</pre></div>
</div>
</div>

<p>Here we have the detailed transaction information in <code>transactionDecodedPayload</code> and can hand it with our business logic.</p>

<p>There is an interesting thing: <code>signedRenewInfo</code> and <code>signedTrasactionInfo</code> are both encoded with JWS again in payload data. I don’t know why: since we’ve already verified the whole payload is signed by Apple, all the content in it should already be valid as well, what’s the point to sign the fields again? I just decoded the fields with <code>JWSObject.parse</code> but you can always verify it with the same method above just to be safe.</p>

<h2 id="5-other-thoughts">5. Other Thoughts</h2>

<p>As I said about a <a href="/2020-11-08-DNS-Resolving-Bug-in-iOS-14.html">previous blog about an iOS bug</a>, I really hate Apple’s close ecosystem. But Apple’s hardware is good and has a large user space, so we cannot avoid it. Hopefully Android can be better at permission management and other mobile OS can also catch up.</p>]]></content><author><name></name></author><category term="iOS" /><category term="Apple" /><category term="In App Purchase" /><category term="Scala" /><category term="Java" /><category term="Programming" /><summary type="html"><![CDATA[When you write an app for iOS, publish it to Apple App Store and want to sell something within it, Apple makes it mandatory to use its own in app purchase framework for non consumable items and subscriptions. If the app has a server, it’s very usual that the server wants to know the payment events and have some followup logic with them. But how to do that? There is always an option to let the app send a request to server, but anyone can use the same endpoint to make false claims. To prevent this, Apple has a server to server notification mechanism: Instead the app itself, a server from Apple will send a request to your server to notify the payment events. Since the message is signed by Apple, you can make sure no one else can fake it by verifying the signature.]]></summary></entry><entry><title type="html">A Library to Make It Easier to Use Scala with gRPC</title><link href="https://www.binwang.me/2022-05-02-A-Library-to-Make-It-Easier-to-Use-Scala-with-GRPC.html" rel="alternate" type="text/html" title="A Library to Make It Easier to Use Scala with gRPC" /><published>2022-05-02T00:00:00-04:00</published><updated>2022-05-02T00:00:00-04:00</updated><id>https://www.binwang.me/A-Library-to-Make-It-Easier-to-Use-Scala-with-GRPC</id><content type="html" xml:base="https://www.binwang.me/2022-05-02-A-Library-to-Make-It-Easier-to-Use-Scala-with-GRPC.html"><![CDATA[<p><em>This article describes why I created the library <a href="https://github.com/wb14123/scala2grpc">Scala2grpc</a>.</em></p>

<p><a href="https://grpc.io/">gPRC</a> is a Remote Procedure Call (RPC) framework made by Google. It uses a domain specific language (DSL) to define the APIs, and provides tools for lots of languages to generate code for both servers and clients. The generated code includes models and API interfaces. The developer can create a gRPC server by implementing the generated interfaces. There are lots of examples in the official document so I’ll not spend more time on the details.</p>

<p>It has lots of advantages compared to traditional HTTP APIs that encode payloads as JSON or XML. Just to name a few: it’s type safe so there are less places to make errors; it has a schema so causes less confusing when communicate APIs between developers; it’s more efficient on both serialization and translation. Because of the advantages and the big name behind it, it’s very popular, especially for mobile apps because of the good client support.</p>

<p>However, I feel the framework is very invasive: models are usually the foundations of a program. With gRPC, the models are generated by the framework, as well as the interfaces. This makes the whole program depends on the framework very heavily. Here is an example:</p>

<div class="language-scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>// ExampleService is generated by gRPC
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>class ExampleServiceImpl() extends ExampleService {
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>        // ExampleInput and ExampleOutput are both generated by gRPC.
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>        def exampleAPI(input ExampleInput): ExampleOut = {
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>                ...
<span class="line-numbers"><a href="#n6" name="n6">6</a></span>  }
<span class="line-numbers"><a href="#n7" name="n7">7</a></span>}
<span class="line-numbers"><a href="#n8" name="n8">8</a></span>
</pre></div>
</div>
</div>

<p>It’s more invasive than most of the (non RPC) libraries: for most of other libraries, you can define the interface and use those libraries to fill in the implementations, so when you change a library you don’t need to change other parts of the code.</p>

<p>Maybe sometimes it doesn’t matter too much: say you are Google and this framework is so fundamental in the services that no one is gonna to change it. But if you don’t like it, a way to work around this is to define the business logic at another place, and invoke those native classes and methods in the implementation of the gRPC generated interfaces. The logic in these implementations should be as simple as possible, usually just the invoking of methods and the converting between gRPC models and native models. Here is an example of this approach:</p>

<div class="language-scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>// define natvie classes
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>case class MyInput(...)
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>case class MyOutput(...)
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>class MyService() {
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>        def myAPI(input MyInput): MyOutput = {
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>                ...
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>        }
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>}
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>
<span class="line-numbers"><a href="#n12" name="n12">12</a></span>
<span class="line-numbers"><a href="#n13" name="n13">13</a></span>// implement gRPC interfaces
<span class="line-numbers"><a href="#n14" name="n14">14</a></span>// ExampleService is generated by gRPC
<span class="line-numbers"><a href="#n15" name="n15">15</a></span>class ExampleServiceImpl(val myService: MyService) extends ExampleService {
<span class="line-numbers"><a href="#n16" name="n16">16</a></span>
<span class="line-numbers"><a href="#n17" name="n17">17</a></span>        // ExampleInput and ExampleOutput are both generated by gRPC.
<span class="line-numbers"><a href="#n18" name="n18">18</a></span>        def exampleAPI(input ExampleInput): ExampleOut = {
<span class="line-numbers"><a href="#n19" name="n19">19</a></span>                val myInput = convertFromGRPC(input)
<span class="line-numbers"><strong><a href="#n20" name="n20">20</a></strong></span>                val myOutput = myService.myAPI(myInput)
<span class="line-numbers"><a href="#n21" name="n21">21</a></span>                convertToGRPC(myOutput)
<span class="line-numbers"><a href="#n22" name="n22">22</a></span>  }
<span class="line-numbers"><a href="#n23" name="n23">23</a></span>}
<span class="line-numbers"><a href="#n24" name="n24">24</a></span>
<span class="line-numbers"><a href="#n25" name="n25">25</a></span>// implement convertFromGRPC ...
<span class="line-numbers"><a href="#n26" name="n26">26</a></span>// implement convertToGRPC ...
<span class="line-numbers"><a href="#n27" name="n27">27</a></span>
</pre></div>
</div>
</div>

<p>However, this results lots of repeated works, especially for the converting between gRPC models and native models.</p>

<p>So here is where the sbt plugin Scala2grpc I’ve written comes in: it will generate all the proto files from the native Scala classes, and also generates the classes to convert between native models and gPRC models, and the classes to implement the gPRC interfaces. For example, in the example above, you only needs to write the code for <code>MyInput</code>, <code>MyOutput</code> and <code>MyService</code>, the generation of the proto files are handled by Scala2grpc, as well as <code>ExampleServiceImpl</code>, <code>convertFromGRPC</code> and <code>convertToGRPC</code>.</p>

<p>Sounds interesting? Check out the <a href="https://github.com/wb14123/scala2grpc">Github page</a> to see how to use it. Don’t forget to star it if you find it helpful!</p>]]></content><author><name></name></author><category term="Programming" /><category term="Scala" /><category term="gRPC" /><category term="backend" /><summary type="html"><![CDATA[This article describes why I created the library Scala2grpc.]]></summary></entry><entry><title type="html">A Travel to Montreal</title><link href="https://www.binwang.me/2022-04-24-A-Travel-to-Montreal.html" rel="alternate" type="text/html" title="A Travel to Montreal" /><published>2022-04-24T00:00:00-04:00</published><updated>2022-04-24T00:00:00-04:00</updated><id>https://www.binwang.me/A-Travel-to-Montreal</id><content type="html" xml:base="https://www.binwang.me/2022-04-24-A-Travel-to-Montreal.html"><![CDATA[<p>I have been in Toronto for more than two years. However, because of Covid-19, I’ve never travelled outside of Greater Toronto Area except Niagara Falls since I came here. Until a few weeks ago, with the Covid-19 measurements mostly lifted, my wife and I decided it’s time to take a travel after such a long time. And the long weekend before Easter day is a perfect time.</p>

<p>The destination we chose was Montreal. It’s very closed to Toronto, and is the second largest populated city in Canada. Because of it’s location at St Lawrence river, the early colonizers stopped at Montreal to create frontier port and fort, which makes it one of the earliest cities in Canada. The French colony history of Montreal produces a culture that is different than other English colonies, and is mostly preserved by Quebec Act after the conquer of British. This makes it a great destination for travelling, since the main purpose of travelling is to escape from daily life and feel something different.</p>

<h2 id="airport-on-toronto-islands">Airport on Toronto Islands</h2>

<p>The flight we booked was departed from Billy Bishop airport. This airport is located at Toronto islands – a chain of small islands in Lake Ontario near Downtown Toronto. Considering the close distance between the airport and downtown, jets are banned to limit noise level. As a result, the destinations from this airport are mostly nearby cities in Canada and the US.</p>

<p>Nevertheless the limited flight options, the location of this airport is so convenient that I can walk there in just half an hour from home. I’ve always appreciated the walkable neighborhoods in Downtown Toronto, but I’ve never thought I can just walk to an airport. The airport entrance is located at the lake shore and is connected to the airport with an underground tunnel. Looking back to the water front, the view is just unbeatable.</p>

<p><img src="/static/images/2022-04-24-A-Travel-to-Montreal/from-aiport.jpg" alt="from-airport" /></p>

<p>It’s lovely inside the airport as well. The advantage of a small airport is you don’t need to walk a mile from the entrance to the boarding gate. The whole waiting area is like a cafeteria: The seats are arranged in curved lines, lots of them with tables. It’s clean and quite, really unlike lots of airports I’ve been to.</p>

<h2 id="old-montreal">Old Montreal</h2>

<p>We arrived at Montreal at an early evening. The moment we came out from the subway station, we saw the moon was shinning and the sky was deep blue. Under the sky was a large building with yellow lighting on the top. There was a curved road near the building. Farther away along the road, there seems to be a gigantic European style gate with complex stone decoration – only at the next day we found out it’s actually the facade of the famous Notre-Dame Basilica of Montreal, whose beautiful and mysterious dome has the color just like the sky of that night.</p>

<p><img src="/static/images/2022-04-24-A-Travel-to-Montreal/old-montreal-night.jpg" alt="old-montreal-night" /></p>

<p><img src="/static/images/2022-04-24-A-Travel-to-Montreal/church-dome.jpg" alt="church-dome" /></p>

<p>Montreal has lots of beautiful old buildings, especially near St Lawrence river, in the area where the city was first established. This area is called Old Montreal now. It has lots of bars, restaurants, cafes, parks and even a Ferris wheel. It’s a vibrant area filled with people during the weekend. Thanks to some pedestrian only streets, and narrow roads that helps to limit the speed of cars, it really feels like an old and classic European city. I hope Toronto has more pedestrian only streets like this.</p>

<p><img src="/static/images/2022-04-24-A-Travel-to-Montreal/old-montreal-street.jpg" alt="old-montreal-street" /></p>

<p>In my opinion, another master piece in this area is not one of the old day buildings. Instead, it’s built from the ashes of old buildings. It’s Pointe-à-Callière, or Montreal Museum of Archaeology and History. Its main building is built on top of the remains of Royal Insurance Building. However, it’s actually much larger than that. A large part of the Museum is underground which connects other smaller aboveground buildings. The underground part covers the remains of multiple historical sites, including the Royal Insurance Building, the old custom house, an old sewerage (which is a result of transforming a river to underground), Fort Ville-Marie and so on. The collections of this museum are not the most valuable ones, but combined with this unique site, it presents them in a both beautiful and informative way. The style of exhibit is like an art gallery, and with the help of lights and audio, it shows how history leaves marks in different periods.</p>

<p>What amazed me the most is the ability to find the exact foundation of the city. This is unthinkable in most Chinese cities, and I think for lots of old world cities as well, because of their long history. It’s so valuable for the new world cities to be able to see its whole history, and learn from it.</p>

<h2 id="downtown-montreal">Downtown Montreal</h2>

<p>Old Montreal is beautiful, but I don’t think it would be the first choice if people want to go shopping – it’s mostly souvenir shops over there. On contrast, there are so many shopping malls at Downtown Montreal, especially along Saint-Catherine Street. Across the whole downtown area, this street is filled with restaurants, bars, shops, and of course, people. And again, I think the vibrant is largely thanks to the wide pedestrian street and narrow roads for cars. I saw some places are still under construction which was probably to make the pedestrian street even wider.</p>

<p>The buildings are nicer and taller than those on Toronto commercial streets. Grand entrance with tall windows on the facade is very usual. Indoor space also feels more open, almost like the shopping malls at suburb areas of Toronto. All of these makes it a pleasant to walk and shopping.</p>

<p><img src="/static/images/2022-04-24-A-Travel-to-Montreal/downtown-montreal.jpg" alt="downtown-montreal" /></p>

<p>Montreal is a city speaks French, but also lots of English, more than any other cities in Quebec. This is most obvious in book stores. In an Indigo store I visited, the first floor is filled with French books while the second floor with English books. It’s fun to think about how two different cultures, even both of them are from Europe thus similar from an outsider’s point of view, are mixed and conflicted. Another example of this conflict can be seen from the introduction movie at Pointe-à-Callière: while it spends little time to introduce how French occupied the Island of Montreal from native people (other parts of the Museum did say the Island of Montreal is kind of unoccupied before French came since it’s easier to get attacked because of the geography), it plays scary music during the narrative of British conquer and releases British from a book like a demon. I’m not sure if it’s a joke or not, but you can see something from it.</p>

<p>Speaking of the multicultural of the city, I must mention the Chinatown. It’s located at the eastern part of downtown, with four Paifang – a Chinese style gate – mark the boundaries. St Laurent Street, which is also often called the Main Street, passes right through it. There is another pedestrian only street in Chinatown, which makes this area very vibrant. I even found a restaurants that provides some special food from my hometown. Those food is hard to find even in Beijing, and I’ve never seen it in Toronto. So this is a happy surprise we didn’t expect.</p>

<p><img src="/static/images/2022-04-24-A-Travel-to-Montreal/chinatown.jpg" alt="chinatown" /></p>

<p>Unlike Toronto, the Chinatown in Montreal is marked with clear boundaries, and seems to attract more people (I’ve never been to the other Chinatown at eastern Toronto so not sure about that one). But it seems to be mostly a commercial zone without many residential buildings. And most of Chinese commercials seems to be limited to this area – unlike Toronto where you can find them almost everywhere. The next day I walked to another area at western downtown near Concordia University, where I heard newer Chinese immigration prefers, and found some Chinese restaurants and shops with more modern style. But my overall impression is the Chinese commercials are more centralized than those in Toronto.</p>

<h2 id="mount-royal-montreal">Mount Royal, Montreal</h2>

<p>Montreal is named after Mount Royal, which I didn’t know before, but makes lots of sense to me after I heard it. It’s not a high mountain, but still high enough to see the whole downtown area and beyond that. I’d like to think it’s the backyard of the city – just next to the commercial and residential area, here is a nice park with wonderful views.</p>

<p><img src="/static/images/2022-04-24-A-Travel-to-Montreal/mount-royal.jpg" alt="mount-royal" /></p>

<p>There is another large church called Saint Joseph’s Oratory near Mount Royal. You can see Montreal at another direction from there. The day we visited there was cold, with freezing rain and heavy wind. Sitting inside the church and watching layers of cloud floating, and the land and rivers under it, I couldn’t stop wondering what have happened on this land. When I was in China and stood at a high point like this, I could almost see the past of the land: the dynasties, the legendaries, the glories, the wars and the tragedies. Sometimes I could feel the pain of the land because it bears so many histories. But it’s different here: everything is so claim, nothing much to tell, only the wind, the cloud and maybe some forgotten stories.</p>

<p><img src="/static/images/2022-04-24-A-Travel-to-Montreal/st-joserph-church.jpg" alt="st-joserph-church" /></p>

<h2 id="food-life">Food, Life</h2>

<p>There are some famous food in Montreal. Poutine, bagel and smoked meat are in the top list. And of course we tried all of them. Poutine is fine, but not much better than the Poutine in Toronto. Smoked meat is very unique, almost like a kind of Chinese stewed beef, but with more delicious fat. However, bagels there was a big disappointment. It’s either we didn’t find the right place, or didn’t eat them in right way. We looked up a very popular bagel store, waited in a long line only to found out there is no bagel sandwich – just the bagel itself. So we went to another popular bagel store, waited in a long line again and found out the same. Luckily they also sold salmon and cream cheese, so we bought those and ate them in a nearby cafe. But the bagels are so normal – far worse than the bagels sold by the stores near my home at Toronto. The bagels at Toronto are so delicious that they are among my favourite food. I don’t think I will miss them if they are all like what I ate in Montreal.</p>

<p>During the time we bought bagels, we had an opportunity to walk in one of the residential areas – Mile End community. I was only in Montreal for three days and have no data to support me, but my first impression is its mixed usage is not as good as Toronto. I know North America cities have (in my mind) absurd zoning code and Montreal should already be better than lots of them, but I really like the place that you can just walk to everywhere for daily needs: offices, grocery stores, shopping malls, parks and so on. Downtown Montreal has all of them but I didn’t see lots of residential buildings over there. While at the Mile End community I saw lots of residential houses, even mid rises, but not a lot of other things.</p>

<p>Public transit is good in Montreal. The subway cars are new and clean. But buses can be hit or miss – not sure if it’s because of the holiday weekend. The worst experience happened when we were waiting for the bus to the airport. There was a long line and the buses were not came as scheduled. Lots of people needed to find other ways to the airport at last. According to the lady before us in the line, who was born and raised at Montreal, it’s crazy but apparently not the first time – this situation happened to her at one Christmas day as well. We were almost decided to take a taxi to airport as well – actually we called a Uber but the bus came at the last minute. Even so, we didn’t make the flight we booked but the airline was kind enough to move us to the next flight, so the end of the travel is not so bad.</p>

<p>Overall the travel was a great experience. It is a good start to get the life back to normal after Covid-19, and a good start of my exploration to the cities in Canada.</p>]]></content><author><name></name></author><category term="life" /><category term="Montreal" /><category term="Canada" /><summary type="html"><![CDATA[I have been in Toronto for more than two years. However, because of Covid-19, I’ve never travelled outside of Greater Toronto Area except Niagara Falls since I came here. Until a few weeks ago, with the Covid-19 measurements mostly lifted, my wife and I decided it’s time to take a travel after such a long time. And the long weekend before Easter day is a perfect time.]]></summary></entry><entry xml:lang="zh"><title type="html">十三年前被隔离的经历</title><link href="https://www.binwang.me/2022-04-04-%E5%8D%81%E4%B8%89%E5%B9%B4%E5%89%8D%E8%A2%AB%E9%9A%94%E7%A6%BB%E7%9A%84%E7%BB%8F%E5%8E%86.html" rel="alternate" type="text/html" title="十三年前被隔离的经历" /><published>2022-04-04T00:00:00-04:00</published><updated>2022-04-04T00:00:00-04:00</updated><id>https://www.binwang.me/%E5%8D%81%E4%B8%89%E5%B9%B4%E5%89%8D%E8%A2%AB%E9%9A%94%E7%A6%BB%E7%9A%84%E7%BB%8F%E5%8E%86</id><content type="html" xml:base="https://www.binwang.me/2022-04-04-%E5%8D%81%E4%B8%89%E5%B9%B4%E5%89%8D%E8%A2%AB%E9%9A%94%E7%A6%BB%E7%9A%84%E7%BB%8F%E5%8E%86.html"><![CDATA[<p><em>注：此文由本人首发于 Reddit 。发布时分了<a href="https://www.reddit.com/r/China_irl/comments/tv45el/%E5%8D%81%E4%B8%89%E5%B9%B4%E5%89%8D%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BB%8F%E5%8E%86%E4%B8%80/">上</a>, <a href="https://www.reddit.com/r/China_irl/comments/tvb9wa/%E5%8D%81%E4%B8%89%E5%B9%B4%E5%89%8D%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BB%8F%E5%8E%86%E4%BA%8C/">中</a>, <a href="https://www.reddit.com/r/China_irl/comments/tvvuuq/%E5%8D%81%E4%B8%89%E5%B9%B4%E5%89%8D%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BB%8F%E5%8E%86%E4%B8%89/">下</a>三篇。今转录于此，也按照原本的格式，并无另外的编辑。</em></p>

<h2 id="一">一</h2>

<p>最近疫情形势紧张，不管网上还是线下都有不少讨论。其中社会百态，令人唏嘘。由此突然想到了十三年前甲流时我被隔离的经历。所以就随便写点东西，算是个记录，也算是对年少时光的一点缅怀。</p>

<p>十三年前，也就是二零零九年，是我上大学的第一年。上大学前从未出过远门的我，对于远方有着浪漫的想象。因此报志愿时选了离家最远的大学之一。去了新的城市，一切都很新奇。大学刚开始第一件事就是军训。不知是否所有大学军训都很严，我们当时每天都要早早起床把被子叠成豆腐块，会有人每天抽查。很多同学为了多睡一会干脆叠了以后晚上睡觉就不盖被子，而是把窗帘拆下来当被子盖。给我们军训的教官是本校的国防生，因此也免不了谁谁谁和教官谈恋爱等八卦和狗血。军训虽苦，但是现在留在记忆里的却是有一天晚上大家一起唱歌，皎皎明月挂在空中，初秋的风吹过，心中对未来充满了憧憬。大学军训还给了拿真枪的机会，不过据说前几年出过事故，所以只是让瞄了瞄靶，并未给实弹射击。最后又学军体拳，也算比其他的项目更有意思一些。按往年惯例，军训最后的大高潮应该是所有军训的学生接受检阅，进行军体拳表演，让学校领导过一把军队领导的瘾。</p>

<p>然而这一年却有所不同。军体拳刚练到一半，突然通知所有军训全部停止 — 因为学校发现了几个甲流病例，好像还成为了当地的重灾区。当时对甲流懵懵懂懂，并未觉得有多可怕，反而还乐得不用军训，几个寝室的人聚在一起天天打牌聊天，倒也自在。</p>

<p>大概这样到了十一假期附近，学校突然通知十一假期期间要封校。这给我们带来了不小的振动。学生时代，对于学生来说最宝贵的就是假期。记得高中的时候为了反对假期补课有人给电视台打电话举报，为了反抗清明节不放假集体从教学楼上往下撒纸钱（其实就是撕了的白纸，嗯……，还有课本）。所以当得知十一不能放假的时候，大家都很受打击。一些人立刻制定了计划，决定当天晚上趁政策还没开始严格执行，在夜色掩护下翻墙逃走。我和另外一个寝室的Z君决定结伴而行。当晚来到宿舍楼下的围墙时，发现早已有人用桌椅整整齐齐地搭好了翻墙通道。于是我们轻轻松松翻出围墙。</p>

<p>然而出去的时候已经很晚，公共交通大部分都已停止，更别说我当时计划回家所要坐的火车了。于是两人商量去哪先过一夜，等第二天早起再做打算。我们先去网吧消磨了一段时间，Z 君提议当晚就在网吧度过。然而我对网吧通宵带来的体力消耗深有体会，Z 君家在本市自然好说，而我为了给第二天坐火车留点精力，并没有接受他的提议，而是自己在旁边找了个小旅馆住下。就此别过了Z君，各奔前程。当晚想了些什么已记得不太清楚，只记得当时觉得经历之离奇，很是让人激动。</p>

<p>第二天一大早刚出旅馆时，看到马路对面的学校围墙刚刚翻出来了一个学生，后面有个国防生飞快的追赶，并且口中大喊要举报给校长。但是可能受限于封校的命令，并不能也随之翻墙出来，所以在墙内追了几步以后只得作罢。看到这惊险一幕，我暗自庆幸昨天晚上逃出来真是个正确的决定。于是加快步伐走到公交站，搭车到了火车站，却发现最早的班次也要到晚上，无奈买了当晚的火车票，在火车站等了一天，又做了二十多个小时的硬座回到了家。回到家中的大部分细节都已忘记，然而让我印象最深刻的一个画面是十一当天上午，父亲在包饺子，背景音是电视里在播放的建国60周年阅兵式。我坐在旁边心不在焉的看着坦克一排排在天安门前经过，想着一会要出去哪里玩。</p>

<p>谁知十一假期我虽逃了一时，后面却有真正的隔离在等着我。写到这里发现已经不少，因此就先告一段落。预知后事如何，且听下回分解。</p>

<h2 id="二">二</h2>

<p>上回说了被隔离之前的一些背景，这次讲讲真正被隔离的经历。这么多年过去其实大多数细节都已模糊，但是从之前的经历来看，现在隔离政策的某些弊端在当时已现端倪。不过在不同的政治氛围和不一样的病毒之下，结果还是有很大不同。是是非非，留待大家自己分辨吧。</p>

<p>上回说到了翻墙通道，有评论觉得惊讶。其实那个时候大多数的政策都挺宽松，有很多灰色地带。学校里对新生管理的比较严，而对于大三大四的学生就是散养状态，只要不犯什么大事，学校基本也懒得管。说到围墙，再补充一个细节。当时学校的围墙是铁栏杆，其实在我们宿舍楼下的那段平时是有一个缺口的。围墙外面就是小吃饭店一条街，这个缺口方便学生直接钻出去买饭而不用绕道大门，也方便外面的社会人员绕过学校管控进来送餐。而在封校的时候这个缺口被铁丝封死了，所以才有了桌椅摆成的翻墙通道。等后来解除封校之后，这个缺口又不知被谁打开了。后来学校也多次尝试封闭这个缺口，包括用铁栏杆修复，但是无一例外都很快又被打开。我们都怀疑是外面的小吃商贩所为。不知这个缺口是否现在还在，也不知现在的小吃街是否还繁华如昔。</p>

<p>书归正传。十一假期之后，学校解除封校，学生全部开学。从学生的角度来看，基本都已回归正常。大一的新生也终于开始上课。这时我们才发现高中老师们说的到了大学就轻松了就是扯淡。对于我们专业来说，大一的必修课就把一天的时间排的满满的，很多时候到了晚上还要上课。除了没有考大学的压力，繁忙程度比高中有过之而无不及。我当时刚上大学，对于所有事情还处在新奇和兴奋的阶段，所以对于满满的课程也并无多大怨言，反而还对学习一些新知识乐在其中。</p>

<p>这样过了几天，最多也就半个月吧。有一天在寝室里，我突然感觉有些不适。于是就准备去校医院拿点药。到了校医院，才发现虽然各种管控都已放松，但是对于发热病人的流程还在。于是给了我口罩，让我量一下体温，发现发烧之后直接通知我需要隔离。当时疫情到了后期，学校里已经好几天没什么病例了，所以我也并没有担心得了甲流，也并不知道还有隔离措施，所以有些惊讶。但是想想觉得确实疫情管控需要，既然发烧了就不要乱跑了，于是决定服从安排。当时校医院已经满员，需要送我去别的地方隔离。整个流程已经记得不甚清楚了：只记得量体温好像是在一个教室里面量的，然而等待隔离却是在校医院。也不记得中间是否回寝室拿了什么东西，还是托室友送了些东西过去。现在想想当时隔离好像也并没有什么强制措施，要是真的不想隔离，直接走了也就是了。</p>

<p>等待隔离是在校医院门口，当时只有我一个人。我等待的时候，恰好有个中年男子开着私家车停在校医院门口，和医生说了几句话，医生就让他顺路送我去隔离。上车之后发现他并没有带口罩，也不清楚是否是医院的工作人员，所以还是有些惊讶 — 因为当时看新闻送人去隔离的医生都是防护服全副武装。闲聊了几句就到了隔离的地方，才知道这里是附属于学校的一个专科学院。这个学院虽然挨着学校，但是并没有通道直接连通，所以算是一个相对独立的区域。下车之后发现这个学院整体环境都比较破旧，空间不大，地上有些地方还有杂草。隔离住的楼也是如此：四四方方的建筑上有一个塔楼高高耸起，旁边连着两个低一些的塔楼，是个明显的斯大林式建筑，样式和我们学校的主楼非常相似，只不过小了一号。建筑表面已经斑驳，不知在非隔离期间是做什么用途。进楼之后发现，所有人都没带口罩，这令我更为吃惊，同时也更坚定了目前疫情情况已不严重的信心。我索性也把口罩摘了 — 其实人很多时候都是不是理性的动物，都是随群的乌合之众。</p>

<p>我被分配到了一个房间，有几张病床，但是房间里只有我一个人。房间里灯光煞白。我躺倒床上，用手机和朋友们聊了几句，却并没有通知家里：因为我觉得疫情已不严重，不想让家人做无谓的担心。安定下来之后，我想着终于能休息休息，吃点药恢复一下了，然而却被告知并没有药。不知是心理作用，还是病情本身原因，当晚就觉得身上酸疼不止。等到了挺晚的时候，大概十一二点的样子，护士上来给了一片白色药片。具体是什么药也没说明，只说是退烧药。现在想想应该是阿司匹林或者泰诺一类的药，也算是对症。但是当时觉得真是有点可怜。吃下药之后浑身出汗，睡了一觉后第二天感觉轻松不少，至此才放下心来。</p>

<p>隔离的这个楼里让我印象最为深刻的就是厕所：这个楼的男女厕所是完全连在一起的，而只有女厕所的门可以连到外面。也就是说，要想去男厕所就必须穿过女厕所。男女厕所之间有一扇门，但也并不是所有时候都是关着的。这个厕所代表着我对隔离这段时间的感受：陌生且略感荒谬。</p>

<p>然而在这个楼里没待多久，我们就被转移到了另一个地方。后面的经历，等下篇再继续更新。</p>

<h2 id="三">三</h2>

<p>上篇文章终于说到了隔离的事情，这一篇就把这个系列讲完。其实整个事情并不复杂，没想到东拉西扯写了不少东西。不论有没有人看，倒是自己找到了一些久违的写文字的快乐。也希望能给这个论坛带来一些不太一样的内容。</p>

<p>前文说到我因为发烧被隔离到了一个附属于大学的专科学院，也并没有什么诊断，只是给了一片退烧药。好在吃过药之后第二天有很大好转。本以为就这样住几天就可以顺利出去，没想到中间又给转移到了另一个地方隔离。</p>

<p>通知是在深夜突然下来的，也没说是什么原因，后来有人说可能是隔离的房间不够用了，所以把我们这些已经隔离了几天的低风险人群转移到另一个地方。通知要转移地方后并没有给什么时间准备，正好我也没多少随身物品。整个楼的人集合起来，由工作人员带头，排成一字长蛇阵向新的地点走去。</p>

<p>新的隔离地点是紧邻学校的一个宾馆。这个宾馆属于学校资产，并对外开放，据传闻是本校失足的女大学生经常光顾的地方。出发的时候夜已非常深，所有寝室楼都已统一关门熄灯。我们十几个人在寂静的校园里穿梭，大家都默不做声，只有走路时衣服摩擦发出沙沙的声音。每个人除了看着前面的人，就是看着路灯照耀下的人影婆娑。一行人七拐八绕到了宾馆后并未走正门，而是在一个犄角旮旯里面由带头的工作人员拿出钥匙，开了一个后门。</p>

<p>从后门向内望去，里面漆黑一片。带头的工作人员又拿出了手电筒，带着我们从楼梯上楼。也不知道一共走了几层，只记得在三四层左右是一个巨大的空间，手电筒照到的地方全部都是水泥地面，斜斜歪歪的散落着破损的饭桌和椅子，极像港剧中打斗发生的场景。最终我和另外一个人被分配到了一个标间。标间环境不错，以我当时相当有限的经验来看算是住过最好的酒店房间了。</p>

<p>和我分配到一个房间的同学也是大一新生，来自另一个学院。两人同住一间也未觉不便，毕竟条件比四人或八人同住的寝室已好上太多，而且更乐得有人聊天排解无聊的时间。按理说患难之下的友情应该更为坚固，可是出去后也不过变成了偶尔见面时的点头之交，到了现在我连此君的名字都已忘记。除了居住条件以外，伙食也相当不错。每顿一个盒饭，至少两荤一素，有鱼有肉。可惜我当时还在生病，吃不了太油腻的东西，所以每顿都会剩下不少。</p>

<p>我被隔离的时候恰值社团纳新。之前也说过，刚上大学的我对所有这些新鲜事物都感到好奇，所以计划怎么也要加入一个。因为并不知道都有哪些社团，所以托寝室室友帮我报名了学院的学生会，可是由于种种原因并未成功。不过从以后学生会的风气来看，还真是庆幸没有报名成功。</p>

<p>对于当时的我来说，错过了社团纳新是一个很大的遗憾。不过可能也正因为此，我报名了下半学期才纳新的学院科技协会，并且由于另一个机缘巧合，并没有参加我计划中的 VC++ 小组，而是加入了 ACM 竞赛小组，从此走上了程序竞赛的道路，为后面找工作面试编程省了不少事情。然而也因为竞赛耗费了一些精力，打乱了我每晚自习、跑步、图书馆的三大项目，并间接导致下学期挂科，以至于最后大学辍学。正所谓祸福相倚，谁都说不清楚，不过这都是后话了。</p>

<p>除此之外，被转移到新的地方后一切顺利，没待几天就顺利出来了。出来以后对于缺了几节高数和线性代数课略感担心，不过好在也并无大碍。自此以后也意识到了身体的重要性，大一一学期每晚都坚持去操场跑上几圈，秋去冬来，寒暑相过，也算是留下了不少美好的记忆。</p>]]></content><author><name></name></author><category term="life" /><category term="Covid-19" /><category term="H1N1" /><summary type="html"><![CDATA[注：此文由本人首发于 Reddit 。发布时分了上, 中, 下三篇。今转录于此，也按照原本的格式，并无另外的编辑。]]></summary></entry><entry><title type="html">Add Index to My Blog</title><link href="https://www.binwang.me/2021-10-31-Add-Index-to-My-Blog.html" rel="alternate" type="text/html" title="Add Index to My Blog" /><published>2021-10-31T00:00:00-04:00</published><updated>2021-10-31T00:00:00-04:00</updated><id>https://www.binwang.me/Add-Index-to-My-Blog</id><content type="html" xml:base="https://www.binwang.me/2021-10-31-Add-Index-to-My-Blog.html"><![CDATA[<p><em>Update: the index page has been removed in favoer of the index sidebar. See <a href="/2023-11-10-Index-Sidebar-on-My-Blog.html">Add Index Sidebar to My Blog</a> for more details.</em></p>

<p>I added an <a href="/index_page.html">index page</a> to my blog yesterday, which can also be accessed from the “Index” entry at the top of every page. When I moved this blog to Jekyll, I <a href="/2012-12-10-Remove-Categories.html">removed categories</a> since I thought tag should be enough. But after a few years, I feel something is missing. One of the reason maybe I never implemented find posts by tags, which makes tags useless. Even if I had that, I feel it’s still not enough. But I don’t want categories , since “category” in Jekyll is normally means a flat level structure. What I want is a nested category structure. It should let me create as many levels as I want, like the index in a book. In this way, I will have a better idea about which field I have covered, and what I should focus on based on my future plan. It can also benefit the readers: they can find the posts they are interested much easier. Not every blog post has the same quality or depth based on the nature of the topic, so in this way the posts under most interesting topics can be grouped together instead of being buried in the timeline.</p>

<p>To implement this feature, I wrote my own little plugin. I only knew some Ruby knowledge from one of Ruby’s author’s books (I read the Chinese version of the book and cannot find the name of the English version, I believe the original book is published in Japanese with the name まつもとゆきひろ コードの世界~スーパー・プログラマになる14の思考法). I never worked on any project with Ruby. But I always wanted to write some plugins for Jekyll so I know I can customize my blog in a better way. Luckily this task is simple enough. The result source code is on my Github of this blog repo. It has a <a href="https://github.com/wb14123/blog/blob/master/jekyll/_plugins/Index.rb">ruby script</a> to generate the index page from post property, with a <a href="https://github.com/wb14123/blog/blob/master/jekyll/index_page.html">template</a> that uses another <a href="https://github.com/wb14123/blog/blob/master/jekyll/_includes/index_page.html">recursive template</a>.</p>

<p>So here is how this plugin works: the only thing I need to do while writing a post is to add a new property called <code>index</code> at the starting of the Markdown file, like this:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>---
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span># these three properties are needed before this feature:
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>layout: post
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>title: Add Index to My Blog
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>tags: [blog, index]
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span># this is the newly added property:
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>index: ['/Projects/Blog']
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>---
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>
</pre></div>
</div>
</div>

<p>Then the plugin will parse <code>index</code> field, break it down into multiple levels based on <code>/</code> in it, and group all the posts in the same field together. The categories are sorted based on alphabet order, while the posts in each category is sorted from old to new. <code>index</code> is an array, so in theory I can add a blog post into multiple categories if I want to, but I try not to do it if it’s not necessary, since it gives a false feeling about how many posts I’ve written. For example, at the beginning, I tried to have both <code>/Computer Science/Database</code> and <code>/Computer Science/Distributed System</code>, and almost every posts under <code>database</code> is also in <code>distributed system</code>. Then I decided it doesn’t make sense: they are more about distributed system because what I care most in those articles is about database transactions. So I remove the category <code>database</code> and put all of them just under <code>distributed system</code>. If I wrote anything like query optimization in the future, I may created another <code>database</code> category, but it’s not needed for now.</p>

<p>I’m very happy with the result. I want it to be simple enough as a static page without the need of Javascript. Javascript can certainly improve some UI like expand/collapse the levels. I may implement both version in the future so people can view it with and without Javascript, but the current UI is good enough for me.</p>

<p>UI aside, I’m much happier about the content. The index has a good structure. It shows the fields I’ve explored. It even shows a correlation between the focus and the timeline, which I never thought about before. For example, the only two posts about algorithm are published when I was in the University, when I joined some programming contest. The posts about Linux are most published at the last year of my college life, when I was in the first few years of using Linx and had an internship at Redhat. It doesn’t mean I’m not interested in Linux anymore. It’s just after so many years of using Linux, it becomes a tool that so fundamental to my work and digital life, and I don’t feel the necessary to actively learn the basic things and tune it. Data processing related topic happened most when I worked in an A/B testing company. And machine learning related topic also happened around that time, when I was most interested in neural networks, which resulted the most popular side project I’ve built. I started to post distributed system related topic after I joined a database company, and still find it’s an interesting topic thus many posts in last year. Which also remains me if I want to learn more about it I should continue to write articles about it in this and following years. But not everything is perfect, mainly because I didn’t write everything I’ve worked on my blog. One of the reason is a lot of things I’ve explored are already somewhere else and I don’t want to just copy something to my blog. The projects I’ve worked on are also not covered. I’m still thinking what’s the best way to put it into the index: either add a link to the project page, or write a blog post about each of the projects. Other than those, there is still something missing. It’s mostly because of lazy, which I should change in the future.</p>

<p>So as a result, the index page achieved some of goal: covering some of my past works and thoughts, and remanding me what I should focus in the future. About the goal of benefiting the readers, you as a reader decide if it’s good or not.</p>]]></content><author><name></name></author><category term="blog" /><category term="index" /><summary type="html"><![CDATA[Update: the index page has been removed in favoer of the index sidebar. See Add Index Sidebar to My Blog for more details.]]></summary></entry></feed>