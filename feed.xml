<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://www.binwang.me/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.binwang.me/" rel="alternate" type="text/html" /><updated>2023-04-10T09:39:14-04:00</updated><id>https://www.binwang.me/feed.xml</id><title type="html">Bin Wang - My Personal Blog</title><subtitle>This is my personal blog about computer science, technology and my life.</subtitle><entry><title type="html">Infrastructure Setup for High Availability</title><link href="https://www.binwang.me/2023-03-13-Infrastructure-Setup-for-High-Availability.html" rel="alternate" type="text/html" title="Infrastructure Setup for High Availability" /><published>2023-03-13T00:00:00-04:00</published><updated>2023-03-13T00:00:00-04:00</updated><id>https://www.binwang.me/Infrastructure-Setup-for-High-Availability</id><content type="html" xml:base="https://www.binwang.me/2023-03-13-Infrastructure-Setup-for-High-Availability.html"><![CDATA[<p>Cloud is popular these days. But sometimes we just want to host something small, maybe just an open source service for family and friends, or some self-built service that we are still experimenting on. In this case, the cloud can be expensive. We can just throw a few nodes at home and run it at a very low cost. But you don’t want the service down when some nodes failed, at least the service should be available when you upgrade and reboot the nodes because it can happen very frequently. In this article, I will talk about how to build high available infrastructure so that the service can be alive even when some nodes are down.</p>

<h2 id="what-is-high-availability">What is High Availability?</h2>

<p>Availability means a service is alive and can serve traffic. High availability (HA) means when some components of the system are down, the service is still alive. The failed components can be in different layers: it can be a region, a DC, a network, or some nodes. In this article, I’ll only talk about things including and above node level, since region and network are usually out of control for a small infrastructure setup. That means you can host the service on multiple machines, and it should still be alive even when some of the machines are down. This is the most useful case of HA in the case anyway since nodes can be down frequently because of OS or software updates.</p>

<p>Before I start with the real setup, I want to clear some myths about HA first. Maybe you’ve heard of the famous CAP theorem, which says only two of the three properties can be met at the same time: consistency, availability, and partition tolerance. Lots of people misinterpret it as a HA system that will sacrifice consistency during a failure. It is not true: the type of partition that makes you must choose between consistency and availability is very rare. In most well-designed HA systems, you can have both as long as more than half of the nodes are alive (alive also means reachable from clients). And HA doesn’t necessarily mean it prioritizes availability over consistency either: it just means it can handle more failure cases when keeping both consistency and availability. When there is a failure it cannot handle, it can choose to keep consistency and make the service unavailable. This is the type of HA I’m going to introduce in this setup.</p>

<p>So to make it clear, the HA goal in this setup is to make the services still alive without sacrificing consistency when we lose less than half of the nodes (either it’s partitioned from the network or actually dead).</p>

<h2 id="ha-setup">HA Setup</h2>

<p>As said before, the HA needs multiple nodes in case of some nodes are down. The setup in this article can tolerate less than half of the node loss. So if you want to have a HA that can handle 1 node loss, the whole system needs at least 3 nodes. There are lots of cheap used machines on the market that have enough power to host many open source services.</p>

<p>The HA setup has multiple layers and we will use different tools for each of the layers:</p>

<ul>
  <li>Compute: Kubernetes</li>
  <li>Storage: GlusterFk</li>
  <li>Database: Cockroach DB</li>
  <li>Network Ingress: Cloudflare Tunnel</li>
</ul>

<p>Here is an overview of the setup:</p>

<p><img src="/static/images/2023-03-13-Infrastructure-Setup-for-High-Availability/HA-self-hosted.png" alt="overview" /></p>

<p>Let’s talk about each of them in detail.</p>

<h2 id="compute-kubernetes">Compute: Kubernetes</h2>

<p><a href="https://kubernetes.io/">Kubernetes</a> is a container orchestration system. Think of it as Docker but across machines. You define what you want to run in the format of YAML or Json, including how much CPU, memory, and storage to use, then Kubernetes will find a node that fits your needs to run your container. It also tries to keep the current system state that meets your definition. For example, if there is a node failed and your service’s container is on it, Kubernetes will try to find another node to start the container so that the state meets the definition. So if the service itself doesn’t have any state between restarts, you get HA for free using Kubernetes.</p>

<p>I must have some warnings about using Kubernetes. It’s a complex project that is used by many big players. It’s not very easy to set up, maintain or upgrade. You need lots of knowledge to make it work. It has so many open issues that your particular needs are most likely not prioritized. While it’s open source so that you can modify the code to meet your use case, and I’ve had good experience contributing code in the early days, the recent experience is not so good anymore: you may need to attend some discussions to push your change instead of async online discussion. That is a lot for a causal contributor. So I end up maintaining a custom branch of Kubernetes with the changes I need locally, which is also a lot for average users.</p>

<p>Even though Kubernetes is heavy, I still think it’s a good tool even for small deployments since it’s already the industry standard. If you want to dedicate the maintenance of the Kubernetes cluster to a third party in the future, you can find lots of providers very easily. And you can just migrate your services to a different cluster without much effort since you’ve already defined the deployment in a language that any Kubernetes cluster can understand.</p>

<p>If you decide Kubernetes is the way to go, Kubernetes can be deployed with Kubeadm. Here is the <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/">official document</a> about how to deploy a Kubernetes cluster. Make sure to finish the <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/">HA setup</a> so that Kubernetes can survive even if some nodes are down.</p>

<p>Because of the nature of multiple nodes, every time the service restarts, the container can end on a different machine. So if your service needs to store anything on a disk, the data can get lost if you use the local disk of the machine. There are a few solutions for this:</p>

<ul>
  <li>Just don’t store data on a disk:
    <ul>
      <li>Store it in a database instead. But this is not an option for a service we don’t write and control.</li>
      <li>Send the files to another system. For example, you can send all the logs to something like Elastic Search so that it’s acceptable to lose logs after the container is restarted.</li>
      <li>Use <a href="https://kubernetes.io/docs/concepts/configuration/configmap/">ConfigMaps</a> for configuration files so that you can access them on any machine.</li>
    </ul>
  </li>
  <li>Bind the service to a specific node so that the container will run on that machine all the time. This needs the service itself to have HA built in so that when that node fails, the containers on the other nodes can still serve traffic. We will see an example of this in Cockroach DB setup.</li>
  <li>The last option is to use a distributed storage system that can be accessed from every machine. We will use GlusterFS for it in this setup.</li>
</ul>

<h2 id="storage-glusterfs">Storage: GlusterFS</h2>

<p><strong>WARNING: Gluster integration for Kubernetes has been removed since Kubernetes 1.26. You can use CephFS instead. Or check <a href="https://github.com/kadalu/kadalu/">Kadulu</a> if you still want to use GlusterFS.</strong></p>

<p><a href="https://www.gluster.org/">Gluster</a> is a distributed storage system. Once you created a GlusterFS volume, you can mount it to a machine just like NFS. The difference is the volume is backed by multiple machines so if even one of the machines fails, the volume is still usable. Kubernetes could mount a GlusterFS volume for containers as well. Sadly, Kubernetes has removed this support since version 1.26. But I’ve had this setup for a while and is still using an older version of Kubernetes, so I’ll still list GlusterFS as a solution here. The documents are still available for older versions. <a href="https://v1-24.docs.kubernetes.io/docs/concepts/storage/volumes/#glusterfs">Here is an example for Kubernetes 1.24</a>. You can select “versions” on the upper right to match your Kubernetes installation. CephFS is another distributed storage system, but it’s less user-friendly than GlusterFS in my opinion since the setup is more complex and it’s harder to mount it locally and explore it like a normal Linux file system. <a href="https://github.com/kadalu/kadalu/">Kadulu</a> seems to be another option if you still want to use GlusterFS, but I’ve never used it and I’m not sure if it’s production ready or not.</p>

<p>See <a href="https://docs.gluster.org/en/latest/Install-Guide/Overview/#what-is-gluster-without-making-me-learn-an-extra-glossary-of-terminology">the official install guide</a> for how to install Gluster and set it up. Most of the Linux distros already have the Gluster in the repo so you can install it by the package manager, and configure it based on the official document. Be aware you need to reserve a separate partition just for Gluster.</p>

<p>When creating a Gluster volume for use with Kubernetes, make sure to create it with at least 3 replicas so that you have HA for this volume. One of the replicas can be “<a href="https://docs.gluster.org/en/v3/Administrator%20Guide/arbiter-volumes-and-quorum/">arbiter</a>”, which means it’s only used for checking consistency and doesn’t store any actual data. So the data is only duplicated across 2 machines instead of 3 to save some space. Here is an example command to create such a volume:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>sudo gluster volume create &lt;volume-name&gt; replica 3 arbiter 1 &lt;host1&gt;:&lt;glusterfs-path&gt; &lt;host2&gt;:&lt;glusterfs-path&gt; &lt;host3&gt;:&lt;glusterfs-path&gt;
</pre></div>
</div>
</div>

<h2 id="database-cockroachdb">Database: CockroachDB</h2>

<p>Even though we can make persistent work with distributed storage, it’s better to avoid it if possible because of the setup complexity and performance impact. (This is more of the case of a self-hosted solution, distributed storage from cloud providers is very easy to use, and is also used by the VM so there is no difference in performance). We’ve listed some options above. In this section, we will look at how to create a database for the services to use so that they don’t need to store data on disks.</p>

<p>Here I will use CockroachDB as an example. But this introduction should help you to set up other similar systems like Elastic Search. Cockroach DB is a distributed database that is compatible with PostgreSQL. It’s built with HA in mind, so it has good guarantees and is easy to set up. I’ve checked lots of HA solutions for PostgreSQL and all of them have less guarantee (lots of them have no information about the consistency and availability level they provide, and I found them half-baked with a closer look) while are much harder to set up. I’ve written <a href="/2018-07-29-A-Review-on-Spanner-and-Open-Source-Implementations.html">a blog about Spanner that also talks about Cockroach DB</a> if you are interested in more details. Overall I have a good impression of it: the tech writings are solid, and the support is nice: when I have an issue and report it in the forum, the response is usually very quick and useful even though I’m just a free user.</p>

<p>CockroachDB has <a href="https://www.cockroachlabs.com/docs/stable/kubernetes-overview.html">an official document</a> about how to install it on Kubernetes. It’s using <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a>. Here is <a href="https://raw.githubusercontent.com/cockroachdb/cockroach/master/cloud/kubernetes/cockroachdb-statefulset.yaml">one of the configurations it uses</a>. However, I still find there are too many limitations in StatefulSets so I deployed it in my own way:</p>

<ul>
  <li>Each CockroachDB instance is in its own StatefulSets with only 1 replica.</li>
  <li>Each of the instances is bound to the physical node with <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">PodAffinity</a>. So that each instance will only ever run on a specific host. In this way, we can just use the local disk as the storage because it will never run on a different host.</li>
  <li>Each CockroachDB instance has its own <a href="https://kubernetes.io/docs/concepts/services-networking/service/">service</a> defined so that they can communicate with each other.</li>
  <li>Copy the parameters from the official configuration and adjust them based on your use case.</li>
</ul>

<p>With a setup like this, it’s like installing CockroachDB on physical nodes but managed by Kubernetes. You don’t need to worry about distributed storage. When a node fails, a CockroachDB instance will also fail. But since CockroachDB itself has HA enabled, the whole CockroachDB cluster is still alive. Here is an example of the Kubernetes resources in my setup:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span># Pods
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>NAME                READY   STATUS    RESTARTS   AGE
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>pod/cockroach01-0   1/1     Running   1          4d11h
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>pod/cockroach02-0   1/1     Running   5          4d11h
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>pod/cockroach03-0   1/1     Running   0          4d11h
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span># StatefulSet
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>NAME          READY   AGE
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>cockroach01   1/1     298d
<span class="line-numbers"><a href="#n12" name="n12">12</a></span>cockroach02   1/1     298d
<span class="line-numbers"><a href="#n13" name="n13">13</a></span>cockroach03   1/1     298d
<span class="line-numbers"><a href="#n14" name="n14">14</a></span>
<span class="line-numbers"><a href="#n15" name="n15">15</a></span># Service
<span class="line-numbers"><a href="#n16" name="n16">16</a></span>
<span class="line-numbers"><a href="#n17" name="n17">17</a></span>NAME                              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                          AGE
<span class="line-numbers"><a href="#n18" name="n18">18</a></span>service/cockroach01-cockroachdb   ClusterIP   None           &lt;none&gt;        26257/TCP,8080/TCP               298d
<span class="line-numbers"><a href="#n19" name="n19">19</a></span>service/cockroach02-cockroachdb   ClusterIP   None           &lt;none&gt;        26257/TCP,8080/TCP               298d
<span class="line-numbers"><strong><a href="#n20" name="n20">20</a></strong></span>service/cockroach03-cockroachdb   ClusterIP   None           &lt;none&gt;        26257/TCP,8080/TCP               298d
<span class="line-numbers"><a href="#n21" name="n21">21</a></span>service/cockroachdb               ClusterIP   10.96.70.142   &lt;none&gt;        26257/TCP,8080/TCP               269d
<span class="line-numbers"><a href="#n22" name="n22">22</a></span>service/cockroachdb-public        NodePort    10.108.23.98   &lt;none&gt;        26257:30005/TCP,8080:30006/TCP   269d
</pre></div>
</div>
</div>

<p>You can see there is a separate StatefulSet for each of the CockroachDB instances, and a service for each of them for internal communications (with the name pattern <code>cockroach**-cockroachdb</code>). Service <code>cockroachdb</code> is for the use in Kubernetes cluster, and service <code>cockroachdb-public</code> is used by the service outside of the Kubernetes cluster (can be disabled if not needed) so that you can see the dashboard from your browser.</p>

<p>It may seem to have more Kubernetes definitions to write with such a method. But remember, while Kubernetes accepts YAML or Json format, how to prepare the definition can be flexible: you can use your favorite programming language to construct the definition and pass it to Kubernetes with a <a href="https://kubernetes.io/docs/reference/using-api/client-libraries/">client library</a>.</p>

<p>The upgrade of CockroachDB is very easy as well. Make sure to check the official release notes and upgrade guides first, but normally the upgrade is just to patch each of the StatefulSet with a newer version of Docker image, for example:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span># run this command for every stateful set
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>kubectl patch statefulset cockroach01 \
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>--type='json' \
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>-p='[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/image&quot;, &quot;value&quot;:&quot;cockroachdb/cockroach:v22.2.5&quot;}]'
</pre></div>
</div>
</div>

<h2 id="network-ingress">Network Ingress</h2>

<p>Once we have everything deployed in the cluster, the last step is to expose our service to the public Internet so that everyone can use it. Here we list two options based on the use case.</p>

<h3 id="cloudflare-tunnel">Cloudflare Tunnel</h3>

<p><a href="https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/">Cloudflare Tunnel</a> is basically a reverse proxy that forwards the traffic from the public Internet to your service. There is a daemon called cloudflared running in Kubernetes. Cloudflare will forward the traffic from clients to cloudflared and cloudflared will forward the traffic to the actual service. Check <a href="https://developers.cloudflare.com/cloudflare-one/tutorials/many-cfd-one-tunnel/">this doc</a> to see how it works with Kubernetes.</p>

<p>The upside of Cloudflare tunnel is that you don’t need to open any port to the public Internet at all. So it’s safer because there is no way to access your service without going to Cloudflare first. Cloudflare also provide some tools to mediate attacks like DDoS.</p>

<p>The downside is it depends on a third-party provider. And it can see all the traffic. It only supports limited protocols. So if you want to avoid Cloudflare seeing your traffic or have a protocol that is not supported, you need a more generic way to do it.</p>

<h3 id="nodeport-with-virtual-ip-and-dynamic-dns">NodePort with Virtual IP and Dynamic DNS</h3>

<p>We need to really open a port to the Internet without something like Cloudflare Tunnel. First, we need to open a port on our nodes, this can be done by defining <a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport">NodePort</a> in Kubernetes’ service.</p>

<p>Once we have the port opened on the nodes, we need to open it to the Internet. How to do it depends on the Internet provider. Usually, you should be able to set up a port mapping from the router to an internal IP for a node. However, to make the setup HA, we shouldn’t map the port just to a single node since that single node can be down, we can set up Keepalived so that there is a virtual IP that always maps to a live node. If you’ve <a href="https://github.com/kubernetes/kubeadm/blob/main/docs/ha-considerations.md#options-for-software-load-balancing">set up HA for Kubernetes with Keepalived and HAProxy</a>, you should be already familiar with how to set it up.</p>

<p>When you open a NodePort, make sure you’ve configured all the protections like authentication and encryption since beyond that it’s public Internet and anyone can access it.
You may want to run Nginx or HAProxy in the Kubernetes cluster, use it as a reverse proxy and only expose it to the Internet so that it’s safer and you have more control over the public traffic.</p>

<p>The client also needs a way to find the IP address of your network. Depending on the Internet provider, the IP address can change from time to time. So we need dynamic DNS to bind the changing IP to a fixed DNS. <a href="https://github.com/ddclient/ddclient">ddclient</a> can do it automatically and supports lots of domain name providers.</p>

<p>After all of this, your service is open to the public Internet and can be accessed by anyone. But if desired, you can still use Cloudflare DNS with proxy enabled, so that the client will send requests to Cloudflare first and you can get protections from Cloudflare. In this case, since the SSL is terminated inside the Kubernetes cluster, Cloudflare will not be able to see the actual payload of the traffic.</p>]]></content><author><name></name></author><category term="Kubernetes" /><category term="GlusterFS" /><category term="CockroachDB" /><category term="tech" /><category term="high availability" /><summary type="html"><![CDATA[Cloud is popular these days. But sometimes we just want to host something small, maybe just an open source service for family and friends, or some self-built service that we are still experimenting on. In this case, the cloud can be expensive. We can just throw a few nodes at home and run it at a very low cost. But you don’t want the service down when some nodes failed, at least the service should be available when you upgrade and reboot the nodes because it can happen very frequently. In this article, I will talk about how to build high available infrastructure so that the service can be alive even when some nodes are down.]]></summary></entry><entry><title type="html">My 2022 in Review</title><link href="https://www.binwang.me/2023-01-26-My-2022-in-Review.html" rel="alternate" type="text/html" title="My 2022 in Review" /><published>2023-01-26T00:00:00-05:00</published><updated>2023-01-26T00:00:00-05:00</updated><id>https://www.binwang.me/My-2022-in-Review</id><content type="html" xml:base="https://www.binwang.me/2023-01-26-My-2022-in-Review.html"><![CDATA[<p>It’s 2023 now! 2022 has ended. It has been 2 years since I wrote <a href="/2021-01-26-My-2020-in-Review.html">the last yearly review</a>. So this article would be more like 2021 and 2022 in review.</p>

<p>There are lots of things that happened in the last two years, if not more than the two years of 2019 and 2020. That includes both my personal life and work life, both individual level and national level. So let’s talk about them one by one.</p>

<p>My personal life has changed a lot in the past two years. My wife and I obtained permanent resident status in Canada in mid-2021, and bought our first home in Toronto. Those two things are a big relief to me: I lived in Beijing for more than 7 years since graduating from university, but I never saw it as a long-term home. Needless to say, the apartments in Beijing are too expensive. There are also lots of restrictions because of the Hukou system: Hukou basically means “citizenship” for a region in China. Without it, the biggest problem is the education of the children. While the children can enroll in schools in Beijing and be educated there, they still need to take Gaokao (the national college entrance examination) as a resident of my hometown, where their Hukou belongs (no, being born in Beijing wouldn’t get them a Beijing Hukou). The eligible scores for colleges vary by region, since the number of students admitted from each region is different and each region’s population is also different. In this case, good colleges usually admit much fewer students from my hometown province, even though the province has a much larger population. As a result, a much higher score is needed to be accepted by the same college. In my opinion, this puts the children into an unfair game since the education they received is not prepared for such a level of competition. (It’s already an unfair game for the students from regions like my hometown province, but for better or for worse, the education system there is prepared for it). While Gaokao is still the way for most Chinese to enroll in a college, there are some other options like studying aboard, which requires studying in a private school for preparation. But all the other paths need much more financial support. This is only the biggest problem, not to mention other inconveniences without Hukou like retirement or even buying a home or a car. In general, without it, you won’t be treated equally by the government, even though you’ve paid lots of tax and social insurance. You may ask, why not get one if it’s so important? Because it’s even harder than immigrating to another country. The only practical way other than marriage is to join a state-owned company or a government department just after graduating from college. Once you miss that opportunity, it’s nearly impossible for average people to get it.</p>

<p>Anyway, even if I can solve all the problems above, Beijing has changed in the last few years when I was there, to the level that I don’t want to live there. The government started to focus on Beijing’s role as China’s capital, aiming for a place like Washington. There is even a “one thousand years” plan to move non-capital functions to a small town called Xiong’an. However, Beijing is not a city that is built for the pure purpose of political capital from scratch. It has a long history, multiple functions, and diverse residents. But the average people are not a priority in the government’s decision. It started to ban commercial usage at the lower level of resident buildings (such commercial usage was encouraged during Beijing’s 2008 Olympic games to make the city more vibrant), demolish restaurants and shopping malls, ban street foods, and so on. Lots of historical buildings were refurbished with shiny walls and unified shop signs. The policy reached an extreme when it began to banish “low-end” people (the phrase “clean up low-end people” was literally used in official publications and slogans). During the winter of 2017, using a fire incident as an excuse, the government started a movement to demolish “slums”, and ban some of the rental rooms which have the proper licenses. The movement caused lots of people didn’t have a place to live during the cold winter. As a result, the city as a whole seems to be dying. While the parks were grander, the streets were also quieter, and more people were leaving. It’s so sad to see a city I lived and loved transferred to a place like that. But my view doesn’t represent the view of big brother. Cai Qi, the leader of Beijing at the time (the leader of a Chinese city is not the mayor, instead, it’s the head of that city’s communist party), used to be seen as an open-minded official because of his interaction with people on social media (which is very rare in China), a close comrade of Xi, entered the standing committee last year and became one of the most powerful political figures in China.</p>

<p>Another bigger event also happened in the same winter. It made me not only want to leave Beijing but also China itself. It was an evening with a beautiful sunset, I went for a walk with my girlfriend to Dong Zhi Men – East Straight Gate of ancient Beijing, a place with a combination of historical buildings, vibrant shopping malls, and sky crawlers. In front of a shopping mall, I saw a large billboard displaying news of amendments to China’s Constitution. The news was pure text with a large font size, which seemed out of place compared to the modern buildings nearby. It’s only a draft of amendments, but there is an important one that caught my eye: it removes the presidential term limits! I’ve never had high hopes for China’s democracy. When some people talk about the ingenuity of Deng Xiaoping’s design of skip-generation appointments for power transfer, I always think it’s childish. However, I never thought the transfer of power would break so quickly. In my opinion, it’s like the last barrier that prevents China from becoming a pure dictatorship country again. Once that barrier is broken, it would lead to a downfall in the foreseeable future. Even if it’s not the last barrier, the change shows Xi’s ambition to become a dictator in the most obvious way, which will take China in the wrong direction.</p>

<p>It was a cold winter day when the draft was passed. Living in a rented apartment, which is close to a military compound and just less than a 30-minute drive to the power center of China, I wrote down the following sentence after a sleepless night:</p>

<blockquote>
  <p>京城一夜风吹雪，万籁无声国岁寒。</p>
</blockquote>

<blockquote>
  <p>Beijing at night, wind and snow blowing,</p>

  <p>All creatures silent, the nation’s cold in growing.</p>
</blockquote>

<p>It seems weird to write about so many things that didn’t happen in the last two years. But they are important contexts to understand the significance of being a permanent resident of Canada. By no means Canada or Toronto is perfect. It has its own problems, but I’d rather live in a sociality where people can discuss the problems and hold the government accountable. So after moving to Toronto, I feel like this is a place that I can live and call home, and finally made it happen. Now, we’re having a baby on the way, which makes me excited and nervous at the same time.</p>

<p>Also because of the PR status, I was able to change the job without worrying about the visa. I left Amazon mainly because the on-call was too stressful to the point where it affected my personal life. I moved to a smaller company and started to do database related stuff again. The workload is much lighter than at Amazon, and I feel I have recovered a lot of energy because of it.</p>

<p>Outside of work, I continued to work on my side project which I mentioned in 2020’s year-end review. It’s now a usable product called <a href="https://rssbrain.com">RSS Brain</a>. It’s an RSS reader. I’ve mentioned it in past blogs so I will not go into details again. But it has been the app that I use the most every day. And it feels so good to write software that meets my own needs and maybe able to help others at the same time. I will eventually open source it, but before that, I still have some plans that need to be finished first. So open source the code became a lower priority. The development has been slowed down after most of the features have been finished: that’s a pattern of my past projects. But for the past projects it often slowed down before they were actually usable. At least for RSS Brain, it’s a usable app now. Hopefully, I still have the passion to continue developing it and finish all of my planned features.</p>

<p>While developing RSS Brain, I also developed and open sourced a <a href="/2022-05-02-A-Library-to-Make-It-Easier-to-Use-Scala-with-GRPC.html">Scala library</a> that makes writing gRPC service with Scala much easier. I’m very happy about the library because of its non-invasive nature. Even though I can see it’s controversial in some projects, I feel like it’s a very useful library that can help a lot in my future projects.</p>

<p>Another thing I want to review, which I value a lot, is this blog. In my opinion, ideas and the interaction of ideas are the most important factor for the advancement of human beings. The most important platforms that I can use to share ideas are open source projects and articles. Work is valuable, but I don’t think the work I’m doing can inspire lots of people because not a lot of people can see it. So I always treat this blog as an important platform for me to contribute to the world, even though it’s small, it shares ideas directly. Who knows if someone can get some inspiration from it, had some ideas on top of it then inspires someone else, and eventually create something revolutionary. In the past two years, I have posted a few articles. The frequency is neither too high nor too low. Quality is similar: neither too high nor too low overall. On some level, I’ve happy I left something, but at the same time, I feel like there is still a lot of room for improvement. There are some topics I wanted to write about but ended up not doing it. One factor is the RSS Brain project I was working on, but I still think there is still a lot of time I’ve wasted which can be used on writing articles.</p>

<p>Other than the output of knowledge and ideas, there is also the input part. One important source is reading books. My readings decreased a lot in the past two years in terms of both quantity and quality. Part of it may be because of working from home: I’ve read lots of books during my commute. I wasted too much time watching videos that didn’t even bring me relaxation or happiness in the end. I should be more aware of that and remember to find some books to read when I’m bored.</p>

<p>Other than the things that happened to me or around me, the events that happened in China also greatly affected me: the Covid restrictions became more and more extreme, preventing me from visiting my family for more than 3 years and greatly impacting the daily life of Chinese people. People were controlled by massive testing and surveillance in the guise of monitoring Covid cases. Cities implemented absurd lockdowns from time to time. The level of lockdown is so strict that lots of people were not able to get enough food or basic medicine. Some people were dead because of the lack of health care. Some people killed themselves because of depression. People are transferred into quarantine centers. Parents and kids were forcibly separated. The better quarantine centers are built from sports centers that have 24-hour overhead lights, the worse ones can be just outdoor parking lots or even public washrooms. During the transfer, some of them were packed onto trains without enough capacity, leading to people sleeping on luggage racks. There was even an incident that happened on a transfer bus, which killed more than 20 people. Government workers went into people’s homes to sterilize, which often destroys furniture and kills pets. Because of the lack of food and infrastructure, people fled lockdown regions on foot, walking tens of miles to airports and train stations, and in some cases, even hundreds of miles on the highway to their hometowns. Eventually, a fire broke out in Xinjiang and killed ten people, because they were locked into the building and were unable to escape. This tragedy sparked protests all around China. People held blank papers to protest without a word, but everyone, including the police and the government, knew what they wanted to say. So some of them were beaten and arrested. It’s like a dark Soviet Union joke but happened in real life China. Then, suddenly, all the Covid restrictions were lifted without any preparation. This led to a medicine shortage and a surge in Covid cases. Everyone I knew in China got Covid, except for those who stayed at home all the time. mRNA vaccines are still not approved because it’s not produced domestically. The strong government which controlled every aspect of people’s life during lockdowns are missing now, the cases and deaths are not even properly counted anymore. Everyone is left in the dark to fight for themselves.</p>

<p>There are so many tragedies that happened in China in the past two years that it’s impossible to write all of them down in this short article. Almost everyone’s life is affected by the bad economy if not by the Covid policies directly. Most of my friends and family members are in China, and my culture is also from there. Seeing the events unfold during the past two years has made me heartbroken. It also makes me frustrated because there is nothing much I can do. As the new year comes, hope everything can be better. And I’ll think more about what I can do, even if it’s just the smallest help.</p>]]></content><author><name></name></author><category term="life" /><summary type="html"><![CDATA[It’s 2023 now! 2022 has ended. It has been 2 years since I wrote the last yearly review. So this article would be more like 2021 and 2022 in review.]]></summary></entry><entry><title type="html">How RSS Brain Shows Related Articles</title><link href="https://www.binwang.me/2022-12-03-How-RSS-Brain-Shows-Related-Articles.html" rel="alternate" type="text/html" title="How RSS Brain Shows Related Articles" /><published>2022-12-03T00:00:00-05:00</published><updated>2022-12-03T00:00:00-05:00</updated><id>https://www.binwang.me/How-RSS-Brain-Shows-Related-Articles</id><content type="html" xml:base="https://www.binwang.me/2022-12-03-How-RSS-Brain-Shows-Related-Articles.html"><![CDATA[<p>In the new version of <a href="https://rssbrain.com">RSS Brain</a>, I added a new feature to show related articles from folders or feeds of your choice, instead of only show related articles from all feeds.</p>

<p>I have mentioned this feature in the <a href="/2022-10-29-RSS-Brain-Yet-Another-RSS-Reader-With-More-Features.html">previous blog post</a>. I also mentioned the algorithms in RSS Brain will be transparent. So in this article, I will talk about the details of this feature, how it can be used, the algorithm that backs it and how RSS Brain implements it.</p>

<h2 id="what-is-the-feature">What is the Feature</h2>

<p>The feature is very straightforward: when you are reading an article from a feed, RSS Brain will show related articles at the end. It’s a very common feature. What makes RSS Brain different are two things:</p>

<ol>
  <li>You can configure where the related articles come from.</li>
  <li>The implementation is transparent. That includes both the algorithm, which I will introduce in this article, and the code, which I will open source it in the future.</li>
</ol>

<p>How do you configure the related articles? By default, there will be no recommended articles. But there is a button to let you add a recommendation section at the end of an article:</p>

<p><img src="/static/images/2022-11-27-How-RSS-Brain-Show-Related-Articles/screenshot_add_section.png" alt="screenshot-add-section" /></p>

<p>Once you click the “Add More” button, it will show all your folders and feeds, with another “All Subscriptions” option. If you select “All Subscriptions”, this recommendation section will find related articles from all your subscriptions. If you select a folder or feed, it will find them from the folder or feed of your choice.</p>

<p>You can add multiple recommendation sections. After that, each recommendation section will be shown after the article. The screenshot below is an example that has a section that shows related articles from a folder called “local-form”, and another section that shows related articles from all the user’s subscriptions.</p>

<p><img src="https://rssbrain.com/images/screenshot_multi_recommend.png" alt="screenshot-sections" /></p>

<p>The recommendation configuration is attached to the feed that this article belongs. So if you read another article from the same feed, it will show related articles from the same recommendation sections.</p>

<h2 id="why-the-feature-is-useful">Why the Feature is Useful</h2>

<p>The first way I use it is to find more discussions about this article. For example, you can configure it to show related articles from Hacker News, some Twitter account or from a sub Reddit. So that you know what other people think about this article, or about this topic.</p>

<p>Another way to use it is to check the coverage from different sources. For example, you can add a recommendation section that contains left wing media and another section that has right wing media, so that you can compare the coverage and get a whole picture.</p>

<p>Last but not least, the recommendation is useful in its traditional way: just show related articles about the same topic so that you can read more details about the same topic. I often just read folders that has high quality sources, and when I want to know more, I will add a recommendation section that has more sources and find articles to read from there.</p>

<h2 id="how-the-feature-is-implemented">How the Feature is Implemented</h2>

<p>The algorithm to find recommended feature is content based instead of user based. RSS Brain doesn’t collect any user’s information in order to make personalized recommendation. It just find the related articles by how similar they are.</p>

<p>Each of the article can be represented by a term vector. The values in this vector are scores of the terms. For example, if article A has the content of “apple boy cat” and article B has the content of “apple boy dog”, the term vectors for each of the articles can be:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>   apple, boy, cat, dog
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>A: [0.5,  0.5,   1,   0]
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>B: [0.5,  0.5,   0,   1]
</pre></div>
</div>
</div>

<p>The score is computed by <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">tf-idf</a>, which is basically a score considers both the frequent of the term in this article, and the frequent in all the articles: the more frequent it is in this article, the bigger the score since it can better represent the article. However the more frequent it is in all the articles, the score should be smaller since it’s not unique enough to represent the feature of this article. Once we have a term vector for each of the article, we can find the similarity by counting the distance between these vectors.</p>

<p>So we have the algorithm, how RSS Brain implements it in the code? We are using <a href="https://www.elastic.co/">ElasticSearch</a> under the hood. It’s widely used and open source. So for the APIs that RSS Brain is using, you can check the code for implementation details if you want. It has <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-termvectors.html">an API to find the term vector</a> for an article, and we use the scores in the term vector to do a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-boosting-query.html">boosting query</a>, which means do a search with different weights for each query term. For example, in the example above, if we want to find related articles for article A, we would find its term vector first, then convert the term vector into a boosting query that searches related articles by the query <code>apple^0.5 boy^0.5 cat^1</code>.</p>

<p>There are more details in this implementation, like adding filter on feed or folder in the query, limiting the term vector size and so on. The details can be found in the code once it’s open sourced. But the main idea doesn’t change.</p>

<p>With this simple and content based recommendation algorithm, instead of letting a black box AI decides what content are shown to you, I believe users can understand why an article is recommended, and judge whether the recommendation can benefit them or not.</p>]]></content><author><name></name></author><category term="RSS" /><category term="project" /><category term="RSS Brain" /><category term="digital life" /><summary type="html"><![CDATA[In the new version of RSS Brain, I added a new feature to show related articles from folders or feeds of your choice, instead of only show related articles from all feeds.]]></summary></entry><entry><title type="html">RSS Brain: Yet Another RSS Reader, With More Features</title><link href="https://www.binwang.me/2022-10-29-RSS-Brain-Yet-Another-RSS-Reader-With-More-Features.html" rel="alternate" type="text/html" title="RSS Brain: Yet Another RSS Reader, With More Features" /><published>2022-10-29T00:00:00-04:00</published><updated>2022-10-29T00:00:00-04:00</updated><id>https://www.binwang.me/RSS-Brain-Yet-Another-RSS-Reader-With-More-Features</id><content type="html" xml:base="https://www.binwang.me/2022-10-29-RSS-Brain-Yet-Another-RSS-Reader-With-More-Features.html"><![CDATA[<p>I’ve written yet another RSS Reader called RSS Brain recently. If you just want to take a quick look at the main features and try it, <a href="https://rssbrain.com/">the official website</a> is the best place to start. Though most of the main features are already finished and I’ve used it personally for a few months, be aware it’s still in beta stage and hasn’t been tested by a larger group yet. So please let me know if you run into any issues. I will open source it as well in the future (for non commercial usage) but it’s not ready yet for now.</p>

<p>In this article, I’ll give an introduction to the motivation behind it. You may have a better reason to try it after read this.</p>

<p>RSS is just a protocol to aggregate news. But how to organize them depends on the RSS reader.  While lots of RSS readers have the basic organization features like folders to manage the feeds, it is not enough. Not a lot of them have taken new technologies from fields like information retrieval and machine learning. Even a few them have done that, they are using some complex and black box algorithms which we don’t know what is going on behind the sense. I’ve written <a href="/2020-08-02-What-Is-Wrong-abount-Recommendation-System.html">an article</a> that talks about the harm of letting black box algorithm decide what we read. So in RSS Brain, I use transparent algorithms to help organize the feeds and articles without use clicks or read time as the optimization target, so that we can have a good understanding about why it behaves in that way, in that way we can decide whether it’s good for us or not.</p>

<p>Here are some pain points in the traditional RSS readers and how I solve them in RSS Brain with transparent algorithms.</p>

<h2 id="1-traditional-rss-readers-dont-rank-articles-by-weight">1. Traditional RSS Readers Don’t Rank Articles by Weight</h2>

<p>The mainstream RSS protocol don’t have a field to indicate how important an article is in a feed. It is very different from news papers or news websites, which have head line for the most important news. When it comes to RSS, there is no difference between the importance of the articles. It is a smaller problem when there are not so many articles in a feed, but for feeds that have lots of articles, or forums like Reddit and Hacker News, that is a very big problem.</p>

<p>Reddit and Hacker News sort the articles by both votes and timeline. The algorithm is relatively transparent (although less and less transparent in the case of Reddit). Some people don’t want the rank to be effected by other users at all, but I’m okay for other this kind of ranking for these reasons:</p>

<ol>
  <li>The posts are just too many to read them all without some kind of priority.</li>
  <li>The community is part of the forum. If I like a subreddit, it means I trust the community and mods to promote high quality posts. Otherwise I will just not subscribe to it.</li>
  <li>If you think about it, the traditional media also rank news for you, even khough it is selected by more professional people. But like I said in point 2, if you trust the community of a subreddit, then it doesn’t make a big difference.</li>
</ol>

<h3 id="11-ranking-algorithm">1.1 Ranking Algorithm</h3>

<p>Some readers have ranking algorithms to sort the articles for you, but those are mostly black box algorithms, which is harmful like I said before. In RSS Brain, we will take the votes from the source, and sort it with an algorithm that is similar to Reddit:</p>

\[S_{vote} = log_{10}v\]

\[S_{time} = { time \over C }\]

\[S = S_{vote} + S_{time}\]

<p>Which <code>v</code> means how many votes this article has. <code>time</code> means when this article is posted. <code>C</code> is a constant number that indicate how much wait time contributes to the whole score. I’m using 12.5 hours in my implementation.</p>

<p>For \(S_{vote}\), it means the first 10 votes will get the most weights, the next 100 votes has the same weight as the first 10 votes, and so on. It’s not a perfect algorithm but works good enough, and is easy to implement. Most importantly it’s very easy to be understood.</p>

<p>Once you have the score, you have the option to sort the articles in a folder or feed based on score instead of time.</p>

<p><img src="https://www.rssbrain.com/images/sort_post.png" alt="ranking" /></p>

<h3 id="12-data-source">1.2 Data Source</h3>

<p>Another obvious problem is how to get the votes. The traditional RSS protocols don’t have a specific field for vote count, luckily atom protocol has the ability to extend it with custom tags. So RSS Brain will parse these tags in <code>&lt;entry&gt;</code> tag if exists: <code>&lt;*:comments&gt;</code>, <code>&lt;*:upvotes&gt;</code>, <code>&lt;*:downvotes&gt;</code>. <code>*</code> can be any namespace. I’ve added <a href="https://docs.rsshub.app/en/joinus/quick-start.html#submit-new-rss-rule-code-the-script-produce-rss-feed-interactions">these fields</a> to a very popular RSS generator <a href="https://rsshub.app/">RSSHub</a> in namespace <code>RSSHub</code>. So if someone include these fields when implement a RSS, RSS Brain will be able to parse it and use <code>upvotes - downvotes</code> as the votes. If they are not available, RSS Brain will try to use <code>comments</code> instead.</p>

<p>Reddit and HackerNews are two of my main daily reading websites, and I believe it’s the same for many other people, so I also included an implementation to fetch the posts from Reddit and HackerNews JSON API. You can just input HackerNews or subreddit URL when add a new source, it will has an option to use the JSON feed when it tries to find the feeds. I know it’s not a very standard way, but it’s easier for me to implement rather than do it in RSSHub. Once there is a RSS implementation that contains the fields above, you have no trouble to use the RSS feed.</p>

<p>Since RSS Brain is parsing the tags in the RSS feed, for data sources other than forum, the RSS generator can also try to generate the votes in some way if necessary. For example, some score based on the article position of the website, the font size and so on. I haven’t done any experiment with it yet but I think it’s an interesting idea to explore.</p>

<h2 id="2-filter-articles-with-search-terms">2. Filter Articles With Search Terms</h2>

<p>Ranking the articles is one way to get the interested aritcles pop up. Another way is to filter the articles: sometimes we only care about a topic in a feed. In traditional RSS readers, there is no easy way to filter the topic out if there is no feed for that topic. In RSS Brain, you can define a search term on a folder, so that when you check the articles in this folder, it will only show the articles that matches the search term.</p>

<p>For example, you’ve got a few news feed, but only care reports about the war between Ukraine and Russia, then you can just set the search term as <code>"Ukraine" AND "Russia"</code> on the folder and enable search filter. After that, when you click on the folder to see articles, it will only show the articles about those news.</p>

<p><img src="https://www.rssbrain.com/images/filter_folder.png" alt="filter folder" /></p>

<p>By the way, you can also just search in a folder or source. This is very useful for me to search some news, since the search quality of search engineers for some news is very bad, especially for Chinese news, which very suspicious news agencies are always shown in the top results.</p>

<h2 id="3-show-related-articles">3. Show Related Articles</h2>

<p>This is a feature I found to be pretty interesting while Google News included it. But Google News selects the news source for you. In RSS Brain, you get get related articles from the feeds you subscribed, so that you can decide which sources are valuable to you instead of letting a big corp decide that.</p>

<p>Currently the implementation only has the ability to select from all your subscribed feeds. But even with this implementation, I find it is very useful in some ways:</p>

<ul>
  <li>When I check the news, the related articles will show a post related to this in forums like Reddit and HackerNews, so that I can check other people’s opinion.</li>
  <li>I can compare the report coverage from both left and right news agencies, to get a whole picture. I don’t use this feature a lot myself, but I know a lot of people like that and there are some popular apps do just that.</li>
  <li>I usually read news from a high quality source. But if I find some news to be interesting, I’d like to read more coverage on that. Those coverage doesn’t need to be high quality but might have more details.</li>
</ul>

<p>I have plan to extend this feature. (<em>Update: this feature has been implemented. Check the <a href="/2022-12-03-How-RSS-Brain-Shows-Related-Articles.html">blog post here</a>.</em>) It will be much better after that: RSS Brain will support related articles groups, so that you can show related articles not only from all the subscriptions, but also from the folders of your choice. So you can configure it to show related forum posts in one group, left/right news coverage on another group, and so on. I’ll also implement a feature to show related articles from a time range, so that it can filter the matches based on time, to make the news more related if you want.</p>

<h2 id="4-just-want-to-write-something">4. Just Want to Write Something</h2>

<p>Last but not least, I just want to write something and feel the happiness of coding. I really enjoy myself a lot while writing this project: coding, choosing the tech stack to use, setting up high availability cluster and database, and so on. Everything is so elegant because I can decide how to do it.</p>

<p>While enjoy myself during the development of the project, I find the product is pretty useful as well. So I decided to share it, and if it benefits more people I’m happier. As I said at the beginning of the article, I’ll open source the whole project and allow non commercial usages when it’s ready. But at the mean time, you can pay a monthly fee to use the software. The reason of this payment mode is two fold: I don’t want this software to be flooded with terrible ads, but I still need some revenue to keep the infrastructure running (for both hardware cost and my time). So I want to set some barriers at first to limit the user base, so that the users can have a better experience. Hope you enjoy this app and let me know if you run into any issues or have any question.</p>]]></content><author><name></name></author><category term="RSS" /><category term="project" /><category term="RSS Brain" /><category term="digital life" /><category term="news" /><summary type="html"><![CDATA[I’ve written yet another RSS Reader called RSS Brain recently. If you just want to take a quick look at the main features and try it, the official website is the best place to start. Though most of the main features are already finished and I’ve used it personally for a few months, be aware it’s still in beta stage and hasn’t been tested by a larger group yet. So please let me know if you run into any issues. I will open source it as well in the future (for non commercial usage) but it’s not ready yet for now.]]></summary></entry><entry><title type="html">Handle Apple In-App-Purchase Server Notification with Scala/Java</title><link href="https://www.binwang.me/2022-08-20-Apple-In-App-Purchase-Server-to-Server-Notification-Handling-with-Scala-Java.html" rel="alternate" type="text/html" title="Handle Apple In-App-Purchase Server Notification with Scala/Java" /><published>2022-08-20T00:00:00-04:00</published><updated>2022-08-20T00:00:00-04:00</updated><id>https://www.binwang.me/Apple-In-App-Purchase-Server-to-Server-Notification-Handling-with-Scala-Java</id><content type="html" xml:base="https://www.binwang.me/2022-08-20-Apple-In-App-Purchase-Server-to-Server-Notification-Handling-with-Scala-Java.html"><![CDATA[<p>When you write an app for iOS, publish it to Apple App Store and want to sell something within it, Apple makes it mandatory to use its own in app purchase framework for non consumable items and subscriptions. If the app has a server, it’s very usual that the server wants to know the payment events and have some followup logic with them. But how to do that? There is always an option to let the app send a request to server, but anyone can use the same endpoint to make false claims. To prevent this, Apple has a server to server notification mechanism: Instead the app itself, a server from Apple will send a request to your server to notify the payment events. Since the message is signed by Apple, you can make sure no one else can fake it by verifying the signature.</p>

<p>While this framework should in theory makes developer’s life easier, the lack of documentation makes it very painful to use. There is also little and often wrong information on the Internet about how to verify the signature, especially for a server written with Java related tech stack. So in this article, I will give an example about how to decode and verify the payment notification messages sent by Apple with Scala. The library we are using is <a href="https://connect2id.com/products/nimbus-jose-jwt">Nimbus JOSE + JWT</a> which is written in Java, so the method applies to other JVM languages as well. Hopefully this can help other developers who are facing the same problem.</p>

<h2 id="1-how-to-trigger-a-server-notification">1. How to Trigger a Server Notification</h2>

<p>Needless to say, to receive a payment notification you must initiate a payment from the app. There are various ways to do it and we will not discuss it in this article. But be aware there are two ways to test the in app payment: create a <a href="https://developer.apple.com/documentation/xcode/setting-up-storekit-testing-in-xcode">StoreKit configuration in Xcode</a> or <a href="https://developer.apple.com/documentation/storekit/in-app_purchase/testing_in-app_purchases_with_sandbox">use a sandbox environment</a>. Only the later one will trigger a server to server notification.</p>

<p>Another requirement is to set up the notification endpoint in Apple Connection settings. The endpoint is a https URL that Apple will send a http post request to. <a href="https://developer.apple.com/documentation/storekit/in-app_purchase/original_api_for_in-app_purchase/subscriptions_and_offers/enabling_app_store_server_notifications">Here</a> is the Apple document about how to do it.</p>

<h2 id="2-server-notification-workflow">2. Server Notification Workflow</h2>

<p>In order to better understand how to handle the notification message, let’s take a look at the server notification workflow first.</p>

<p><img src="/static/images/2022-08-20-Apple-In-App-Purchase-Server-to-Server-Notification-Handling-with-Scala-Java/iap-server-notification.png" alt="iap-server-notification" /></p>

<p>When Apple receives some payment information, whether it’s from the app, or from subscription renew, or subscription expiration or other events, it will try to send the event to your server by sending an http POST request. In order to make sure no other people can fake a request to the same http endpoint, Apple signs the request payload with a private key that only Apple has access, so that when your server received the message, you can verify it by Apple’s public key.</p>

<p>The way Apple signs the message is using a standard called <a href="https://www.rfc-editor.org/rfc/rfc7515.html">JWS</a>. This is a complex standard with multiple implementation options. To fully understand it you also need to know things like <a href="https://www.rfc-editor.org/rfc/rfc7518.html">JWA</a> and <a href="https://www.rfc-editor.org/rfc/rfc7517">JWK</a>. Apple has very little document about how to decode its own message other than throw this RFC page into the document. Even in support forums, their response is like “use your favourite crypto library”, which doesn’t really help anything.</p>

<h3 id="3-jws-overview">3. JWS Overview</h3>

<p>To make it easy, I will give a very simple overview of JWS, JWS has three parts: a header that contains metadata like keys and algorithm to use, the actual payload, and a signature:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>header (metadata)
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>-----------
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>payload (actual message we want, base64 encoded JSON)
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>-----------
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>signature (generated by applying crypto algrithm on payload with keys in header)
</pre></div>
</div>
</div>

<p>So in order to make sure the whole message is actually sent by Apple, we need to verify:</p>

<ul>
  <li>The signature is generated by the keys in header and the payload.</li>
  <li>The keys in header is generated by Apple.</li>
</ul>

<p>Since the payload is only base64 encoded, for a developer that is not familiar with JWS, even with the help of a JWS library, both verification steps can be easily missed since it only affects the verification, not the decoding of the message.</p>

<p>In the section next, we will have an example about how to decode the message while really verify the message is sent by Apple as well.</p>

<h2 id="4-decode-and-verify-notification">4. Decode and Verify Notification</h2>

<p>Here we are using <a href="https://developer.apple.com/documentation/appstoreservernotifications/app_store_server_notifications_v2">App Store server notifications v2</a>. Let’s say you’ve already got the http POST body from your configured endpoint:</p>

<div class="language-Scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>val responseBodyV2Str: String = ... // your code to get POST body from HTTP request
</pre></div>
</div>
</div>

<p><code>responseBodyV2Str</code> itself is not a JWS object but a JSON string like this:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>{&quot;signedPayload&quot;:&quot;eyJhbGciOiJFUzI1NiIsIng1YyI6WyJNSUlFTU....&quot;}
</pre></div>
</div>
</div>

<p>The value of <code>signedPayload</code> is the encoded JWS string we want to parse. So we need to get that value first. I’m using <a href="https://circe.github.io/circe/">circe</a> to parse the JSON string, but any method that can parse it and get the value is fine. In my case, I defined some classes based on the JSON structures so that we can parse them in a more type safe way.</p>

<div class="language-Scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>import io.circe.generic.extras._
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>import io.circe.parser
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>object ApplePaymentService {
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>  implicit val config: Configuration = Configuration.default.withDefaults
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>  @ConfiguredJsonCodec case class AppleResponseBodyV2(
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>    signedPayload: String,
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>  )
<span class="line-numbers"><a href="#n12" name="n12">12</a></span>
<span class="line-numbers"><a href="#n13" name="n13">13</a></span>  @ConfiguredJsonCodec case class AppleResponseBodyV2DecodedPayload(
<span class="line-numbers"><a href="#n14" name="n14">14</a></span>    notificationType: String,
<span class="line-numbers"><a href="#n15" name="n15">15</a></span>    subtype: Option[String],
<span class="line-numbers"><a href="#n16" name="n16">16</a></span>    data: ApplePayloadData,
<span class="line-numbers"><a href="#n17" name="n17">17</a></span>  )
<span class="line-numbers"><a href="#n18" name="n18">18</a></span>
<span class="line-numbers"><a href="#n19" name="n19">19</a></span>  @ConfiguredJsonCodec case class ApplePayloadData(
<span class="line-numbers"><strong><a href="#n20" name="n20">20</a></strong></span>    appAppleId: Option[String],
<span class="line-numbers"><a href="#n21" name="n21">21</a></span>    bundleId: String,
<span class="line-numbers"><a href="#n22" name="n22">22</a></span>    bundleVersion: String,
<span class="line-numbers"><a href="#n23" name="n23">23</a></span>    environment: String,
<span class="line-numbers"><a href="#n24" name="n24">24</a></span>    signedRenewalInfo: String,
<span class="line-numbers"><a href="#n25" name="n25">25</a></span>    signedTransactionInfo: String,
<span class="line-numbers"><a href="#n26" name="n26">26</a></span>  )
<span class="line-numbers"><a href="#n27" name="n27">27</a></span>
<span class="line-numbers"><a href="#n28" name="n28">28</a></span>  @ConfiguredJsonCodec case class AppleJWSTransactionDecodedPayload(
<span class="line-numbers"><a href="#n29" name="n29">29</a></span>    appAccountToken: String,
<span class="line-numbers"><strong><a href="#n30" name="n30">30</a></strong></span>    bundleId: String,
<span class="line-numbers"><a href="#n31" name="n31">31</a></span>    environment: String,
<span class="line-numbers"><a href="#n32" name="n32">32</a></span>    expiresDate: Long, // timestamp in ms
<span class="line-numbers"><a href="#n33" name="n33">33</a></span>    inAppOwnershipType: String,
<span class="line-numbers"><a href="#n34" name="n34">34</a></span>    originalPurchaseDate: Long, // timestamp in ms
<span class="line-numbers"><a href="#n35" name="n35">35</a></span>    originalTransactionId: String,
<span class="line-numbers"><a href="#n36" name="n36">36</a></span>    productId: String,
<span class="line-numbers"><a href="#n37" name="n37">37</a></span>    purchaseDate: Long, // timestamp in ms
<span class="line-numbers"><a href="#n38" name="n38">38</a></span>    quantity: Int,
<span class="line-numbers"><a href="#n39" name="n39">39</a></span>    transactionId: String,
<span class="line-numbers"><strong><a href="#n40" name="n40">40</a></strong></span>    `type`: String,
<span class="line-numbers"><a href="#n41" name="n41">41</a></span>    webOrderLineItemId: String,
<span class="line-numbers"><a href="#n42" name="n42">42</a></span>  )
<span class="line-numbers"><a href="#n43" name="n43">43</a></span>}
</pre></div>
</div>
</div>

<p>With the help with the classes and JSON parser, we can get the value of <code>signedPayload</code>. (I removed <code>\n</code> from the JSON string since it’s not valid to have newlines in JSON string, not sure why Apple’s request body has newline in it):</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>val responseBodyV2 = parser.parse(responseBodyV2Str.replace(&quot;\n&quot;, &quot;&quot;)).flatMap(_.as[AppleResponseBodyV2]).toTry.get
</pre></div>
</div>
</div>

<p>After get the JWS string, we can parse it with <a href="https://connect2id.com/products/nimbus-jose-jwt/examples">Numbus Jose + JWT</a> (follow the document to add this dependency into your project first):</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>import com.nimbusds.jose.JWSObject
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>import com.nimbusds.jose.crypto.ECDSAVerifier
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>import com.nimbusds.jose.crypto.bc.BouncyCastleProviderSingleton
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>import com.nimbusds.jose.jwk.ECKey
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>import com.nimbusds.jose.util.X509CertUtils
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>val jwsObject = JWSObject.parse(responseBodyV2.signedPayload)
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>val jwsCerts = jwsObject.getHeader.getX509CertChain.asScala.map(c =&gt; X509CertUtils.parse(c.decode()))
</pre></div>
</div>
</div>

<h3 id="41-verify-keys-in-jws-header-is-signed-by-apple">4.1 Verify keys in JWS header is signed by Apple</h3>

<p><code>jwsCerts</code> is a list of <code>X509Certificate</code>, which is the key chain in JWS header. A cert in the list can be verified by the cert behind it. And the last cert should be verified by Apple’s public key so that we can make sure the whole key chain is signed by Apple.</p>

<p>So let’s first get the root cert of Apple first: download <a href="https://www.apple.com/certificateauthority/AppleRootCA-G3.cer">Apple Root CA - G3 Root</a> from <a href="https://www.apple.com/certificateauthority/">Apple PKI website</a> and put it under your project’s <code>src/resources/certs</code> (or any where the program can read, we are just using it as an example here). Then we can read the Apple root cert with this code:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>val appleRootCa = X509CertUtils.parse(getClass.getResourceAsStream(&quot;/certs/AppleRootCA-G3.cer&quot;).readAllBytes())
</pre></div>
</div>
</div>

<p>With both the key chain and root cert, we can verify the key chain is both valid and signed by Apple:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>jwsCerts.sliding(2).foreach { x =&gt;
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>  x.head.verify(x.last.getPublicKey)
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>}
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>jwsCerts.last.verify(appleRootCa.getPublicKey)
</pre></div>
</div>
</div>

<p>It will throw exception if the verify doesn’t pass.</p>

<h3 id="42-verify-jws-is-signed-by-keys-in-jws-header">4.2 Verify JWS is signed by keys in JWS header</h3>

<p>Once we verified the keys in JWS header is signed by Apple, we need to verify JWS itself is signed by these keys. Since the <code>alg</code> field in this JWS header is <code>ES256</code>, we will use <code>ECDSAVerifier</code> to verify it:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>val jwk = ECKey.parse(jwsCerts.head)
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>val jwsVerifier = new ECDSAVerifier(jwk)
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>if (!jwsObject.verify(jwsVerifier)) {
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>  throw new RuntimeException(&quot;Apple JWS object cannot be verified&quot;)
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>}
</pre></div>
</div>
</div>

<h3 id="43-parse-the-payload">4.3 Parse the payload</h3>

<p>After verify the JWS is valid, we can start to parse the payload. I’m using the JSON parser and the structure I defined above. Please refer to Apple’s document about the actual fields in the payload:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>val responseBodyV2Payload = jwsObject.getPayload.toString
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>val responseBodyV2DecodedPayload = parser.parse(responseBodyV2Payload).flatMap(_.as[AppleResponseBodyV2DecodedPayload]).toTry.get
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>val transactionPayload = responseBodyV2DecodedPayload.data.signedTransactionInfo
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>val transactionDecodedPayloadStr = JWSObject.parse(transactionPayload).getPayload.toString
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>val transactionDecodedPayload = parser.parse(transactionDecodedPayloadStr).flatMap(_.as[AppleJWSTransactionDecodedPayload]).toTry.get
</pre></div>
</div>
</div>

<p>Here we have the detailed transaction information in <code>transactionDecodedPayload</code> and can hand it with our business logic.</p>

<p>There is an interesting thing: <code>signedRenewInfo</code> and <code>signedTrasactionInfo</code> are both encoded with JWS again in payload data. I don’t know why: since we’ve already verified the whole payload is signed by Apple, all the content in it should already be valid as well, what’s the point to sign the fields again? I just decoded the fields with <code>JWSObject.parse</code> but you can always verify it with the same method above just to be safe.</p>

<h2 id="5-other-thoughts">5. Other Thoughts</h2>

<p>As I said about a <a href="/2020-11-08-DNS-Resolving-Bug-in-iOS-14.html">previous blog about an iOS bug</a>, I really hate Apple’s close ecosystem. But Apple’s hardware is good and has a large user space, so we cannot avoid it. Hopefully Android can be better at permission management and other mobile OS can also catch up.</p>]]></content><author><name></name></author><category term="iOS" /><category term="Apple" /><category term="In App Purchase" /><category term="Scala" /><category term="Java" /><category term="Programming" /><summary type="html"><![CDATA[When you write an app for iOS, publish it to Apple App Store and want to sell something within it, Apple makes it mandatory to use its own in app purchase framework for non consumable items and subscriptions. If the app has a server, it’s very usual that the server wants to know the payment events and have some followup logic with them. But how to do that? There is always an option to let the app send a request to server, but anyone can use the same endpoint to make false claims. To prevent this, Apple has a server to server notification mechanism: Instead the app itself, a server from Apple will send a request to your server to notify the payment events. Since the message is signed by Apple, you can make sure no one else can fake it by verifying the signature.]]></summary></entry><entry><title type="html">A Library to Make It Easier to Use Scala with gRPC</title><link href="https://www.binwang.me/2022-05-02-A-Library-to-Make-It-Easier-to-Use-Scala-with-GRPC.html" rel="alternate" type="text/html" title="A Library to Make It Easier to Use Scala with gRPC" /><published>2022-05-02T00:00:00-04:00</published><updated>2022-05-02T00:00:00-04:00</updated><id>https://www.binwang.me/A-Library-to-Make-It-Easier-to-Use-Scala-with-GRPC</id><content type="html" xml:base="https://www.binwang.me/2022-05-02-A-Library-to-Make-It-Easier-to-Use-Scala-with-GRPC.html"><![CDATA[<p><em>This article describes why I created the library <a href="https://github.com/wb14123/scala2grpc">Scala2grpc</a>.</em></p>

<p><a href="https://grpc.io/">gPRC</a> is a Remote Procedure Call (RPC) framework made by Google. It uses a domain specific language (DSL) to define the APIs, and provides tools for lots of languages to generate code for both servers and clients. The generated code includes models and API interfaces. The developer can create a gRPC server by implementing the generated interfaces. There are lots of examples in the official document so I’ll not spend more time on the details.</p>

<p>It has lots of advantages compared to traditional HTTP APIs that encode payloads as JSON or XML. Just to name a few: it’s type safe so there are less places to make errors; it has a schema so causes less confusing when communicate APIs between developers; it’s more efficient on both serialization and translation. Because of the advantages and the big name behind it, it’s very popular, especially for mobile apps because of the good client support.</p>

<p>However, I feel the framework is very invasive: models are usually the foundations of a program. With gRPC, the models are generated by the framework, as well as the interfaces. This makes the whole program depends on the framework very heavily. Here is an example:</p>

<div class="language-scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>// ExampleService is generated by gRPC
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>class ExampleServiceImpl() extends ExampleService {
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>        // ExampleInput and ExampleOutput are both generated by gRPC.
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>        def exampleAPI(input ExampleInput): ExampleOut = {
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>                ...
<span class="line-numbers"><a href="#n6" name="n6">6</a></span>  }
<span class="line-numbers"><a href="#n7" name="n7">7</a></span>}
<span class="line-numbers"><a href="#n8" name="n8">8</a></span>
</pre></div>
</div>
</div>

<p>It’s more invasive than most of the (non RPC) libraries: for most of other libraries, you can define the interface and use those libraries to fill in the implementations, so when you change a library you don’t need to change other parts of the code.</p>

<p>Maybe sometimes it doesn’t matter too much: say you are Google and this framework is so fundamental in the services that no one is gonna to change it. But if you don’t like it, a way to work around this is to define the business logic at another place, and invoke those native classes and methods in the implementation of the gRPC generated interfaces. The logic in these implementations should be as simple as possible, usually just the invoking of methods and the converting between gRPC models and native models. Here is an example of this approach:</p>

<div class="language-scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>// define natvie classes
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>case class MyInput(...)
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>case class MyOutput(...)
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>class MyService() {
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>        def myAPI(input MyInput): MyOutput = {
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>                ...
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>        }
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>}
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>
<span class="line-numbers"><a href="#n12" name="n12">12</a></span>
<span class="line-numbers"><a href="#n13" name="n13">13</a></span>// implement gRPC interfaces
<span class="line-numbers"><a href="#n14" name="n14">14</a></span>// ExampleService is generated by gRPC
<span class="line-numbers"><a href="#n15" name="n15">15</a></span>class ExampleServiceImpl(val myService: MyService) extends ExampleService {
<span class="line-numbers"><a href="#n16" name="n16">16</a></span>
<span class="line-numbers"><a href="#n17" name="n17">17</a></span>        // ExampleInput and ExampleOutput are both generated by gRPC.
<span class="line-numbers"><a href="#n18" name="n18">18</a></span>        def exampleAPI(input ExampleInput): ExampleOut = {
<span class="line-numbers"><a href="#n19" name="n19">19</a></span>                val myInput = convertFromGRPC(input)
<span class="line-numbers"><strong><a href="#n20" name="n20">20</a></strong></span>                val myOutput = myService.myAPI(myInput)
<span class="line-numbers"><a href="#n21" name="n21">21</a></span>                convertToGRPC(myOutput)
<span class="line-numbers"><a href="#n22" name="n22">22</a></span>  }
<span class="line-numbers"><a href="#n23" name="n23">23</a></span>}
<span class="line-numbers"><a href="#n24" name="n24">24</a></span>
<span class="line-numbers"><a href="#n25" name="n25">25</a></span>// implement convertFromGRPC ...
<span class="line-numbers"><a href="#n26" name="n26">26</a></span>// implement convertToGRPC ...
<span class="line-numbers"><a href="#n27" name="n27">27</a></span>
</pre></div>
</div>
</div>

<p>However, this results lots of repeated works, especially for the converting between gRPC models and native models.</p>

<p>So here is where the sbt plugin Scala2grpc I’ve written comes in: it will generate all the proto files from the native Scala classes, and also generates the classes to convert between native models and gPRC models, and the classes to implement the gPRC interfaces. For example, in the example above, you only needs to write the code for <code>MyInput</code>, <code>MyOutput</code> and <code>MyService</code>, the generation of the proto files are handled by Scala2grpc, as well as <code>ExampleServiceImpl</code>, <code>convertFromGRPC</code> and <code>convertToGRPC</code>.</p>

<p>Sounds interesting? Check out the <a href="https://github.com/wb14123/scala2grpc">Github page</a> to see how to use it. Don’t forget to star it if you find it helpful!</p>]]></content><author><name></name></author><category term="Programming" /><category term="Scala" /><category term="gRPC" /><category term="backend" /><summary type="html"><![CDATA[This article describes why I created the library Scala2grpc.]]></summary></entry><entry><title type="html">A Travel to Montreal</title><link href="https://www.binwang.me/2022-04-24-A-Travel-to-Montreal.html" rel="alternate" type="text/html" title="A Travel to Montreal" /><published>2022-04-24T00:00:00-04:00</published><updated>2022-04-24T00:00:00-04:00</updated><id>https://www.binwang.me/A-Travel-to-Montreal</id><content type="html" xml:base="https://www.binwang.me/2022-04-24-A-Travel-to-Montreal.html"><![CDATA[<p>I have been in Toronto for more than two years. However, because of Covid-19, I’ve never travelled outside of Greater Toronto Area except Niagara Falls since I came here. Until a few weeks ago, with the Covid-19 measurements mostly lifted, my wife and I decided it’s time to take a travel after such a long time. And the long weekend before Easter day is a perfect time.</p>

<p>The destination we chose was Montreal. It’s very closed to Toronto, and is the second largest populated city in Canada. Because of it’s location at St Lawrence river, the early colonizers stopped at Montreal to create frontier port and fort, which makes it one of the earliest cities in Canada. The French colony history of Montreal produces a culture that is different than other English colonies, and is mostly preserved by Quebec Act after the conquer of British. This makes it a great destination for travelling, since the main purpose of travelling is to escape from daily life and feel something different.</p>

<h2 id="airport-on-toronto-islands">Airport on Toronto Islands</h2>

<p>The flight we booked was departed from Billy Bishop airport. This airport is located at Toronto islands – a chain of small islands in Lake Ontario near Downtown Toronto. Considering the close distance between the airport and downtown, jets are banned to limit noise level. As a result, the destinations from this airport are mostly nearby cities in Canada and the US.</p>

<p>Nevertheless the limited flight options, the location of this airport is so convenient that I can walk there in just half an hour from home. I’ve always appreciated the walkable neighborhoods in Downtown Toronto, but I’ve never thought I can just walk to an airport. The airport entrance is located at the lake shore and is connected to the airport with an underground tunnel. Looking back to the water front, the view is just unbeatable.</p>

<p><img src="/static/images/2022-04-24-A-Travel-to-Montreal/from-aiport.jpg" alt="from-airport" /></p>

<p>It’s lovely inside the airport as well. The advantage of a small airport is you don’t need to walk a mile from the entrance to the boarding gate. The whole waiting area is like a cafeteria: The seats are arranged in curved lines, lots of them with tables. It’s clean and quite, really unlike lots of airports I’ve been to.</p>

<h2 id="old-montreal">Old Montreal</h2>

<p>We arrived at Montreal at an early evening. The moment we came out from the subway station, we saw the moon was shinning and the sky was deep blue. Under the sky was a large building with yellow lighting on the top. There was a curved road near the building. Farther away along the road, there seems to be a gigantic European style gate with complex stone decoration – only at the next day we found out it’s actually the facade of the famous Notre-Dame Basilica of Montreal, whose beautiful and mysterious dome has the color just like the sky of that night.</p>

<p><img src="/static/images/2022-04-24-A-Travel-to-Montreal/old-montreal-night.jpg" alt="old-montreal-night" /></p>

<p><img src="/static/images/2022-04-24-A-Travel-to-Montreal/church-dome.jpg" alt="church-dome" /></p>

<p>Montreal has lots of beautiful old buildings, especially near St Lawrence river, in the area where the city was first established. This area is called Old Montreal now. It has lots of bars, restaurants, cafes, parks and even a Ferris wheel. It’s a vibrant area filled with people during the weekend. Thanks to some pedestrian only streets, and narrow roads that helps to limit the speed of cars, it really feels like an old and classic European city. I hope Toronto has more pedestrian only streets like this.</p>

<p><img src="/static/images/2022-04-24-A-Travel-to-Montreal/old-montreal-street.jpg" alt="old-montreal-street" /></p>

<p>In my opinion, another master piece in this area is not one of the old day buildings. Instead, it’s built from the ashes of old buildings. It’s Pointe-à-Callière, or Montreal Museum of Archaeology and History. Its main building is built on top of the remains of Royal Insurance Building. However, it’s actually much larger than that. A large part of the Museum is underground which connects other smaller aboveground buildings. The underground part covers the remains of multiple historical sites, including the Royal Insurance Building, the old custom house, an old sewerage (which is a result of transforming a river to underground), Fort Ville-Marie and so on. The collections of this museum are not the most valuable ones, but combined with this unique site, it presents them in a both beautiful and informative way. The style of exhibit is like an art gallery, and with the help of lights and audio, it shows how history leaves marks in different periods.</p>

<p>What amazed me the most is the ability to find the exact foundation of the city. This is unthinkable in most Chinese cities, and I think for lots of old world cities as well, because of their long history. It’s so valuable for the new world cities to be able to see its whole history, and learn from it.</p>

<h2 id="downtown-montreal">Downtown Montreal</h2>

<p>Old Montreal is beautiful, but I don’t think it would be the first choice if people want to go shopping – it’s mostly souvenir shops over there. On contrast, there are so many shopping malls at Downtown Montreal, especially along Saint-Catherine Street. Across the whole downtown area, this street is filled with restaurants, bars, shops, and of course, people. And again, I think the vibrant is largely thanks to the wide pedestrian street and narrow roads for cars. I saw some places are still under construction which was probably to make the pedestrian street even wider.</p>

<p>The buildings are nicer and taller than those on Toronto commercial streets. Grand entrance with tall windows on the facade is very usual. Indoor space also feels more open, almost like the shopping malls at suburb areas of Toronto. All of these makes it a pleasant to walk and shopping.</p>

<p><img src="/static/images/2022-04-24-A-Travel-to-Montreal/downtown-montreal.jpg" alt="downtown-montreal" /></p>

<p>Montreal is a city speaks French, but also lots of English, more than any other cities in Quebec. This is most obvious in book stores. In an Indigo store I visited, the first floor is filled with French books while the second floor with English books. It’s fun to think about how two different cultures, even both of them are from Europe thus similar from an outsider’s point of view, are mixed and conflicted. Another example of this conflict can be seen from the introduction movie at Pointe-à-Callière: while it spends little time to introduce how French occupied the Island of Montreal from native people (other parts of the Museum did say the Island of Montreal is kind of unoccupied before French came since it’s easier to get attacked because of the geography), it plays scary music during the narrative of British conquer and releases British from a book like a demon. I’m not sure if it’s a joke or not, but you can see something from it.</p>

<p>Speaking of the multicultural of the city, I must mention the Chinatown. It’s located at the eastern part of downtown, with four Paifang – a Chinese style gate – mark the boundaries. St Laurent Street, which is also often called the Main Street, passes right through it. There is another pedestrian only street in Chinatown, which makes this area very vibrant. I even found a restaurants that provides some special food from my hometown. Those food is hard to find even in Beijing, and I’ve never seen it in Toronto. So this is a happy surprise we didn’t expect.</p>

<p><img src="/static/images/2022-04-24-A-Travel-to-Montreal/chinatown.jpg" alt="chinatown" /></p>

<p>Unlike Toronto, the Chinatown in Montreal is marked with clear boundaries, and seems to attract more people (I’ve never been to the other Chinatown at eastern Toronto so not sure about that one). But it seems to be mostly a commercial zone without many residential buildings. And most of Chinese commercials seems to be limited to this area – unlike Toronto where you can find them almost everywhere. The next day I walked to another area at western downtown near Concordia University, where I heard newer Chinese immigration prefers, and found some Chinese restaurants and shops with more modern style. But my overall impression is the Chinese commercials are more centralized than those in Toronto.</p>

<h2 id="mount-royal-montreal">Mount Royal, Montreal</h2>

<p>Montreal is named after Mount Royal, which I didn’t know before, but makes lots of sense to me after I heard it. It’s not a high mountain, but still high enough to see the whole downtown area and beyond that. I’d like to think it’s the backyard of the city – just next to the commercial and residential area, here is a nice park with wonderful views.</p>

<p><img src="/static/images/2022-04-24-A-Travel-to-Montreal/mount-royal.jpg" alt="mount-royal" /></p>

<p>There is another large church called Saint Joseph’s Oratory near Mount Royal. You can see Montreal at another direction from there. The day we visited there was cold, with freezing rain and heavy wind. Sitting inside the church and watching layers of cloud floating, and the land and rivers under it, I couldn’t stop wondering what have happened on this land. When I was in China and stood at a high point like this, I could almost see the past of the land: the dynasties, the legendaries, the glories, the wars and the tragedies. Sometimes I could feel the pain of the land because it bears so many histories. But it’s different here: everything is so claim, nothing much to tell, only the wind, the cloud and maybe some forgotten stories.</p>

<p><img src="/static/images/2022-04-24-A-Travel-to-Montreal/st-joserph-church.jpg" alt="st-joserph-church" /></p>

<h2 id="food-life">Food, Life</h2>

<p>There are some famous food in Montreal. Poutine, bagel and smoked meat are in the top list. And of course we tried all of them. Poutine is fine, but not much better than the Poutine in Toronto. Smoked meat is very unique, almost like a kind of Chinese stewed beef, but with more delicious fat. However, bagels there was a big disappointment. It’s either we didn’t find the right place, or didn’t eat them in right way. We looked up a very popular bagel store, waited in a long line only to found out there is no bagel sandwich – just the bagel itself. So we went to another popular bagel store, waited in a long line again and found out the same. Luckily they also sold salmon and cream cheese, so we bought those and ate them in a nearby cafe. But the bagels are so normal – far worse than the bagels sold by the stores near my home at Toronto. The bagels at Toronto are so delicious that they are among my favourite food. I don’t think I will miss them if they are all like what I ate in Montreal.</p>

<p>During the time we bought bagels, we had an opportunity to walk in one of the residential areas – Mile End community. I was only in Montreal for three days and have no data to support me, but my first impression is its mixed usage is not as good as Toronto. I know North America cities have (in my mind) absurd zoning code and Montreal should already be better than lots of them, but I really like the place that you can just walk to everywhere for daily needs: offices, grocery stores, shopping malls, parks and so on. Downtown Montreal has all of them but I didn’t see lots of residential buildings over there. While at the Mile End community I saw lots of residential houses, even mid rises, but not a lot of other things.</p>

<p>Public transit is good in Montreal. The subway cars are new and clean. But buses can be hit or miss – not sure if it’s because of the holiday weekend. The worst experience happened when we were waiting for the bus to the airport. There was a long line and the buses were not came as scheduled. Lots of people needed to find other ways to the airport at last. According to the lady before us in the line, who was born and raised at Montreal, it’s crazy but apparently not the first time – this situation happened to her at one Christmas day as well. We were almost decided to take a taxi to airport as well – actually we called a Uber but the bus came at the last minute. Even so, we didn’t make the flight we booked but the airline was kind enough to move us to the next flight, so the end of the travel is not so bad.</p>

<p>Overall the travel was a great experience. It is a good start to get the life back to normal after Covid-19, and a good start of my exploration to the cities in Canada.</p>]]></content><author><name></name></author><category term="life" /><category term="Montreal" /><category term="Canada" /><summary type="html"><![CDATA[I have been in Toronto for more than two years. However, because of Covid-19, I’ve never travelled outside of Greater Toronto Area except Niagara Falls since I came here. Until a few weeks ago, with the Covid-19 measurements mostly lifted, my wife and I decided it’s time to take a travel after such a long time. And the long weekend before Easter day is a perfect time.]]></summary></entry><entry xml:lang="zh"><title type="html">十三年前被隔离的经历</title><link href="https://www.binwang.me/2022-04-04-%E5%8D%81%E4%B8%89%E5%B9%B4%E5%89%8D%E8%A2%AB%E9%9A%94%E7%A6%BB%E7%9A%84%E7%BB%8F%E5%8E%86.html" rel="alternate" type="text/html" title="十三年前被隔离的经历" /><published>2022-04-04T00:00:00-04:00</published><updated>2022-04-04T00:00:00-04:00</updated><id>https://www.binwang.me/%E5%8D%81%E4%B8%89%E5%B9%B4%E5%89%8D%E8%A2%AB%E9%9A%94%E7%A6%BB%E7%9A%84%E7%BB%8F%E5%8E%86</id><content type="html" xml:base="https://www.binwang.me/2022-04-04-%E5%8D%81%E4%B8%89%E5%B9%B4%E5%89%8D%E8%A2%AB%E9%9A%94%E7%A6%BB%E7%9A%84%E7%BB%8F%E5%8E%86.html"><![CDATA[<p><em>注：此文由本人首发于 Reddit 。发布时分了<a href="https://www.reddit.com/r/China_irl/comments/tv45el/%E5%8D%81%E4%B8%89%E5%B9%B4%E5%89%8D%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BB%8F%E5%8E%86%E4%B8%80/">上</a>, <a href="https://www.reddit.com/r/China_irl/comments/tvb9wa/%E5%8D%81%E4%B8%89%E5%B9%B4%E5%89%8D%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BB%8F%E5%8E%86%E4%BA%8C/">中</a>, <a href="https://www.reddit.com/r/China_irl/comments/tvvuuq/%E5%8D%81%E4%B8%89%E5%B9%B4%E5%89%8D%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BB%8F%E5%8E%86%E4%B8%89/">下</a>三篇。今转录于此，也按照原本的格式，并无另外的编辑。</em></p>

<h2 id="一">一</h2>

<p>最近疫情形势紧张，不管网上还是线下都有不少讨论。其中社会百态，令人唏嘘。由此突然想到了十三年前甲流时我被隔离的经历。所以就随便写点东西，算是个记录，也算是对年少时光的一点缅怀。</p>

<p>十三年前，也就是二零零九年，是我上大学的第一年。上大学前从未出过远门的我，对于远方有着浪漫的想象。因此报志愿时选了离家最远的大学之一。去了新的城市，一切都很新奇。大学刚开始第一件事就是军训。不知是否所有大学军训都很严，我们当时每天都要早早起床把被子叠成豆腐块，会有人每天抽查。很多同学为了多睡一会干脆叠了以后晚上睡觉就不盖被子，而是把窗帘拆下来当被子盖。给我们军训的教官是本校的国防生，因此也免不了谁谁谁和教官谈恋爱等八卦和狗血。军训虽苦，但是现在留在记忆里的却是有一天晚上大家一起唱歌，皎皎明月挂在空中，初秋的风吹过，心中对未来充满了憧憬。大学军训还给了拿真枪的机会，不过据说前几年出过事故，所以只是让瞄了瞄靶，并未给实弹射击。最后又学军体拳，也算比其他的项目更有意思一些。按往年惯例，军训最后的大高潮应该是所有军训的学生接受检阅，进行军体拳表演，让学校领导过一把军队领导的瘾。</p>

<p>然而这一年却有所不同。军体拳刚练到一半，突然通知所有军训全部停止 — 因为学校发现了几个甲流病例，好像还成为了当地的重灾区。当时对甲流懵懵懂懂，并未觉得有多可怕，反而还乐得不用军训，几个寝室的人聚在一起天天打牌聊天，倒也自在。</p>

<p>大概这样到了十一假期附近，学校突然通知十一假期期间要封校。这给我们带来了不小的振动。学生时代，对于学生来说最宝贵的就是假期。记得高中的时候为了反对假期补课有人给电视台打电话举报，为了反抗清明节不放假集体从教学楼上往下撒纸钱（其实就是撕了的白纸，嗯……，还有课本）。所以当得知十一不能放假的时候，大家都很受打击。一些人立刻制定了计划，决定当天晚上趁政策还没开始严格执行，在夜色掩护下翻墙逃走。我和另外一个寝室的Z君决定结伴而行。当晚来到宿舍楼下的围墙时，发现早已有人用桌椅整整齐齐地搭好了翻墙通道。于是我们轻轻松松翻出围墙。</p>

<p>然而出去的时候已经很晚，公共交通大部分都已停止，更别说我当时计划回家所要坐的火车了。于是两人商量去哪先过一夜，等第二天早起再做打算。我们先去网吧消磨了一段时间，Z 君提议当晚就在网吧度过。然而我对网吧通宵带来的体力消耗深有体会，Z 君家在本市自然好说，而我为了给第二天坐火车留点精力，并没有接受他的提议，而是自己在旁边找了个小旅馆住下。就此别过了Z君，各奔前程。当晚想了些什么已记得不太清楚，只记得当时觉得经历之离奇，很是让人激动。</p>

<p>第二天一大早刚出旅馆时，看到马路对面的学校围墙刚刚翻出来了一个学生，后面有个国防生飞快的追赶，并且口中大喊要举报给校长。但是可能受限于封校的命令，并不能也随之翻墙出来，所以在墙内追了几步以后只得作罢。看到这惊险一幕，我暗自庆幸昨天晚上逃出来真是个正确的决定。于是加快步伐走到公交站，搭车到了火车站，却发现最早的班次也要到晚上，无奈买了当晚的火车票，在火车站等了一天，又做了二十多个小时的硬座回到了家。回到家中的大部分细节都已忘记，然而让我印象最深刻的一个画面是十一当天上午，父亲在包饺子，背景音是电视里在播放的建国60周年阅兵式。我坐在旁边心不在焉的看着坦克一排排在天安门前经过，想着一会要出去哪里玩。</p>

<p>谁知十一假期我虽逃了一时，后面却有真正的隔离在等着我。写到这里发现已经不少，因此就先告一段落。预知后事如何，且听下回分解。</p>

<h2 id="二">二</h2>

<p>上回说了被隔离之前的一些背景，这次讲讲真正被隔离的经历。这么多年过去其实大多数细节都已模糊，但是从之前的经历来看，现在隔离政策的某些弊端在当时已现端倪。不过在不同的政治氛围和不一样的病毒之下，结果还是有很大不同。是是非非，留待大家自己分辨吧。</p>

<p>上回说到了翻墙通道，有评论觉得惊讶。其实那个时候大多数的政策都挺宽松，有很多灰色地带。学校里对新生管理的比较严，而对于大三大四的学生就是散养状态，只要不犯什么大事，学校基本也懒得管。说到围墙，再补充一个细节。当时学校的围墙是铁栏杆，其实在我们宿舍楼下的那段平时是有一个缺口的。围墙外面就是小吃饭店一条街，这个缺口方便学生直接钻出去买饭而不用绕道大门，也方便外面的社会人员绕过学校管控进来送餐。而在封校的时候这个缺口被铁丝封死了，所以才有了桌椅摆成的翻墙通道。等后来解除封校之后，这个缺口又不知被谁打开了。后来学校也多次尝试封闭这个缺口，包括用铁栏杆修复，但是无一例外都很快又被打开。我们都怀疑是外面的小吃商贩所为。不知这个缺口是否现在还在，也不知现在的小吃街是否还繁华如昔。</p>

<p>书归正传。十一假期之后，学校解除封校，学生全部开学。从学生的角度来看，基本都已回归正常。大一的新生也终于开始上课。这时我们才发现高中老师们说的到了大学就轻松了就是扯淡。对于我们专业来说，大一的必修课就把一天的时间排的满满的，很多时候到了晚上还要上课。除了没有考大学的压力，繁忙程度比高中有过之而无不及。我当时刚上大学，对于所有事情还处在新奇和兴奋的阶段，所以对于满满的课程也并无多大怨言，反而还对学习一些新知识乐在其中。</p>

<p>这样过了几天，最多也就半个月吧。有一天在寝室里，我突然感觉有些不适。于是就准备去校医院拿点药。到了校医院，才发现虽然各种管控都已放松，但是对于发热病人的流程还在。于是给了我口罩，让我量一下体温，发现发烧之后直接通知我需要隔离。当时疫情到了后期，学校里已经好几天没什么病例了，所以我也并没有担心得了甲流，也并不知道还有隔离措施，所以有些惊讶。但是想想觉得确实疫情管控需要，既然发烧了就不要乱跑了，于是决定服从安排。当时校医院已经满员，需要送我去别的地方隔离。整个流程已经记得不甚清楚了：只记得量体温好像是在一个教室里面量的，然而等待隔离却是在校医院。也不记得中间是否回寝室拿了什么东西，还是托室友送了些东西过去。现在想想当时隔离好像也并没有什么强制措施，要是真的不想隔离，直接走了也就是了。</p>

<p>等待隔离是在校医院门口，当时只有我一个人。我等待的时候，恰好有个中年男子开着私家车停在校医院门口，和医生说了几句话，医生就让他顺路送我去隔离。上车之后发现他并没有带口罩，也不清楚是否是医院的工作人员，所以还是有些惊讶 — 因为当时看新闻送人去隔离的医生都是防护服全副武装。闲聊了几句就到了隔离的地方，才知道这里是附属于学校的一个专科学院。这个学院虽然挨着学校，但是并没有通道直接连通，所以算是一个相对独立的区域。下车之后发现这个学院整体环境都比较破旧，空间不大，地上有些地方还有杂草。隔离住的楼也是如此：四四方方的建筑上有一个塔楼高高耸起，旁边连着两个低一些的塔楼，是个明显的斯大林式建筑，样式和我们学校的主楼非常相似，只不过小了一号。建筑表面已经斑驳，不知在非隔离期间是做什么用途。进楼之后发现，所有人都没带口罩，这令我更为吃惊，同时也更坚定了目前疫情情况已不严重的信心。我索性也把口罩摘了 — 其实人很多时候都是不是理性的动物，都是随群的乌合之众。</p>

<p>我被分配到了一个房间，有几张病床，但是房间里只有我一个人。房间里灯光煞白。我躺倒床上，用手机和朋友们聊了几句，却并没有通知家里：因为我觉得疫情已不严重，不想让家人做无谓的担心。安定下来之后，我想着终于能休息休息，吃点药恢复一下了，然而却被告知并没有药。不知是心理作用，还是病情本身原因，当晚就觉得身上酸疼不止。等到了挺晚的时候，大概十一二点的样子，护士上来给了一片白色药片。具体是什么药也没说明，只说是退烧药。现在想想应该是阿司匹林或者泰诺一类的药，也算是对症。但是当时觉得真是有点可怜。吃下药之后浑身出汗，睡了一觉后第二天感觉轻松不少，至此才放下心来。</p>

<p>隔离的这个楼里让我印象最为深刻的就是厕所：这个楼的男女厕所是完全连在一起的，而只有女厕所的门可以连到外面。也就是说，要想去男厕所就必须穿过女厕所。男女厕所之间有一扇门，但也并不是所有时候都是关着的。这个厕所代表着我对隔离这段时间的感受：陌生且略感荒谬。</p>

<p>然而在这个楼里没待多久，我们就被转移到了另一个地方。后面的经历，等下篇再继续更新。</p>

<h2 id="三">三</h2>

<p>上篇文章终于说到了隔离的事情，这一篇就把这个系列讲完。其实整个事情并不复杂，没想到东拉西扯写了不少东西。不论有没有人看，倒是自己找到了一些久违的写文字的快乐。也希望能给这个论坛带来一些不太一样的内容。</p>

<p>前文说到我因为发烧被隔离到了一个附属于大学的专科学院，也并没有什么诊断，只是给了一片退烧药。好在吃过药之后第二天有很大好转。本以为就这样住几天就可以顺利出去，没想到中间又给转移到了另一个地方隔离。</p>

<p>通知是在深夜突然下来的，也没说是什么原因，后来有人说可能是隔离的房间不够用了，所以把我们这些已经隔离了几天的低风险人群转移到另一个地方。通知要转移地方后并没有给什么时间准备，正好我也没多少随身物品。整个楼的人集合起来，由工作人员带头，排成一字长蛇阵向新的地点走去。</p>

<p>新的隔离地点是紧邻学校的一个宾馆。这个宾馆属于学校资产，并对外开放，据传闻是本校失足的女大学生经常光顾的地方。出发的时候夜已非常深，所有寝室楼都已统一关门熄灯。我们十几个人在寂静的校园里穿梭，大家都默不做声，只有走路时衣服摩擦发出沙沙的声音。每个人除了看着前面的人，就是看着路灯照耀下的人影婆娑。一行人七拐八绕到了宾馆后并未走正门，而是在一个犄角旮旯里面由带头的工作人员拿出钥匙，开了一个后门。</p>

<p>从后门向内望去，里面漆黑一片。带头的工作人员又拿出了手电筒，带着我们从楼梯上楼。也不知道一共走了几层，只记得在三四层左右是一个巨大的空间，手电筒照到的地方全部都是水泥地面，斜斜歪歪的散落着破损的饭桌和椅子，极像港剧中打斗发生的场景。最终我和另外一个人被分配到了一个标间。标间环境不错，以我当时相当有限的经验来看算是住过最好的酒店房间了。</p>

<p>和我分配到一个房间的同学也是大一新生，来自另一个学院。两人同住一间也未觉不便，毕竟条件比四人或八人同住的寝室已好上太多，而且更乐得有人聊天排解无聊的时间。按理说患难之下的友情应该更为坚固，可是出去后也不过变成了偶尔见面时的点头之交，到了现在我连此君的名字都已忘记。除了居住条件以外，伙食也相当不错。每顿一个盒饭，至少两荤一素，有鱼有肉。可惜我当时还在生病，吃不了太油腻的东西，所以每顿都会剩下不少。</p>

<p>我被隔离的时候恰值社团纳新。之前也说过，刚上大学的我对所有这些新鲜事物都感到好奇，所以计划怎么也要加入一个。因为并不知道都有哪些社团，所以托寝室室友帮我报名了学院的学生会，可是由于种种原因并未成功。不过从以后学生会的风气来看，还真是庆幸没有报名成功。</p>

<p>对于当时的我来说，错过了社团纳新是一个很大的遗憾。不过可能也正因为此，我报名了下半学期才纳新的学院科技协会，并且由于另一个机缘巧合，并没有参加我计划中的 VC++ 小组，而是加入了 ACM 竞赛小组，从此走上了程序竞赛的道路，为后面找工作面试编程省了不少事情。然而也因为竞赛耗费了一些精力，打乱了我每晚自习、跑步、图书馆的三大项目，并间接导致下学期挂科，以至于最后大学辍学。正所谓祸福相倚，谁都说不清楚，不过这都是后话了。</p>

<p>除此之外，被转移到新的地方后一切顺利，没待几天就顺利出来了。出来以后对于缺了几节高数和线性代数课略感担心，不过好在也并无大碍。自此以后也意识到了身体的重要性，大一一学期每晚都坚持去操场跑上几圈，秋去冬来，寒暑相过，也算是留下了不少美好的记忆。</p>]]></content><author><name></name></author><category term="life" /><category term="Covid-19" /><category term="H1N1" /><summary type="html"><![CDATA[注：此文由本人首发于 Reddit 。发布时分了上, 中, 下三篇。今转录于此，也按照原本的格式，并无另外的编辑。]]></summary></entry><entry><title type="html">Add Index to My Blog</title><link href="https://www.binwang.me/2021-10-31-Add-Index-to-My-Blog.html" rel="alternate" type="text/html" title="Add Index to My Blog" /><published>2021-10-31T00:00:00-04:00</published><updated>2021-10-31T00:00:00-04:00</updated><id>https://www.binwang.me/Add-Index-to-My-Blog</id><content type="html" xml:base="https://www.binwang.me/2021-10-31-Add-Index-to-My-Blog.html"><![CDATA[<p>I added an <a href="/index_page.html">index page</a> to my blog yesterday, which can also be accessed from the “Index” entry at the top of every page. When I moved this blog to Jekyll, I <a href="/2012-12-10-Remove-Categories.html">removed categories</a> since I thought tag should be enough. But after a few years, I feel something is missing. One of the reason maybe I never implemented find posts by tags, which makes tags useless. Even if I had that, I feel it’s still not enough. But I don’t want categories , since “category” in Jekyll is normally means a flat level structure. What I want is a nested category structure. It should let me create as many levels as I want, like the index in a book. In this way, I will have a better idea about which field I have covered, and what I should focus on based on my future plan. It can also benefit the readers: they can find the posts they are interested much easier. Not every blog post has the same quality or depth based on the nature of the topic, so in this way the posts under most interesting topics can be grouped together instead of being buried in the timeline.</p>

<p>To implement this feature, I wrote my own little plugin. I only knew some Ruby knowledge from one of Ruby’s author’s books (I read the Chinese version of the book and cannot find the name of the English version, I believe the original book is published in Japanese with the name まつもとゆきひろ コードの世界~スーパー・プログラマになる14の思考法). I never worked on any project with Ruby. But I always wanted to write some plugins for Jekyll so I know I can customize my blog in a better way. Luckily this task is simple enough. The result source code is on my Github of this blog repo. It has a <a href="https://github.com/wb14123/blog/blob/master/jekyll/_plugins/Index.rb">ruby script</a> to generate the index page from post property, with a <a href="https://github.com/wb14123/blog/blob/master/jekyll/index_page.html">template</a> that uses another <a href="https://github.com/wb14123/blog/blob/master/jekyll/_includes/index_page.html">recursive template</a>.</p>

<p>So here is how this plugin works: the only thing I need to do while writing a post is to add a new property called <code>index</code> at the starting of the Markdown file, like this:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>---
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span># these three properties are needed before this feature:
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>layout: post
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>title: Add Index to My Blog
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>tags: [blog, index]
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span># this is the newly added property:
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>index: ['/Projects/Blog']
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>---
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>
</pre></div>
</div>
</div>

<p>Then the plugin will parse <code>index</code> field, break it down into multiple levels based on <code>/</code> in it, and group all the posts in the same field together. The categories are sorted based on alphabet order, while the posts in each category is sorted from old to new. <code>index</code> is an array, so in theory I can add a blog post into multiple categories if I want to, but I try not to do it if it’s not necessary, since it gives a false feeling about how many posts I’ve written. For example, at the beginning, I tried to have both <code>/Computer Science/Database</code> and <code>/Computer Science/Distributed System</code>, and almost every posts under <code>database</code> is also in <code>distributed system</code>. Then I decided it doesn’t make sense: they are more about distributed system because what I care most in those articles is about database transactions. So I remove the category <code>database</code> and put all of them just under <code>distributed system</code>. If I wrote anything like query optimization in the future, I may created another <code>database</code> category, but it’s not needed for now.</p>

<p>I’m very happy with the result. I want it to be simple enough as a static page without the need of Javascript. Javascript can certainly improve some UI like expand/collapse the levels. I may implement both version in the future so people can view it with and without Javascript, but the current UI is good enough for me.</p>

<p>UI aside, I’m much happier about the content. The index has a good structure. It shows the fields I’ve explored. It even shows a correlation between the focus and the timeline, which I never thought about before. For example, the only two posts about algorithm are published when I was in the University, when I joined some programming contest. The posts about Linux are most published at the last year of my college life, when I was in the first few years of using Linx and had an internship at Redhat. It doesn’t mean I’m not interested in Linux anymore. It’s just after so many years of using Linux, it becomes a tool that so fundamental to my work and digital life, and I don’t feel the necessary to actively learn the basic things and tune it. Data processing related topic happened most when I worked in an A/B testing company. And machine learning related topic also happened around that time, when I was most interested in neural networks, which resulted the most popular side project I’ve built. I started to post distributed system related topic after I joined a database company, and still find it’s an interesting topic thus many posts in last year. Which also remains me if I want to learn more about it I should continue to write articles about it in this and following years. But not everything is perfect, mainly because I didn’t write everything I’ve worked on my blog. One of the reason is a lot of things I’ve explored are already somewhere else and I don’t want to just copy something to my blog. The projects I’ve worked on are also not covered. I’m still thinking what’s the best way to put it into the index: either add a link to the project page, or write a blog post about each of the projects. Other than those, there is still something missing. It’s mostly because of lazy, which I should change in the future.</p>

<p>So as a result, the index page achieved some of goal: covering some of my past works and thoughts, and remanding me what I should focus in the future. About the goal of benefiting the readers, you as a reader decide if it’s good or not.</p>]]></content><author><name></name></author><category term="blog" /><category term="index" /><summary type="html"><![CDATA[I added an index page to my blog yesterday, which can also be accessed from the “Index” entry at the top of every page. When I moved this blog to Jekyll, I removed categories since I thought tag should be enough. But after a few years, I feel something is missing. One of the reason maybe I never implemented find posts by tags, which makes tags useless. Even if I had that, I feel it’s still not enough. But I don’t want categories , since “category” in Jekyll is normally means a flat level structure. What I want is a nested category structure. It should let me create as many levels as I want, like the index in a book. In this way, I will have a better idea about which field I have covered, and what I should focus on based on my future plan. It can also benefit the readers: they can find the posts they are interested much easier. Not every blog post has the same quality or depth based on the nature of the topic, so in this way the posts under most interesting topics can be grouped together instead of being buried in the timeline.]]></summary></entry><entry><title type="html">Personal ZFS Offsite Backup Solution</title><link href="https://www.binwang.me/2021-09-19-Personal-ZFS-Offsite-Online-Backup-Solution.html" rel="alternate" type="text/html" title="Personal ZFS Offsite Backup Solution" /><published>2021-09-19T00:00:00-04:00</published><updated>2021-09-19T00:00:00-04:00</updated><id>https://www.binwang.me/Personal-ZFS-Offsite-Online-Backup-Solution</id><content type="html" xml:base="https://www.binwang.me/2021-09-19-Personal-ZFS-Offsite-Online-Backup-Solution.html"><![CDATA[<p>Digital data was never as important as nowadays. Not only for business, but also for every individual. And it’s challenging to make data safe. For example, we see the news about ransomware encrypted all the data of big companies, made their business suspended. We also see a lot of extreme weathers recently: fires, flood and so on, which threats the hardware that stores the data. So it’s necessary to have backups. There should be at least two kinds of backups to keep data safe: offline backup and offsite backup.</p>

<p>Offline backup means once the data is backed up, it will be isolated from the system. So it can prevent bugs and security risks of the system. How isolated the backup should be depends on the use case and threaten model. For personal usage, a removable disk that is disconnected from any machine should be good enough. It’s easy to implement, so we will not focus on it in this article.</p>

<p>Offsite backup means the backup data is in a different location. So that in the case of nature disaster, fire, flood, hardware stolen and so on, you can still recover it from a different location. Again, depends on the threaten model, this different location can be a different location in the same city, in a different city, or even in a different country or land. (By the way, I’m always confused when the characters in movies can just destroy a data center, or even a single machine in order to destroy some big plan or very important data. If the people dealing with such important data know nothing about backup, we are in bigger troubles.)</p>

<p>Offline and offsite backup are not mutually exclusive. Some kinds of backup can be both offline and offsite. For example, you can burn all the data onto CDs and mail them away every week. The CDs are both offline and offsite backup. But because of the cost and efficiency, the offsite backup is usually online so it can make backups very frequently and quickly.</p>

<p>In this article, I’ll introduce how to setup a machine used for personal offsite backup. ZFS is needed since it enables incremental encrypted backup. As we can see later, it makes our setup much easier.</p>

<h2 id="1-goals">1. Goals</h2>

<p>Let’s consider what’s the requirements of this setup solution. Data security is obviously the No. 1 requirement: nobody other than ourselves should be able to obtain the data.</p>

<p>User friendly is another big factor. Most likely we will put this backup machine to a family or friend’s place. So we want it to have very low setup and maintenance effort. Actually, with my setup bellow, the only thing needed for this machine to operate is to plugin the power and Ethernet cable, then press the power button. No monitor, keyboard, mouse, or NAT port forwarding is needed.</p>

<h2 id="2-threaten-mode">2. Threaten Mode</h2>

<p>Since data security is our No. 1 goal, let’s analysis the threaten mode in the backup system. First, think about what is trusted and what is untrusted. I divide it into three levels: fully trusted, partially trusted and untrusted.</p>

<p>Fully trusted: the environment is theoretically fully controlled by ourselves and no one else can see the data in it. I know it’s hard to be 100% safe for an environment, but how to achieve that is totally another topic. Let’s assume in this setup, the machine that holds the source data can be seen as fully trusted.</p>

<p>Partially trusted: we can trust the environment on some level but it’s still possible to be unsafe. For example, for the machine located at a different location, which can be physically accessed by other people, it’s not fully trusted. Though we can usually have high trust level for the people that manages the place, which is usually our family or friend, it’s still possible that the data on the machine can be obtained.</p>

<p>Not trusted: the parts that cannot be trusted at all. Anyone with some security background can obtain data from this part. For example, the public network is not trusted.</p>

<p>Here is a diagram about the trust levels in our system: fully trusted part is in green, partially trusted is in yellow and not trusted is in red.</p>

<p><img src="/static/images/2021-09-19-Personal-ZFS-Offsite-Online-Backup-Solution/2021-09-19-zfs-backup-trust-level.png" alt="trust-level" style="max-width: 400px;" /></p>

<p>Then let’s also analysis the parts of the system we want to keep secure. In the diagram bellow, the red part means we want 100% security. Yellow part means we want it as secure as possible, but it’s also okay if it’s not 100% secure.</p>

<p><img src="/static/images/2021-09-19-Personal-ZFS-Offsite-Online-Backup-Solution/2021-09-19-zfs-backup-security-level.png" alt="security-level" style="max-width: 400px;" /></p>

<p>So with these two graphs combined, we identified some parts we need to consider:</p>

<ul>
  <li>Network and backup data. These parts are either not trusted or partially trusted, but we want fully security for the data goes through there. So we need fully encryption on these components. Luckily it’s easy to do with ZFS: ZFS can send encrypted snapshots and receive it on another machine without ever decrypt it.</li>
  <li>Backup machine. It’s under partially trusted environment. We shouldn’t put any sensitive data into its system. If it’s compromised, the backup data can still be safe: no one can see the content of the backup data, but the backup process may no longer work as expected.</li>
</ul>

<p>With this in mind, let’s start to setup our backup machine.</p>

<h2 id="3-backup-machine-setup">3. Backup Machine Setup</h2>

<h3 id="31-choose-the-hardware">3.1 Choose the Hardware</h3>

<p>Since we need to have ZFS on this machine, we’d better to choose a x86 machine instead of an ARM one. I know ZFS has ARM builds but I’ve never tried that, and don’t know how good it is. Data safe is very important so I’d rather to use a tested solution.</p>

<p>The machine doesn’t need to be too powerful but larger than 4G memory is recommended because of ZFS. It should be able to handle two disks since it makes data recovery easier as we can see later. Considering we will place this machine to someone else’s home, it’s better to have a small form factor.</p>

<p>It’s better to have TPM as well. Otherwise we cannot setup disk encryption (other than our backup data) in a user friendly way, which means anyone with physical access to the machine can get access of system data (which is also okay since we don’t have sensitive data on system). Old machines usually has TPM 1.2 chip, which is not as secure as TPM 2.0. But it’s good enough considering our use case and the cost.</p>

<p>There are a lot of cheap choices for used machines like this. Like ThinkCentre tiny desktops, or HP EliteDesktop. The Youtube channel <a href="https://www.youtube.com/c/ETAPRIME">ETA Prime</a> has a lot of reviews for such machines. A potential risk of buying used hardware is it may contains security backdoor, but it’s very rare if you are not targeted and you buy it from sellers with good reputation. Even it happens, as we’ve analysed above, the backup machine is not treated as fully trusted environment, so it’s fine.</p>

<h3 id="32-secure-bios">3.2 Secure BIOS</h3>

<p>The first thing we need to do is securing the BIOS, so no one else can change any configurations. If we don’t protect it, anything we do in the later steps is meaningless.</p>

<p>How to secure the BIOS depends on the manufacturer. Most likely there will be a “security” section in the BIOS, which has “password protection” option. So that you can protect the BIOS with a password.</p>

<p>Keep in mind most of the machines can reset BIOS with some hardware button or wire, thus reset the password as well. But it will also reset the TPM, which means it doesn’t expose the encryption key of our system disk, which we will setup in the next step.</p>

<h3 id="33-setup-system-disk-encryption-with-tpm">3.3 Setup System Disk Encryption with TPM</h3>

<p>I choose Arch Linux as the operating system for the backup machine. I know it’s controversial as Arch Linux is not considered as the most stable Linux distribution. But this is a personal setup and I’m using Arch Linux for all my own machines. I’m very familiar with it. It’s very stable in my experience, and I can keep the ZFS versions the same. So I’ll mainly focus on the setup for Arch Linux from here, but any main stream Linux distribution should be able to do these setups with minimal adjustment.</p>

<p>The first thing we want to do is to have disk encryption for the system. (We use another disk for ZFS and it’s already encrypted, so we don’t need worry about that one.) While installing the OS, you can choose encrypt the disks and use either a password or a key file stored at another place like a USB drive. In this way, you can input the password or plugin the key file during boot. But it needs human interactive, and we don’t want other people to have the password or key in order to boot the machine. So we will setup another key that stores in TPM and can be decrypted automatically during start up.</p>

<p>It’s very easy to setup disk encryption with TPM 2.0. Systemd has very good support for that. The steps are in the <a href="https://wiki.archlinux.org/title/Trusted_Platform_Module#Using_TPM_2.0">Arch wiki</a>. I’ve never tried this since my backup machine only has TPM 1.2 chip, but the steps looks very straightforward.</p>

<p>With TPM 1.2, it’s much harder and took me a lot of time to figure that out. At last I found a <a href="https://github.com/danielfomin96/arch-linux-luks-tpm-boot">repo on Github</a> and forked it with some modifications in order to support busybox initramfs, which is the default setup for Arch Linux. Here is <a href="https://github.com/wb14123/arch-linux-luks-tpm-boot">my updated repo</a>. It’s not easy but I believe the security it adds worth the effort.</p>

<h3 id="34-setup-secure-boot">3.4 Setup Secure Boot</h3>

<p>The disk encryption above doesn’t include boot loader. In order to make the boot loader safe, we can setup secure boot, which will prevent the system from booting if the boot loader is not signed by trusted party. There is an <a href="https://wiki.archlinux.org/title/Unified_Extensible_Firmware_Interface/Secure_Boot#Using_your_own_keys">Arch wiki</a> described the steps as well. I used the option of “Using your own keys” and use <a href="https://wiki.archlinux.org/title/Unified_Extensible_Firmware_Interface/Secure_Boot#Fully_automated_unified_kernel_generation_and_signing_with_sbupdate">sbupdate</a> to automatic sign new kernels after update. Make sure to add kernel parameters, especially the parameters related to disk encryption into <code>CMDLINE_DEAFULT</code> in <code>/etc/sbupdate.conf</code> as mentioned in the <a href="https://github.com/andreyv/sbupdate">sbupdate document</a>.</p>

<h3 id="35-setup-vpn-and-ssh">3.5 Setup VPN and SSH</h3>

<p>In order to make ZFS remote backup work, we need to establish a SSH connection between the source machine which holds the data we want to backup, and the backup machine. Obviously it’s not safe to let the source machine act as the SSH server, since the backup machine is not fully trusted. But we cannot make the backup machine as a SSH server on public network either, since it needs port forwarding setup if it’s placed in a home network, which is very user unfriendly. So we need a VPN network to let the backup machine connect as a client, then it can act as a SSH server in the VPN network.</p>

<p>There are many VPN solutions out there. OpenVPN is the most popular one with lots of features. But WireGuard is newer, more efficient and easier to setup. So I choose WireGuard as my VPN solution. I setup the VPN server on my source machine and use NAT port forwarding to make it accessible from public network, so the backup machine is able to connect as a client. The setup guide can also be found in an <a href="https://wiki.archlinux.org/title/WireGuard#Specific_use-case:_VPN_server">Arch Wiki</a>, with some additional notes:</p>

<ul>
  <li>Use DNS name for the source machine address, so that in the case of IP changes, the backup machine can still connect to the VPN automatically.</li>
  <li>Add <code>PersistentKeepalive = 1</code> in the client configuration, so that the client will send a package every 1 second to keep the connection alive.</li>
  <li>Enable the VPN and SSH service so that the backup machine will be able to connect after reboot.</li>
  <li>Setup the firewall properly, since the backup machine is not trusted.</li>
</ul>

<h3 id="36-install-zfs">3.6 Install ZFS</h3>

<p>Finally we can install ZFS on the backup machine so it can receive ZFS datasets. Since we don’t need ZFS on the root file system, it’s very easy to do. Just follow the <a href="https://wiki.archlinux.org/title/ZFS">Arch Wiki for ZFS</a>. I also wrote a <a href="/2020-01-28-Migrate-Arch-Linux-to-Zfs.html">blog post</a> about it but I don’t think it’s needed for this simple setup.</p>

<h3 id="37-conclusion">3.7 Conclusion</h3>

<p>After all the setup, let’s look at what we have achieved for this backup machine.</p>

<p>From the security point of view, we make it reasonable secure: the disk is encrypted and the encryption key is stored in TPM safely. The boot loader is also prevented from modification because of secure boot. It’s not 100% secure though, for example, when the machine is powered on there are ways to freeze the memory and read the disk encryption key from it; TPM 1.2 has security risks as well. But even though there are some security risks, it’s very hard to actually compromise the system for average people. And as I’ve mentioned multiple times above, the security risks doesn’t effect the security of our backup data since the transfer and storage are all encrypted without ever exposed the encryption key.</p>

<p>From user friendly point of view, as I mentioned at the starting, the machine can be put anywhere that has a good network connection. And with just power and Ethernet cable plugged in, it would be ready to operate.</p>

<h2 id="4-zfs-send-and-recv">4. ZFS Send and Recv</h2>

<p>After the backup machine setup, we can finally backup the ZFS datasets to it. It’s very easy for ZFS to do remote backup, just make sure to use the flag <code>-w</code> when send so it will send the encrypted ZFS dataset (given the source dataset is already encrypted). For example:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>zfs send -R -v -w zroot/root@snapshot-name | ssh &lt;backup-machine-ip-in-vpn&gt; sudo zfs recv -Fu zoffsite-backup/root
</pre></div>
</div>
</div>

<p>It’s also easy to send incremental back using command like this:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>zfs send -R -v -w -i zroot/root@from-snapshot zroot/root@to-snapshot | ssh &lt;backup-machine-ip-in-vpn&gt; sudo zfs recv -Fu zoffsite-backup/root
</pre></div>
</div>
</div>

<p>In my case, I don’t want to backup all the datasets since I can download some of them like Steam games from somewhere else. So I wrote a script to get all the dataset I want to backup and send them one by one without suing <code>-R</code> flag. Anyway, with the power of ZFS you can basically backup in whatever way you want. You can put the script in cronjob or systemd timer to run it everyday for instance.</p>

<p>At last, <strong>there is an important note about data recovery</strong>. If you ever need to recover the data, make sure don’t mount it in the backup system directly since the backup system is not trusted. Instead, get the physical backup disk, mount it on a trusted system, and recover from there.</p>]]></content><author><name></name></author><category term="zfs" /><category term="backup" /><category term="secure boot" /><category term="linux" /><category term="tech" /><category term="vpn" /><summary type="html"><![CDATA[Digital data was never as important as nowadays. Not only for business, but also for every individual. And it’s challenging to make data safe. For example, we see the news about ransomware encrypted all the data of big companies, made their business suspended. We also see a lot of extreme weathers recently: fires, flood and so on, which threats the hardware that stores the data. So it’s necessary to have backups. There should be at least two kinds of backups to keep data safe: offline backup and offsite backup.]]></summary></entry><entry><title type="html">Why Big Companies Need to Adopt Open Source</title><link href="https://www.binwang.me/2021-06-30-Why-Big-Companies-Need-to-Adopt-Open-Source.html" rel="alternate" type="text/html" title="Why Big Companies Need to Adopt Open Source" /><published>2021-06-30T00:00:00-04:00</published><updated>2021-06-30T00:00:00-04:00</updated><id>https://www.binwang.me/Why-Big-Companies-Need-to-Adopt-Open-Source</id><content type="html" xml:base="https://www.binwang.me/2021-06-30-Why-Big-Companies-Need-to-Adopt-Open-Source.html"><![CDATA[<p>Recently I’m really frustrated by some internal tools in the company. Lots of people think the internal tools in big company should be much better than open source ones. Maybe that’s true in the past or in some other companies. But that’s not true in my experience. While we are seeing more and more companies adopting open source, I think it would be helpful to list some of its advantages. I know there are already lots of articles and people talking about this, but while I’m not in the position to make open source decisions for the internal tools I use, writing this article is at least what I can do.</p>

<p>Basically open source brings better developers and more users. All the benefits come from these. We will talk about these first and then the benefits they make.</p>

<h2 id="attract-more-and-better-software-engineers">Attract More and Better Software Engineers</h2>

<p>An open source project is a perfect tool to show the engineers’ skills and experience. It’s human nature that wants to be acknowledged for something they’ve done. This is especially true for the ones who love and are good at programming. Open source project provides this excellent opportunity. Other than this psychic satisfaction, it also provides real world benefits: the ability to show the skills and experience enables better career opportunities. So it’s no wonder given the same circumstances, open source projects can attract better software engineers.</p>

<p>Even more, if an open source project is done right, it not only attracts people apply for the job to work on the project, it may also attract people or even other companies to work on this project for free.</p>

<h2 id="make-participants-to-be-better-software-engineers">Make Participants to be Better Software Engineers</h2>

<p>Open source project not only attract better software engineers, it can also make the participants to be better software engineers. Since open source shows the code and commit history to everyone, code quality is associated to programmers’ reputation. The programmers have more motivations to write better code, instead of just make things work for now, which happens a lot while the project is under a tight timeline.</p>

<h2 id="attract-more-users">Attract More Users</h2>

<p>Open source provides more features than closed source software: the user can see and modify the source if they want. Most open source licenses also allow the users to re-distribute the software with certain requirements. This is a big deal for lots of the users. Because it makes the worst case better: when the software is not maintained or going to the wrong direction, the user can continue to find some way to support it since they have the source code. It also makes the system more secure because they can better audit the source code to find security risks instead of treating it as a black box.</p>

<p>However, having more users can be a double edge sword sometimes. For some tools and infrastructure software, the company doesn’t get direct benefits from having more users. In contrast, the users can be competitors, and they can get benefits by not spending their own resource to write the software. But the company can actually get indirect benefits, which can overweight the short term hurts at most of the time. We will discuss this in details later.</p>

<h2 id="better-software-quality">Better Software Quality</h2>

<p>I think this is the most important reason to adopt open source software. We always want better software if the cost is reasonable. Some people associate open source project with bad quality, but that’s obviously not true for widely adopted ones. For example, the most famous open source project Linux is the core infrastructure of modern Internet. Not to mention so many open source language compilers, interpreters and build tools. These tools needs complex skills to be developed, and most of them is well written and maintained.</p>

<p>The reason open source software can have better quality is because of the advantages listed above: obviously better software engineer will write better quality software. And with more users, more bugs can be found. And they may also able to provide some insightful ideas about how to improve the software from the user’s point of view. This doesn’t add a burden to the company: whether user reports the bug or not, the bug is there. If it’s important, better to fix it before it really break things. And for the feature requests, the company can decide how to implement them based on the priority. A user request doesn’t mean the company should always accept the request and spend time to implement it.</p>

<p>The software quality doesn’t only include the software itself, it also include the document and ecosystem. While there are more users, there will be more supports from the community and more software will be developed to work with it.</p>

<h2 id="less-time-needed-for-onboarding-new-hires">Less Time Needed for Onboarding New Hires</h2>

<p>This is an indirect benefits from having more users. If the software is popular, lots of new hires already know how to use it before they come to the company. This can save lots of time depends on the complexity of the software. If we take account into the software quality and ecosystem improvement discussed above, the time saved can be much more.</p>

<h2 id="become-the-industry-standard">Become the Industry Standard</h2>

<p>This is another big benefits from having more users. Become the industry standard can combine the software with the companies’ strategy and get lots of reputation for the company. One example is Kubernetes. Google open sourced this technology to make it a final winner in the container orchestration area. Even its competitors like AWS need to support it in their product. I don’t think Google can get this kind of success without make it open source.</p>

<h2 id="more-cost-efficient">More Cost Efficient</h2>

<p>As I said above, if an open source project is done right, it can attract lots of software engineers even other companies to contribute. This is basically free resource.</p>

<h2 id="feedback-to-the-community">Feedback to the Community</h2>

<p>While I listed all the real benefits above, sometimes we may need to think in a more idealism way. Open source started with the idea of openness, to make the code and technology available to everyone. While companies’ main focus is making money, why not have some effort to really make the world a better place on the way?</p>]]></content><author><name></name></author><category term="thoughts" /><category term="tech" /><category term="open source" /><summary type="html"><![CDATA[Recently I’m really frustrated by some internal tools in the company. Lots of people think the internal tools in big company should be much better than open source ones. Maybe that’s true in the past or in some other companies. But that’s not true in my experience. While we are seeing more and more companies adopting open source, I think it would be helpful to list some of its advantages. I know there are already lots of articles and people talking about this, but while I’m not in the position to make open source decisions for the internal tools I use, writing this article is at least what I can do.]]></summary></entry><entry><title type="html">In Defence of Disabling Swap</title><link href="https://www.binwang.me/2021-03-08-In-Defence-of-Disabling-Swap.html" rel="alternate" type="text/html" title="In Defence of Disabling Swap" /><published>2021-03-08T00:00:00-05:00</published><updated>2021-03-08T00:00:00-05:00</updated><id>https://www.binwang.me/In-Defence-of-Disabling-Swap</id><content type="html" xml:base="https://www.binwang.me/2021-03-08-In-Defence-of-Disabling-Swap.html"><![CDATA[<p>Recently, I read an article <a href="https://chrisdown.name/2018/01/02/in-defence-of-swap.html"><em>In Defence of Swap</em></a> that talks about memory swap in Linux. I happened to see some problems at work related to it as well. So in this article, I’ll talk about swap and why we want to disable it.</p>

<p>In the article above, the author recommends to use swap because it makes the memory reclamation more efficient. Which means the OS can swap out the memory that allocated by the program, so that more physical memory can be used for file page to improve cache hit. The author thinks a lot of people don’t like swap because they don’t understand how swap works. But I think it’s quite the opposite: most people with experience understand that, and that’s exactly one of the reasons that they want to disable swap.</p>

<p>The first and biggest reason of disabling swap is transparency. Programmers have some expectations about data access latency while writing the program: if they access it from memory, it will be faster; if access from file, it will be slower. When the program put something into memory, it maybe rarely used. But when it needs to be used, it is expected to be accessed fast. When the program access data from file with swap disabled, it maybe slower because there are less memory for file cache, but it’s okay because it’s expected. So yes, swap can sometimes make memory usage more efficient, but that’s in exchange of the stability. It’s probably okay for desktop users, but not for servers. If you really need efficiency, you need to manage cache by yourself. For example, if some data is really barely used and the latency doesn’t matter, the program can optimize it by putting the data into file or database instead of hand over the control to OS, because the OS doesn’t really know which part of memory is important – less often used memory doesn’t mean less important memory. It’s much harder to optimize the program while the memory management is complex and a black box. Another advantage to manage the memory by itself is, we can add metrics about cache misses and so on, so when there is a performance problem it’s easier to trace down what’s happening.</p>

<p>The second reason is OOM killer. This reason applies when the system is running in high load. Swap get its bad reputation mostly because of the very slow response when memory is not enough, and it’s fair in my opinion. Even for a desktop, it’s better to kill the program instead of letting the whole system to be slow. For an online service, you may think it’s better to handle requests slow rather than completely down. It’s true if your service only has one node, but it’s barely the case for web services. It’s better to let the node down, so you know something is wrong. If there is memory leak, kill the service and restart it can normally solve the problem and you can resolve the root cause later. If there are more requests than the service can handle, it’s better to detect that and scale up or limit requests accordingly. Both cases are better than running the node in an unknown state.</p>

<p>So in conclusion, whether a feature is good depends on real world use cases rather than how the designer imagine it will be used. There is a reason when disabling a feature becomes normal practice for some use cases. Maybe it’s out of topic, but as developers, we must always keep the real world use cases in mind instead of resolving hypothetical problems.</p>]]></content><author><name></name></author><category term="Linux" /><category term="swap" /><category term="memory" /><category term="technology" /><summary type="html"><![CDATA[Recently, I read an article In Defence of Swap that talks about memory swap in Linux. I happened to see some problems at work related to it as well. So in this article, I’ll talk about swap and why we want to disable it.]]></summary></entry><entry><title type="html">Define Infrastructure as Code</title><link href="https://www.binwang.me/2021-02-21-Define-Infrastructure-as-Code.html" rel="alternate" type="text/html" title="Define Infrastructure as Code" /><published>2021-02-21T00:00:00-05:00</published><updated>2021-02-21T00:00:00-05:00</updated><id>https://www.binwang.me/Define-Infrastructure-as-Code</id><content type="html" xml:base="https://www.binwang.me/2021-02-21-Define-Infrastructure-as-Code.html"><![CDATA[<p>I’m using a lot of <a href="https://aws.amazon.com/cdk/">CDK</a> at work recently. So in this article, I want to talk about defining infrastructure as code.</p>

<h2 id="why-define-infrastructure-as-code">Why Define Infrastructure as Code?</h2>

<p>Why do we want to define infrastructure as code? The most important reason is we can make the progress automatic. After all, that’s the purpose of programming. Normally, there are many manual steps to setup the infrastructure, like buying the machine, installing OS, setting up the network and so on. Luckily, with the cloud providers to provide the hardware, and with technologies like Docker and Kubernetes, it’s possible to make more and more steps automatic.</p>

<p>But it’s not always easier to automatically do things than manually do it. Another useful feature of code is it can be the source of truth and make the process reproducible. It’s like the document, but more precise and complete. If you have the code, you can build all the systems from scratch without rely on other people’s undocumented knowledge. It can make sure what you have is a clean system, without random things that people did and forgot. And since it’s reproducible, you can improve the process and know whether it’s better or worse.</p>

<p>The third advantage is you can use version control to manage the infrastructures. You can rollback to a good state. You can also see what has been changed from beginning, which is like an even better document.</p>

<h2 id="define-operating-system">Define Operating System</h2>

<p>For a software, the most related infrastructure is the operating system and the dependencies it needs on the operating system. Traditionally, while using physical machine, the operating system can be automatically installed by PXE through network booting. But it’s hard to setup and test: you need a physical machine to do that.</p>

<p>After virtualization and cloud is popular, it’s easier to build operating system images and test them. Since it’s virtual machine, you can test it at your own development desktop and things will be the same while it’s running on production. There are some tools like <a href="https://www.vagrantup.com/">Vagrant</a> to do it. But it also has it’s own disadvantage: it’s hard to track what you really did in the image. It’s true there are some tools to build the image with code, but I don’t find it to be a normal practise.</p>

<p>Then there comes Docker and it makes defining operation system as code much easier. There are multiple reasons: it’s command line by default so it’s easier to write script. And it’s normal practise to build image with Dockerfile instead of manually doing things in the image and save it. It’s even easier to test it since it’s more lightweight. It also has version number in it’s image format, which makes people have version control in mind.</p>

<h2 id="define-hardware-resource-and-cluster-structure">Define Hardware Resource and Cluster Structure</h2>

<p>On some level, Docker can define the hardware resource, like how much CPU and memory to use, file volumes and local network. But it can only define these at local machine level. When you want to run the service on a cluster and talk to other services, you need more powerful tool. There are many container orchestration tools but after these years it seems Kubernetes has become the standard. With Kubernetes, the things beyond operating system, like network structure, service structure and so on, can also be defined within yaml files.</p>

<p>The feature is not only available in container world. Even before containers are popular, lots of cloud providers have the ability to use code define the resources. It can not only define the hardware resource and network structure, but also for basically every service the cloud provides, like hosted database, logging management and so on.</p>

<h2 id="use-code-instead-of-configuration-files">Use Code Instead of Configuration Files</h2>

<p>For tools like Kubernetes which can use yaml configuration files to define infrastructures, it’s trivial to use code generate the configuration files. Why we want to use code instead of using configuration files directly? Because sometimes there are something can be reused, or some logic depends on different situations.</p>

<p>Of course most tools include Kubernetes also provide API that you can call with code. But I think the better way is to generate configuration file about what you want, and let the tool to do all the actual work. It’s like declarative programming and functional programming. Infrastructure creation is very complex and error-phone: the things can go wrong include how to safely change from one state to another one, failure handling, resource cleanup, rollback, rolling upgrade and so on. It’s better to just describe what’s the desired state and let the tools to do the actual work. It also makes it easier to optimize the workflow to make it faster.</p>

<p>CDK is such a tool that supports lots of programming languages. After run the code it can generate AWS Cloud Formation and run it. Unfortunately it’s from AWS which means you are basically locked into AWS if you choose to use it. Though there is a project <a href="https://cdk8s.io/">cdk8s</a> tries to support Kubernetes for CDK, I’m not sure if it gets the first level support. I really hope there would be an open source project like CDK that supports major cloud solutions.</p>]]></content><author><name></name></author><category term="cloud" /><category term="docker" /><category term="aws" /><category term="cdk" /><category term="technology" /><category term="programming" /><summary type="html"><![CDATA[I’m using a lot of CDK at work recently. So in this article, I want to talk about defining infrastructure as code.]]></summary></entry><entry><title type="html">Setup SSH Authentication with YubiKey</title><link href="https://www.binwang.me/2021-02-12-Setup-SSH-Authentication-with-YubiKey.html" rel="alternate" type="text/html" title="Setup SSH Authentication with YubiKey" /><published>2021-02-12T00:00:00-05:00</published><updated>2021-02-12T00:00:00-05:00</updated><id>https://www.binwang.me/Setup-SSH-Authentication-with-YubiKey</id><content type="html" xml:base="https://www.binwang.me/2021-02-12-Setup-SSH-Authentication-with-YubiKey.html"><![CDATA[<p><a href="https://www.yubico.com/">YubiKey</a> is a kind of hardware security token. The idea is to authenticate a person not only based on something he knows (password), but also on something he owns. It can be a digital file, but a more secure option would be a hardware token like Yubikey since no one can steal it without physical access. I use it for a lot of services. Not surprisingly, it can also be used in ssh authentication. But the official Yubikey tutorials are not very straightforward and the <del>Archlinux wiki pages are more generic instead of Yubikey specific</del>. So in this article, I’ll introduce how to setup ssh to include Yubikey in the authentication process. The operating system I’m using is Arch Linux, but the process for other Linux systems should be very similar.</p>

<h2 id="generate-openssh-hardware-token">Generate OpenSSH Hardware Token</h2>

<p>The most easy way is to generate a ssh key file based on Yubikey. OpenSSH supports this since 8.2.</p>

<ol>
  <li>Run <code>ssh-keygen -t ecdsa-sk</code></li>
  <li>Touch the Yubikey for a few seconds.</li>
</ol>

<p>Then you can use the generated ssh key like other key files with <code>-i</code> option. After type in the login command, you need to touch Yubikey for a few seconds, then you should be able to login.</p>

<h2 id="use-pam"><del>Use PAM</del></h2>

<p><strong>Update: this way only works while the key is plugged into the ssh host, which makes it useless for SSH. However, it’s still useful for things like local login.</strong></p>

<p>A more generic way is to use <a href="https://en.wikipedia.org/wiki/Linux_PAM">PAM</a> with Yubikey. It’s a modular authentication mechanism not only for SSH, but also for lots of other things like local login.</p>

<h3 id="1-install-packages">1. Install packages</h3>

<p>PAM should be installed by default for Archlinux. So the only package we need to install is the PAM module for Yubikey <code>pam-u2f</code>:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>sudo pacman -S pam-u2f
</pre></div>
</div>
</div>

<h3 id="2-generate-u2f-mapping-file">2. Generate u2f mapping file</h3>

<p>Run this command first:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>pamu2fcfg -u&lt;username&gt; # Replace &lt;username&gt; by your username
</pre></div>
</div>
</div>

<p>Touch your Yubikey for a few seconds and save the command result to a configuration file, for example, <code>/etc/u2f_mappings</code>.</p>

<h3 id="3-config-pam-for-ssh">3. Config PAM for SSH</h3>

<p>The PAM config file for ssh is located at <code>/etc/pam.d/sshd</code>. In order to add Yubikey as part of the authentication, add this line to the file:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>auth required pam_u2f.so authfile=/etc/u2f_mappings
</pre></div>
</div>
</div>

<p><code>required</code> means Yubikey authentication is necessary. The other options are <code>requisite</code>, <code>sufficient</code> and <code>optional</code>. Refer to <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system-level_authentication_guide/pam_configuration_files">Redhat document</a> for more details.</p>

<p>The parts after <code>pam_u2f.so</code> are the parameters. <code>authfile</code> is one of them. For all the supported parameters, refer to <a href="https://developers.yubico.com/pam-u2f/">Yubico pam-u2f document</a>.</p>

<h3 id="4-config-ssh-to-include-password-authentication">4. Config SSH to include password authentication</h3>

<p>In order to actually use PAM in ssh, ssh server needs to include password as part of authorization methods. The configuration is <code>AuthenticationMethods</code> in <code>sshd_config</code>. For example, if you want to use password + Yubikey + ssh key file, you can config it like this:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>AuthenticationMethods &quot;publickey,password&quot;
</pre></div>
</div>
</div>

<p>And make sure <code>PasswordAuthentication</code> and <code>ChallengeResponseAuthentication</code> are both <code>yes</code>:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>PasswordAuthentication Yes
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>ChallengeResponseAuthentication Yes
</pre></div>
</div>
</div>

<p>After this, restart sshd then you can login with Yubikey authentication: type in ssh login command, input user password and press enter, touch the Yubikey for a few seconds, then you should be able to login!</p>]]></content><author><name></name></author><category term="linux" /><category term="ssh" /><category term="security" /><category term="yubikey" /><category term="yubico" /><category term="pam" /><category term="pam-u2f" /><summary type="html"><![CDATA[YubiKey is a kind of hardware security token. The idea is to authenticate a person not only based on something he knows (password), but also on something he owns. It can be a digital file, but a more secure option would be a hardware token like Yubikey since no one can steal it without physical access. I use it for a lot of services. Not surprisingly, it can also be used in ssh authentication. But the official Yubikey tutorials are not very straightforward and the Archlinux wiki pages are more generic instead of Yubikey specific. So in this article, I’ll introduce how to setup ssh to include Yubikey in the authentication process. The operating system I’m using is Arch Linux, but the process for other Linux systems should be very similar.]]></summary></entry><entry><title type="html">My 2020 in Review</title><link href="https://www.binwang.me/2021-01-26-My-2020-in-Review.html" rel="alternate" type="text/html" title="My 2020 in Review" /><published>2021-01-26T00:00:00-05:00</published><updated>2021-01-26T00:00:00-05:00</updated><id>https://www.binwang.me/My-2020-in-Review</id><content type="html" xml:base="https://www.binwang.me/2021-01-26-My-2020-in-Review.html"><![CDATA[<p>2020 is a special year to everyone because of Covid-19. It’s special to me not only because of it, but also because of a big change in my life. I moved from China to Canada at November 2019. I cannot review my life in 2020 without mention that part, so this article is more like 2019 and 2020 in review.</p>

<p>In retrospect, the immigration progress didn’t spend much of my time and energy: find a job and submit documents for visa, that’s all. But it’s a mental challenge because of all the unknowns during the progress: the unknowns about whether I can pass the interview, whether I can get the visa, whether the work in new position is good, whether the life in Canada would be better, and so on. The progress started at early April, so most of the time in 2019 I was waiting and nervous. It seems unnecessary, since the interview went very well, and it’s rare to be rejected for visa in cases like mine. But still, it’s such a big decision in my life, and the political climate seems to be changing everyday. As things rolling out at the beginning of 2020, it proves my worries didn’t come from no where. If my visa was delayed by 2 months, I may not be able to come here until now. It’s because of Covid-19, which is not something one can predict, but the predictable political reasons made it worse.</p>

<p>Toronto is better than I thought. I arrived here at a weekend night. The temporary apartment I booked was at downtown. During the way from airport to the apartment, along Queen Street, I saw the side walk was packed with people. The day after, when I went outside for subway, I found lots of people at subway stations and food court. That surprised me. I expected Toronto would be better than Beijing in many aspects, but I didn’t expect the population density is higher (and after I looked up on Wikipedia, it’s true: 4,334.4/\(km^2\) for Toronto compared to 1,300/\(km^2\) for Beijing). Maybe it depends on the person, but I like to live in a place with more people as long as it’s not too crowded. The city feels more alive and has more energy in this way. During last few years, Beijing made lots of policies to drive people outside. A lot of markets and restaurants were forced close and teared down. Even for the still open ones, there were more restrictions like cannot have outdoor dining spaces or even cannot light a candle. From where I was living, closed to Second Ring Road, which is core downtown area in Beijing, the city seems to be dying for average people. Any Chinese city I traveled around that time was more alive than Beijing. I figured if I couldn’t make it to Canada, I would still move to another city. So when I came to Toronto and found out there are people and even constructions everywhere, I was very happy about it.</p>

<p>The most challenging part after I moved here is the language barrier. Most Chinese in my generation started to study English from fifth grade. I’m not an exception. I constantly read professional English materials since the University. I’ve been worked in international companies for my internship and my last job. Both of them require lots of communications in English. I’ve been to English speaking countries include the US, which has identical culture as Canada. But still, it can be challenging sometimes especially during informal communications. True or not, in China, I believe I’m good at writing articles, have good sense of humor, know more words and historical stories than average person. I could speak with strangers all the way in a 20+ train journey, or in a youth hotel. I’ve started to live in a new city along and dealt with all the things and all kinds of people. I can feel the power of language, words and the way I use them. I know how they can make me to be better understood, make people feel different, and make something to live forever. Suddenly, after moved to a different country with a different language, the advantage became disadvantage. It’s frustrating when I cannot find the right word to express myself during speaking. But that’s also one of the reasons I decided to move here – I want to be better at such an important language. Luckily, I see improvements during the last year even most of the time I was working from home. After all, with all the things happened in 2020, there were always enough topics to talk about.</p>

<p>Another stressful part I started to feel last year is not all because of relocation, but also because of the age. I’m still young, but not as young as before. During the years in University and a few years after, I skipped lots of classes even exams, I spoke out whatever I think, I resigned without ever found next job first. I did whatever I thought was right, and rejected whatever I didn’t want, without care about the risk, because I had few things to lose, and even I lost all of them, I could always start over because I was so young. But from last year, I don’t think like that anymore. Maybe it’s because of the marriage, maybe it’s because of the immigration, maybe it’s because of working in a big company. I suppose that means I’m more mature. I cannot say it’s good or bad. I think it’s just a stage of life.</p>

<p>Other than the challenges, the life here is very good even under pandemic. I’m living in a condo at downtown, which makes everything very convenience. I can go to most places I need by walk or public transport. This is another good surprise – I thought car is a must have in a North American city. The Chinese community is big here. I can find all kinds of Chinese food I used to have in China. Some of them, especially Cantonese ones, are even better than I had in Beijing. I’ve made new friends from work. The nature environment is beautiful and there are lots of outdoor activities available. The political system is more stable and transparent. Last not least, even though the house price is also very high in Toronto, it’s more feasible to make here as home because of the less percentage of down payment. (In theory, down payment percentage 30% in Beijing is not much higher than Toronto. But in practice, the prices for average apartments are so high that they are categorized as luxury ones, which requires much higher percentage of down payment. It usually ends up to almost 50%).</p>

<p>The work here is not bad. Especially the work hours are much better than most companies in China. I get good projects in the team. The good part of working in a big company is the things I developed here have a big impact because so many people are using it. But on the other hand, because it has so many old services, and because of the specific organization I’m in, there are less technical challenges. The nature of our business makes databases able to be easily partitioned, so there is basically no bottleneck for the services. And it involves physical hardware which makes debug harder. Lots of work are around business logic and meetings. Depends on the time of the year, operation works and oncall can be heavy.</p>

<p>So in order to not be rusty, I started to write a side project for fun during part time. I had lots of side projects before. Though one of them gained some popularity, most of them are not finished. Unlike others, I started this project to use all my preferred technicals, best practices and open source tools that I don’t have opportunity to use at work. There is no deadline, no debate in order to restructure the code or choose some tech stacks. I feel very good about it. Maybe it will not be finished at last either, but I find the joys I had when I first started programming.</p>

<p>I also started to write articles on my blog in a higher frequency. I posted 2 to 3 articles per month in average at the second half of 2020. The quality of articles also improved. It helps me to practice English, forces me to learn and have a deeper understand by writing things down. Most importantly, as I said above about the power of words, it makes moments in my life not just slip away. I hope I can continue this in the new year. I feel on some level, it makes part of me eternal.</p>]]></content><author><name></name></author><category term="life" /><summary type="html"><![CDATA[2020 is a special year to everyone because of Covid-19. It’s special to me not only because of it, but also because of a big change in my life. I moved from China to Canada at November 2019. I cannot review my life in 2020 without mention that part, so this article is more like 2019 and 2020 in review.]]></summary></entry><entry><title type="html">Redis Implementation for Cache and Database Consistency</title><link href="https://www.binwang.me/2020-12-14-Redis-Implementation-for-Cache-and-Database-Consistency.html" rel="alternate" type="text/html" title="Redis Implementation for Cache and Database Consistency" /><published>2020-12-14T00:00:00-05:00</published><updated>2020-12-14T00:00:00-05:00</updated><id>https://www.binwang.me/Redis-Implementation-for-Cache-and-Database-Consistency</id><content type="html" xml:base="https://www.binwang.me/2020-12-14-Redis-Implementation-for-Cache-and-Database-Consistency.html"><![CDATA[<p><em>This article belongs to a series of articles about caching. The code in this article can be found at <a href="https://github.com/wb14123/redis_lease">my Github repo</a>.</em></p>

<ol>
  <li><em><a href="/2020-11-02-Use-TLA+-to-Verify-Cache-Consistency.html">Use TLA+ to Verify Cache Consistency</a>.</em></li>
  <li><em>Redis Implementation for Cache and Database Consistency. (This one)</em></li>
</ol>

<p>In the <a href="/2020-11-02-Use-TLA+-to-Verify-Cache-Consistency.html">last article</a>, we introduced an algorithm (described in paper <a href="https://pdos.csail.mit.edu/6.824/papers/memcache-fb.pdf">Scaling Memcache at Facebook</a>) that can do a better job to maintain the data consistency between cache and database. We also used TLA+ to model the algorithm and verified it. In this article, we are going to implement the algorithm in real world for Redis. The implementation is very simple and doesn’t need to change Redis itself. It’s implemented it by using Redis script. However, it’s much harder to verify the correctness. In order to do it, I used <a href="https://jepsen.io/">Jepsen</a> to test it. If you look at the language analysis for the Github repo, you can see most of them are tests. The Redis script implementation, which is written in Lua, is only 5.1% of the project.</p>

<h2 id="algorithm-description">Algorithm Description</h2>

<p>We’ve described the algorithm in the previous article and even write a TLA+ model for it. But just make it easier for the readers, I’ll briefly describe the algorithm here again. Basically, whenever the client get a value from cache, it will be assigned a unique ID (lease) for the key. When the client writes back a new value, it needs to provide the  key’s newest lease ID. And delete the key will also invalidate all its leases.</p>

<h2 id="implementation">Implementation</h2>

<p>The implementation uses <a href="https://redis.io/commands/eval">Redis script</a>, which is written in Lua. It can implement multiple operations and make them atomic. In theory, this can also be done by client, but Redis script provides a consistent implementation across different clients and makes it easier to use. The algorithm is easy, so the implementation is also straight forward. The implementations are under <a href="https://github.com/wb14123/redis_lease/tree/master/scripts">scripts directory of the repo</a>. These scripts also works for Redis cluster (but I didn’t use Jepsen to test it under cluster mode). Here is an example implementation for get:</p>

<div class="language-lua highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span><span style="color:#080;font-weight:bold">local</span> <span style="color:#950">key</span> = <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">{</span><span style="color:#710">'</span></span>..KEYS[<span style="color:#00D">1</span>]..<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">}</span><span style="color:#710">'</span></span>
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span><span style="color:#080;font-weight:bold">local</span> <span style="color:#950">token</span> = ARGV[<span style="color:#00D">1</span>]
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span><span style="color:#080;font-weight:bold">local</span> <span style="color:#950">value</span> = redis.call(<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">get</span><span style="color:#710">'</span></span>, key)
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span><span style="color:#080;font-weight:bold">if</span> <span style="color:#080;font-weight:bold">not</span> value <span style="color:#080;font-weight:bold">then</span>
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>    redis.replicate_commands()
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>    <span style="color:#080;font-weight:bold">local</span> <span style="color:#950">lease_key</span> = <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">lease:</span><span style="color:#710">'</span></span>..key
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>    redis.call(<span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">set</span><span style="color:#710">'</span></span>, lease_key, token)
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>    <span style="color:#080;font-weight:bold">return</span> <span style="background-color:hsla(200,100%,50%,0.06)"><span style="color:#40A">{</span><span style="color:#069">false</span>, token<span style="color:#40A">}</span></span>
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span><span style="color:#080;font-weight:bold">else</span>
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>    <span style="color:#080;font-weight:bold">return</span> <span style="background-color:hsla(200,100%,50%,0.06)"><span style="color:#40A">{</span>value, <span style="color:#069">false</span><span style="color:#40A">}</span></span>
<span class="line-numbers"><a href="#n11" name="n11">11</a></span><span style="color:#080;font-weight:bold">end</span>
</pre></div>
</div>
</div>

<p>After load the script, you can use it like this:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>redis-cli evalsha &lt;script_sha1&gt; 1 &lt;key&gt; &lt;uniq_id&gt;
</pre></div>
</div>
</div>

<p>It will return <code>value, nil</code> if there is value for the key, or <code>nil, lease</code> if there is no value.</p>

<p>One optimization here is, if we have value for the key, we will not store the lease. That’s because in our use case, if we can get the value, we will not get it from database and write back to the cache. This avoids a lot of memory overhead.</p>

<p>Another important decision I made is, when get the value, the client needs to provide a unique ID instead of let Redis provide one. This is because I cannot find a good way to generate unique ID in Redis cluster. In a single instance, it’s easy: just use a key and inc the value each time. You can still generate unique IDs for different keys on a cluster, but it adds a lot of memory overhead. So I decided to let clients provide it. Luckily, it’s not hard, basically every language has UUID implementation and that’s good enough.</p>

<h2 id="testing">Testing</h2>

<p>It’s easy to implement something, but very hard to make sure it’s correct. We can use TLA+ to model the algorithm and explore the state space, or use mathematical method to proof the correctness in theory. But once we implement the algorithm, it’s something different. We cannot make sure it’s exactly the same as what we’ve proofed. That’s why I find using Coq to implement, proof and generate real code is fantastic. But in this case, it’s not implemented in Coq, so we must find some other way to test it. By testing, we still cannot make sure it’s 100% correct, we can just explore as many situations as we can and make sure the system doesn’t behaves in a way we don’t expect.</p>

<p>The tool we use here is <a href="https://jepsen.io/">Jepsen</a>. It provides lots of tools to make it easy to test distributed systems. It can generate many concurrent requests, import different kinds of failures (host down, network partition, clock drift, and so on) to the system, record all the requests and responses, and check the history at the end.</p>

<p>Here is the test case I write: for each client, generate random read and write operations. For read operation, read from cache first, if the value is not found, read from database and write back to the data. For write operations, write to the database and delete the key from cache. Then after all the read and write operations, check whether the data in cache and database are the same. The test case is very simple, it implemented the way we would use the cache.</p>

<p>By providing different arguments to the test command, you can run the test case with raw Redis get/set/del operations, or use get/set/del operation implemented by the scripts. You can also import cache failure during the test.</p>

<p>If we run the test with raw Redis operations, we can find the test is failed. In the last article, we discussed that using plain get/set/del cache operations cannot guarantee cache consistency, so this is expected. If we run the test with our scripts, we can find the test passed. If we run the test with cache system failure, we can see the test failed, which is also expected from last article. The inconsistent because of cache failure can be resolved by clean up cache data after restart. But if the client is failed, it will have the same problem (I didn’t write the test case for this because it’s hard to test client failure in Jepsen), but it’s not a very good idea to cleanup cache in this case. Because client fails all the time, cleanup cache will make operations slow. The best way might be to setup an expire time so the data can be consistent after the key is expired.</p>

<p>Even though all the test result is expected, it doesn’t make sure the implementation is correct, since there are still many situation I didn’t test, like Redis cluster, network partition, database failure and so on. So welcome to add new test cases and break the system!</p>]]></content><author><name></name></author><category term="Redis" /><category term="database" /><category term="consistency" /><category term="Jepsen" /><category term="distributed system" /><summary type="html"><![CDATA[This article belongs to a series of articles about caching. The code in this article can be found at my Github repo.]]></summary></entry><entry><title type="html">Keep Data Consistency During Database Migration</title><link href="https://www.binwang.me/2020-11-29-Keep-Data-Consistency-During-Database-Migration.html" rel="alternate" type="text/html" title="Keep Data Consistency During Database Migration" /><published>2020-11-29T00:00:00-05:00</published><updated>2020-11-29T00:00:00-05:00</updated><id>https://www.binwang.me/Keep-Data-Consistency-During-Database-Migration</id><content type="html" xml:base="https://www.binwang.me/2020-11-29-Keep-Data-Consistency-During-Database-Migration.html"><![CDATA[<p>When a system has been live for a long time, it’s not rare to use newer technologies to improve performance, maintainability, or add new features. One of such changes can be which database to use. This can be the most difficult kind of change. During the migration, there are two data sources, which makes it a distributed system. Make data consistent under a distributed system is very hard and can easily go wrong. In this article, we will explore a way to keep the data consistent during the migration, and maintain a low downtime at the same time.</p>

<h2 id="requirements">Requirements</h2>

<p>There are some requirements in order to use the way described in this article:</p>

<ul>
  <li>The source database support capture data change (CDC) method like MySQL bin log.</li>
  <li>The source database can be dumped with a consistent view and mark the position in data change logs.</li>
  <li>The target database support ACID transactions.</li>
  <li>Both source and target database support read and write permission control.</li>
</ul>

<h2 id="steps">Steps</h2>

<p>There are two basic ideas behind the steps:</p>

<ol>
  <li>The clients only write to one of the databases at a given time. So we can avoid distributed transaction which is error prone and slow.</li>
  <li>We make the switch of database by setup the database permissions. It’s faster than switch from client code and easier to make sure to switch all the clients.</li>
</ol>

<p>Here are the detailed steps:</p>

<h3 id="1-dump-the-source-database-to-target-database">1. Dump the source database to target database</h3>

<p>First, we need to dump the source database with a consistent view. And mark the position we’ve dumped. For example, in MySQL, you can use <code>mysqldump</code> with <code>--master-data</code> to dump the database with a bin log position. (<a href="https://dev.mysql.com/doc/refman/8.0/en/mysqldump.html#option_mysqldump_master-data">Document about the usage</a>). After we get all the data from source database, we can insert them into the target database.</p>

<p>Since this is the first step, it’s very easy to handle failure: just start again from beginning. So it’s very important to capture any error while import the dumped data.</p>

<h3 id="2-capture-changes-from-source-to-target">2. Capture changes from source to target</h3>

<p>The next step is to use the capture data changes from the source database. For example, in MySQL, you can use <a href="https://dev.mysql.com/doc/refman/8.0/en/binary-log.html">bin log</a> to capture the changes and insert them to the target database. Since we have the start position from last step, we know where to start parse and import the changes.</p>

<p>It’s very important to keep order of the changes while importing. So it’s better to use only one process to parse and import the changes. This part is challenging: the performance matters here. <strong>The time to sync all the changes is the downtime we need for migration</strong>.</p>

<p>We also need to make sure we don’t miss any changes or import any changes multiple times even there are system failures. So it’s very important to record the change log position. It’s convenience to write the position into the target database with the same transaction that imports the data. So the position will be synced with the data we imported.</p>

<h3 id="3-deny-writes-to-the-target-database-from-clients">3. Deny writes to the target database from clients</h3>

<p>The easy way to keep data consistency is to have a single source of truth. Until now, we are using the source database as the source of truth and sync changes to the target database. We don’t want to mess up the target database with other writes. So we need to setup the target database permission to deny all the writes from clients. For example, in MySQL, you can grant only <code>select</code> permission to the table for the clients and deny other operations. We allow the read permission so that we can compare the read results at the next step.</p>

<h3 id="4-modify-the-clients-to-read-and-write-both-databases">4. Modify the clients to read and write both databases</h3>

<p>The next step is to make the clients to read and write both source and target databases.</p>

<p>We want to read/write source database first. Use this result if there is no permission error, use the read/write result from target database otherwise.</p>

<p>The read/write to target database has two purposes:</p>

<ol>
  <li>Before switch to the target database, we can verify the target database works as expected by compare read results and write operations. Note that the target database may have lag to sync up, so the results may not always the same. But we can have an understanding of the correctness based on the percentage of same results.</li>
  <li>After we switch to the target database, the read/write results will be used as the real results.</li>
</ol>

<p><strong>If you want to make sure the target database can handle the load, it’s a good idea to allow read/write to the target database for a while .But it’s just as a verification, the data in the target database will not be consistent after that. So after we verify the target database can handle the traffic, we need to cleanup the target database and start from step 1 again.</strong> (We don’t need to modify the client code during the steps).</p>

<p>For the error handling, there are two key points:</p>

<ol>
  <li>Only use target database result if there is permission error from source database. Throw other errors from source database.</li>
  <li>Ignore errors for the target database if the result is not used but make sure to log them, so that it will not affect the current operation while also make sure we don’t have errors before the switch.</li>
</ol>

<p>The client code would be like this:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>db_operation() {
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>  try {
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>    source_result = source_db_operation()
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>  } catch (PermissionException e) {
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>    return target_db_operation()
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>  }
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>  async {
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>    // do the following things async so it will not impact the performance
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>    try {
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>      target_result = target_db_operation()
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>      compare_result(source_result, target_result)
<span class="line-numbers"><a href="#n12" name="n12">12</a></span>    } catch (Exception e) {
<span class="line-numbers"><a href="#n13" name="n13">13</a></span>      log_error(e)
<span class="line-numbers"><a href="#n14" name="n14">14</a></span>    }
<span class="line-numbers"><a href="#n15" name="n15">15</a></span>  }
<span class="line-numbers"><a href="#n16" name="n16">16</a></span>  return source_result
<span class="line-numbers"><a href="#n17" name="n17">17</a></span>}
</pre></div>
</div>
</div>

<h3 id="5-deny-access-to-the-source-database-from-clients-and-wait-for-changes-to-by-synced">5. Deny access to the source database from clients and wait for changes to by synced</h3>

<p>After we are confident with the read and write to the target database, we can make the switch. We switch the database by change the database permissions. First, we deny all the access to the source database from clients. Then we wait for the changes to be full synced to the target database. During this time, the system is down. So how fast the changes are synced from source database to target database determines how much down time it will be.</p>

<h3 id="6-allow-write-to-target-database">6. Allow write to target database</h3>

<p>After the target database is fully synced, we can enable the target database permission for all the clients. After this, the system should be online again and the database is fully switched over.</p>

<h3 id="7-optional-fallback-to-source-database-if-anything-goes-wrong">7. Optional: Fallback to source database if anything goes wrong</h3>

<p>It’s good if everything works well so far. But that may not always the case. Maybe the target database cannot handle the new traffic (that’s why it’s important to test it in step 4). In this case, we need fallback to the source database.</p>

<p>If it’s fine to lost committed data during the migration time, it would be relatively easy to fallback:</p>

<ol>
  <li>Allow access to the source database. After this, the clients should be using the source database again.</li>
  <li>Cleanup the target database and start from the beginning.</li>
</ol>

<p>If it’s critical to save the committed data and make sure they are consistent, then before step 5, we should setup a mechanism to capture changes from target database to source database, and mark the change position after step 6. Then the fallback steps would be:</p>

<ol>
  <li>Deny all the writes to target database.</li>
  <li>Sync from target database to source database (make sure to stop it after fully synced).</li>
  <li>Allow access to the source database.</li>
  <li>Cleanup the target database and start from the beginning again.</li>
</ol>

<p>The sync from target database to source database is very dangerous and hard to test, so it’s really important to test the target database can handle the operations in step 4.</p>

<h3 id="8-cleanup-client-code">8. Cleanup client code</h3>

<p>Once the database is switched to the target database, we can cleanup the code that access the source database. Then the database is fully migrated and you can enjoin it!</p>]]></content><author><name></name></author><category term="database" /><category term="distributed system" /><category term="consistency" /><summary type="html"><![CDATA[When a system has been live for a long time, it’s not rare to use newer technologies to improve performance, maintainability, or add new features. One of such changes can be which database to use. This can be the most difficult kind of change. During the migration, there are two data sources, which makes it a distributed system. Make data consistent under a distributed system is very hard and can easily go wrong. In this article, we will explore a way to keep the data consistent during the migration, and maintain a low downtime at the same time.]]></summary></entry><entry><title type="html">DNS Resolving Bug in iOS 14</title><link href="https://www.binwang.me/2020-11-08-DNS-Resolving-Bug-in-iOS-14.html" rel="alternate" type="text/html" title="DNS Resolving Bug in iOS 14" /><published>2020-11-08T00:00:00-05:00</published><updated>2020-11-08T00:00:00-05:00</updated><id>https://www.binwang.me/DNS-Resolving-Bug-in-iOS-14</id><content type="html" xml:base="https://www.binwang.me/2020-11-08-DNS-Resolving-Bug-in-iOS-14.html"><![CDATA[<h2 id="the-bug-description">The Bug Description</h2>

<p>iOS 14 has a bug for DNS resolving under this circumstances:</p>

<ul>
  <li>Manually specify a custom DNS server for a WiFi network.</li>
  <li>For a domain, this custom DNS server has different DNS record than the default public DNS server.</li>
  <li>The DNS record type of this domain is CNAME on public DNS server.</li>
</ul>

<p>Under this setup, after connecting to WiFi with custom DNS, iOS 14 should get the IP for this domain according the record on custom DNS server. However, it still gets the IP that’s on public DNS server.</p>

<p>Here is an example. The table below shows the DNS records on public DNS servers:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Domain</th>
      <th style="text-align: left">DNS Record Type</th>
      <th style="text-align: left">Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">domain-1</td>
      <td style="text-align: left">CNAME</td>
      <td style="text-align: left">domain-2</td>
    </tr>
    <tr>
      <td style="text-align: left">domain-2</td>
      <td style="text-align: left">A</td>
      <td style="text-align: left">1.1.1.1</td>
    </tr>
  </tbody>
</table>

<p>So <code>domain-1</code> will be resolved to <code>1.1.1.1</code> by using the default public DNS server.</p>

<p>Then we have a custom DNS server, which modifies record for <code>domain-1</code>. For other records, it uses the upstream ones:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Domain</th>
      <th style="text-align: left">DNS Record Type</th>
      <th style="text-align: left">Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">domain-1</td>
      <td style="text-align: left">A</td>
      <td style="text-align: left">2.2.2.2</td>
    </tr>
    <tr>
      <td style="text-align: left">domain-2</td>
      <td style="text-align: left">A</td>
      <td style="text-align: left">1.1.1.1</td>
    </tr>
  </tbody>
</table>

<p>So if you use the custom DNS server, <code>domain-1</code> should be resolved to <code>2.2.2.2</code>.</p>

<p>However in iOS 14, even if you manually specify the custom DNS server for the WiFi network, <code>domain-1</code> is still resolved to <code>1.1.1.1</code>.</p>

<p>I guess this is because of DNS cache problem. I tried to clean up the DNS cache by changing my device to airplane mode, reboot the device, or stay in the WiFi with custom DNS for days. Despite all the attempts, the problem still exists.</p>

<p>I filled a bug report to Apple but didn’t get any response after almost one month. So I think I can share it here so it may help someone else with the same problem.</p>

<h2 id="how-do-i-know-its-not-the-problem-of-my-dns-setup">How Do I know It’s not the Problem of My DNS Setup</h2>

<p>The setup above works on every other devices I have (Linux devices, MacOS devices). It also works on the same iOS device before I upgraded it to iOS 14. I also installed the app <a href="https://apps.apple.com/us/app/network-analyzer-pro/id557405467">Network Analyzer Pro</a> to debug the setup. In Network Analyzer Pro, if I use the DNS resolving tool with the custom DNS server, it can resolve the right IP address. But if I ping the domain directly, it resolved to the wrong IP. So there is something wrong at the system level of iOS 14.</p>

<h2 id="workaround">Workaround</h2>

<p>This bug is very frustrating. I spent lots of time to identify what’s the issue: for the application, to the permission setup (iOS 14 has a new local network permission), to the system. And finally found it’s iOS system DNS resolving problem and found a workaround.</p>

<p>The workaround is to add a DNS record for the domain behind CNAME (in this case <code>domain-2</code>) into the custom DNS server:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Domain</th>
      <th style="text-align: left">DNS Record Type</th>
      <th style="text-align: left">Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">domain-1</td>
      <td style="text-align: left">A</td>
      <td style="text-align: left">2.2.2.2</td>
    </tr>
    <tr>
      <td style="text-align: left">domain-2</td>
      <td style="text-align: left">A</td>
      <td style="text-align: left">2.2.2.2</td>
    </tr>
  </tbody>
</table>

<p>However, if you have multiple domains point to <code>domain-2</code> and don’t want to change IP addresses for those domains, this workaround may not be able to support that use case.</p>

<h2 id="a-little-more-story">A Little More Story</h2>

<p>It may looks weird that I have a DNS setup like this. I host some services on my desktop machine. Because of my home router provided by ISP disabled NAT loopback (which means the router denies all the traffic that comes from itself), I must use the IP address in local network to access the services if my devices are in the same network. So I setup a custom DNS server that resolves the service domains to my desktop’s internal IP address, and use this DNS server when I’m using home WiFi.</p>

<p>I depend on this workflow heavily. Everything worked smoothly until I upgraded to iOS 14. For more than 1 month, under the home WiFi connection (which is most of the time because of CoVID-19), I cannot sync my calendar or notes, or fetch an E-Book from my collections to iPad, or upload my photos automatically, or chat with my friends on phone, and so on. It’s very frustrating and is still not fixed after two version upgrades of iOS 14.</p>

<p>After my previous Android phone broken, I decided to buy an old generation iPhone. One big reason is it’s really cheap with the carrier contract. I’m trying to reduce my time on phones so an old generate is more than enough. In many ways it’s much better than an Android phone: for an Android phone, I need so many tweaks to make sure it respects my privacy and apps not running at background all the time. And after that, many apps are not usable or don’t have proper notifications. On contrast, Apple controls the ecosystem strongly to make sure the developers don’t abuse the system. But on the other hand, Apple also controls the users strongly. It’s hard to downgrade the system. It’s hard to debug the system. It’s even hard to submit a bug report: you need to enroll Beta profile and use an app to submit it. (I’m not sure if there are other ways but it’s the suggested way on Apple’s website). With an Apple mobile device, I feel more like renting it instead of owning it. I really hope one day there is a device that’s both open to users and have a strong permission management to limit the app behaviors (including the apps owned by device provider). It must be hard based on the user base, but doesn’t hurt to have some hopes.</p>]]></content><author><name></name></author><category term="DNS" /><category term="iOS" /><category term="technology" /><category term="self hosted" /><summary type="html"><![CDATA[The Bug Description]]></summary></entry><entry><title type="html">Use TLA+ to Verify Cache Consistency</title><link href="https://www.binwang.me/2020-11-02-Use-TLA+-to-Verify-Cache-Consistency.html" rel="alternate" type="text/html" title="Use TLA+ to Verify Cache Consistency" /><published>2020-11-02T00:00:00-05:00</published><updated>2020-11-02T00:00:00-05:00</updated><id>https://www.binwang.me/Use-TLA+-to-Verify-Cache-Consistency</id><content type="html" xml:base="https://www.binwang.me/2020-11-02-Use-TLA+-to-Verify-Cache-Consistency.html"><![CDATA[<blockquote>
  <p>There are only two hard things in Computer Science: cache invalidation and naming things.</p>

  <p>– Phil Karlton</p>
</blockquote>

<p><em>This article belongs to a series of articles about caching.</em></p>

<ol>
  <li><em>Use TLA+ to Verify Cache Consistency (This one)</em></li>
  <li><em><a href="2020-12-14-Redis-Implementation-for-Cache-and-Database-Consistency.html">Redis Implementation for Cache and Database Consistency</a></em></li>
</ol>

<p>During web service development, it’s very usual to use a cache before the database. It’s so common that almost becomes the default solution whenever there is a performance issue. But a lot of people don’t really think about the consistency between database and cache. A main reason is it’s so hard to reason about the consistency under a distributed system. So in this article, I will explore how to use TLA+ to specify different cache algorithms and use TLC to check whether it will keep the data consistency between cache and database. All the code in this article is available at <a href="https://github.com/wb14123/tla-cache">my Github repo</a>. The code may be updated after this article is published.</p>

<h2 id="system-architecture">System Architecture</h2>

<p>Let’s first describe the normal architecture of the cache and database system. Normally, a web service query data from a database and save data to it. The service itself usually doesn’t store any stateful data. This makes it’s very easy to scale up: just start a bunch of servers and put them behind a load balancer. But for the database, it’s not so easy. It’s usually very hard to scale up a database. So when the performance of the database is an issue, we usually put a cache before it. The cache stores everything in memory so it would be much faster and can handle much more requests than a traditional database that needs to persistent everything. When we read data, we read cache first and only load it from database if there is no data in cache. When save data, we must persistent the data to database. Here is a graph about what this architecture looks like:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>web_server_1
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>web_server_2
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>web_server_3  &lt;--&gt; cache &lt;--&gt; database
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>...
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>web_server_n
</pre></div>
</div>
</div>

<p>We need to notice this is a different architecture than the cache of CPU. In multi-core CPUs, each core has it’s own cache instead of having a shared cache, which looks like this:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>core_1 &lt;--&gt; cache_1
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>core_2 &lt;--&gt; cache_2
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>core_3 &lt;--&gt; cache_3  &lt;---&gt; main memory
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>...
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>core_n &lt;--&gt; cache_n
</pre></div>
</div>
</div>

<p>Because of the differences of architectures, consistency and latency requirements, they usually needs different solutions. In this article, we only talk about the cache algorithms for web services.</p>

<h2 id="cache-algorithms">Cache Algorithms</h2>

<p>The cache algorithms are very simple. When read data, read data from cache first. If there is no data in cache, read from database and write it back to cache. When write data, because normally database has stronger consistency model and can persistent data better, we usually write to database first, then write to cache or invalidate cache. In our example, we invalidate data after write since it’s the most widely used one and has less consistency issues.</p>

<p>This is the pseudocode for this algorithm:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>read(key) {
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>    cache = readCache(key)
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>    if (cache != null) {
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>        return cache
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>    }
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>    data = readDB(key)
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>    writeCache(key, data)
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>    return data
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>}
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>write(key, value) {
<span class="line-numbers"><a href="#n12" name="n12">12</a></span>    writeDB(key, value)
<span class="line-numbers"><a href="#n13" name="n13">13</a></span>    invalidateCache(key)
<span class="line-numbers"><a href="#n14" name="n14">14</a></span>}
</pre></div>
</div>
</div>

<h2 id="tla-specification">TLA+ Specification</h2>

<p>Once we have the algorithm in mind, we can write a TLA+ specification and let TLC to check whether it has the properties we want. A TLA+ specification is not a 100% map from the system, it’s an abstraction that omits irrelevant details. In our specification, we make two key abstractions:</p>

<ol>
  <li>Data is inconsistent between cache and database if one row is inconsistent. So in the specification, we only care about one row. Which means we don’t need the parameter for <code>key</code>.</li>
  <li>In the specification, we don’t care about what’s the actual value as long as each client writes different values. So we let the client write it’s own ID as the value. Thus we can also omit <code>value</code> from write behavior.</li>
</ol>

<p>We need also to notice that if we write multiple state changes in one TLA+ statement, it means those state changes are atomic. In the code, we assume read/write cache/database is atomic while others are not, which means each line is an atomic operation but the lines between them are not. So for each of the lines, we should write separate statements.</p>

<p>So keeping this in mind, we have these variables for our specification:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>CONSTANT CLIENTS
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>VARIABLE cache, db, cacheResults, dbResults, cacheWritten, states
</pre></div>
</div>
</div>

<ul>
  <li>CLIENTS: all the clients</li>
  <li>cache: the value in cache</li>
  <li>db: the value in database</li>
  <li>cacheResults: read cache results for each client</li>
  <li>dbResults: read DB results for each client</li>
  <li>cacheWritten: whether the cache has been written.</li>
  <li>states: the state of clients</li>
</ul>

<p>The reason we want to have a variable <code>cacheWritten</code> is, we want to make sure the algorithm really wrote to the cache. Otherwise it would be simple to keep data consistent by not using the cache at all. (We don’t check this property in this article, but it’s not hard to add that).</p>

<p>For <code>states</code>, we want all client start with <code>free</code> and ends with <code>done</code> when read/write is done. It can go from <code>done</code> to <code>free</code> again to start another read/write.</p>

<p>Then we have <code>Null</code> value for no data in cache or database, <code>InitValue</code> for any value that exists before the system is running, and all the data that could be put into cache and database;</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>Null == &quot;null&quot;
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>InitValue == &quot;init&quot;
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>Data == CLIENTS \union {Null, InitValue}
</pre></div>
</div>
</div>

<p>Once these basic values are ready, it’s not hard to write the specification for cache and database interface. The specification of it is in <a href="https://github.com/wb14123/tla-cache/blob/master/CacheInterface.tla">CacheInterface.tla</a>. Then we can use the interface to specify the algorithm described above: <a href="https://github.com/wb14123/tla-cache/blob/master/WriteInvalidateCache.tla">WriteInvalidateCache.tla</a>.</p>

<p>Finally, we want to also specify what property we want for our system. We want the data in cache and database to be consistent. It would be very hard to make them to be the same all the time. So we make a reasonable weaker statement: we want to make sure once all the clients are done, either the cache doesn’t have any data, or it has the same data as in database:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>AllDone == \A c \in CLIENTS: states[c] = &quot;done&quot;
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>Consistency == IF AllDone THEN (cache = db \/ cache = Null) ELSE TRUE
</pre></div>
</div>
</div>

<p>We can put the specification and all the properties we want to check into a TLC model config file <a href="https://github.com/wb14123/tla-cache/blob/master/WriteInvalidateCache.cfg">WriteInvalidateCache.cfg</a> and let TLC to check it:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>tlc WriteInvalidateCache
</pre></div>
</div>
</div>

<p>After running this, it will show it violates <code>Consistency</code>, and also show all the sequences to reach the violation. In this way, we know this algorithm cannot guarantee consistency between cache and database.</p>

<h2 id="a-better-algorithm">A Better Algorithm</h2>

<p>An algorithm that makes data possible to be inconsistent doesn’t necessary makes it a bad algorithm. It maybe faster than stronger consistent algorithms and some application is fine with stale data. It all depends on the use case. What makes it a bad algorithm is people use it without truly understand it.</p>

<p>In this section, I’ll introduce an algorithm with better consistency. It’s introduced by the paper <a href="https://pdos.csail.mit.edu/6.824/papers/memcache-fb.pdf">Scaling Memcache at Facebook</a>. In this algorithm, if there is a cache miss during read, the cache server will return a lease token to client. A newer token or the invalidate of the cache will make previous token invalidate. The client can only write cache if it has a valid token.</p>

<p>I implemented the specification of this algorithm in <a href="https://github.com/wb14123/tla-cache/blob/master/WriteInvalidateLease.tla">WriteInvalidateLease.tla</a> and the model config <a href="https://github.com/wb14123/tla-cache/blob/master/WriteInvalidateLease.cfg">WriteInvalidateLease.cfg</a>. If you run the TLC checker on it:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>tlc WriteInvalidateLease
</pre></div>
</div>
</div>

<p>You will find it passed the check. Because we specify two clients in <code>WriteInvalidateLease.cfg</code>, so it means it meets the <code>Consistency</code> requirement with two concurrent clients:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>CLIENTS = {&quot;c1&quot;, &quot;c2&quot;}
</pre></div>
</div>
</div>

<p>You can make it as many clients as you want. But increase one client will increase the checking time a lot. Usually 3 clients is enough. It will not guarantee it meets the property under unlimited clients (we need formal proof to do that), but it will give us much higher confidence on the algorithm.</p>

<h2 id="what-else-can-go-wrong">What Else Can Go Wrong</h2>

<p>In the algorithm above, we can see the data in cache and database can be consistent once the client has done the work. But it doesn’t say any thing about client failure. Because the client write to database then invalid the cache, if the client is down during these two operations, it will not invalid the cache so it will have stale data. To specify this in TLA+, we can add two behaviors into Next:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>Failure(c) == /\ states' = [states EXCEPT ![c] = &quot;fail&quot;]
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>              /\ UNCHANGED &lt;&lt;cache, db, cacheResults, dbResults, cacheWritten, lease&gt;&gt;
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>Recover(c) == /\ states[c] = &quot;fail&quot;
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>              /\ states' = [states EXCEPT ![c] = &quot;start&quot;]
<span class="line-numbers"><a href="#n6" name="n6">6</a></span>              /\ UNCHANGED &lt;&lt;cache, db, cacheResults, dbResults, cacheWritten, lease&gt;&gt;
</pre></div>
</div>
</div>

<p>Since it’s much less likely for the server to not graceful shutdown, this problem is much smaller than the previous one. And the effect can be limited by having a TTL for cache. However, you need to understand that to know if the algorithm really meets your need.</p>

<p>Another thing need to notice is the consistency property we defined above is a really weak one. It doesn’t guarantee you always read the newest data.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this article, we can see how easy a cache algorithm can go wrong and how to use TLA+ to specify and verify it. Based on the specification examples above, you can verify any cache algorithm you like: for example, instead of invalidate the cache after write to database, update the cache and see how it works.</p>

<p>In next articles, I will write an implementation for Redis to use the lease token algorithm. And compare the performance between database and cache to see if we really need to use cache in some use cases.</p>]]></content><author><name></name></author><category term="TLA+" /><category term="cache" /><category term="database" /><category term="consistency" /><category term="algorithm" /><category term="distributed system" /><summary type="html"><![CDATA[There are only two hard things in Computer Science: cache invalidation and naming things. – Phil Karlton]]></summary></entry><entry><title type="html">How to Estimate Max TPS from TPM</title><link href="https://www.binwang.me/2020-10-18-How-to-Estimate-Max-TPS-from-TPM.html" rel="alternate" type="text/html" title="How to Estimate Max TPS from TPM" /><published>2020-10-18T00:00:00-04:00</published><updated>2020-10-18T00:00:00-04:00</updated><id>https://www.binwang.me/How-to-Estimate-Max-TPS-from-TPM</id><content type="html" xml:base="https://www.binwang.me/2020-10-18-How-to-Estimate-Max-TPS-from-TPM.html"><![CDATA[<p>It’s good to understand the TPS (transaction per second) of a service. But sometimes we only have TPM (transaction per minute) metrics. It may because we don’t have TPS metric at all since it needs resources to compute, or it has been deleted because storing all the historical per second metrics needs a lot of storage space. So we need to estimate TPS from TPM (or even longer time period, which the method below also applies). It’s not hard to get an average TPS from TPM: just divide TPM by 60. However, because the database and the dependency services have a limit on how many concurrent requests it can handle, we also need to understand what’s the max TPS. In this article, we will explore how to do that.</p>

<p>We have an assumption before we go to the solution: we assume that in the time period of one minute, the requests to the service has the same probability to happen at any time. In another word, the requests are independently of the time since last request. This means the time of the requests is a uniform distribution. This is a reasonable assumption: though most services has peak requests during a day, it tends to be distributed evenly in a short period like one minute,. We need to notice that the equal of probability doesn’t mean all the requests <strong>will</strong> arrive evenly in the minute, otherwise max TPS will be the same as average TPS.</p>

<p>With this assumption in mind, we can use <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a> to solve this problem. The probability of how many times the event occurs in the interval of time can be solved by this:</p>

<p><span>\(P(TPS=k) =  \frac{\lambda^k e^{-\lambda}}{k!}\)</span></p>

<p><span>\(k\)</span> means how many times the event happens in the interval of time. <span>\(\lambda\)</span> means the average of times that the event will occur in the interval.</p>

<p>In our case, the interval of time is 1 second. So <span>\(\lambda\)<span> is the average TPS: <span>\(TPM / 60\)</span>. And <span>\(P(k)\)</span> means the probability of the TPS during this minute.</span></span></p>

<p>So we have the probability of the TPS. But what we want is the max TPS. If we want to know what’s the probability of max TPS equals n, we can add all the probabilities of TPS under n:</p>

<p><span>\(P(max TPS = n) = \sum_{k=0}^{n} P(TPS=k) = \sum_{k=0}^{n} \frac{\lambda^k e^{-\lambda}}{k!}\)</span></p>

<p>Then we can draw a graph of this function and select n that makes the probability almost to 1. I recommend <a href="https://www.wolframalpha.com">Wolfram Alpha</a> to draw the graph. Though it needs paid version to show a more clear graph, the free version is enough for our use case.</p>

<p>Let’s give an example. Suppose we find our max TPM during peak time is 1200, then the average TPS during that minute is 200, which means <span>\(\lambda = 200\)</span>. Then we can draw a graph of <span>\(P(max TPS=n)\)</span> with <span>\(\lambda = 200\)</span>:</p>

<p><img src="/static/images/2020-10-18-How-to-Estimate-max-TPS-from-TPM/p-lambda-200.png" alt="p-lambda-200" /></p>

<p>From the graph, we can see a max TPS of 260 is a safety choice. And in this minute, about 50% of the chance that the TPS will above the average TPS 200.</p>

<p>Sometimes the dependency has a throttling mechanism. It may has a target throttling configuration as TPS, but actually count the throttling number by sub-second metrics like transactions per 100ms. (Ideally this shouldn’t be the case but sometimes that happens and we don’t always have control over dependency services). In this case, we need to count max transactions per 100ms. Which <span>\(\lambda = 20\)</span>:</p>

<p><img src="/static/images/2020-10-18-How-to-Estimate-max-TPS-from-TPM/p-lambda-20.png" alt="p-lambda-20" /></p>

<p>From the graph, we can see max transactions per 100ms would be more like 36. And when we provide a target throttling TPS, we should multiply this by 10 which is 360, a lot higher than 260.</p>

<p>The calculation above also applies when it count throttling number independently on multiple hosts. (Again, it should have a better throttling counting mechanism). For example, if the dependency service has 10 hosts and it chooses random host to handle the request, then we should count max TPS per host. Which <span>\(\lambda\)</span> is also 20 and target TPS configuration should be 360 instead of 260.</p>

<p><em>Update at Sep 22, 2022: Fix the formula from <span>\(e^{-k}\)</span> to <span>\(e^{-\lambda}\)</span>. The graphs and results were computed with <span>\(e{-\lambda}\)</span> so they are still correct.</em></p>]]></content><author><name></name></author><category term="probability theory" /><category term="math" /><category term="technology" /><summary type="html"><![CDATA[It’s good to understand the TPS (transaction per second) of a service. But sometimes we only have TPM (transaction per minute) metrics. It may because we don’t have TPS metric at all since it needs resources to compute, or it has been deleted because storing all the historical per second metrics needs a lot of storage space. So we need to estimate TPS from TPM (or even longer time period, which the method below also applies). It’s not hard to get an average TPS from TPM: just divide TPM by 60. However, because the database and the dependency services have a limit on how many concurrent requests it can handle, we also need to understand what’s the max TPS. In this article, we will explore how to do that.]]></summary></entry></feed>