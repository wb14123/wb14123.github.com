<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://www.binwang.me/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.binwang.me/" rel="alternate" type="text/html" /><updated>2024-04-11T11:10:44-04:00</updated><id>https://www.binwang.me/feed.xml</id><title type="html">Bin Wang - My Personal Blog</title><subtitle>This is my personal blog about computer science, technology and my life.</subtitle><entry><title type="html">Prevent htmx Lazy Loaded Content From Reloading</title><link href="https://www.binwang.me/2024-03-26-Prevent-HTMX-Lazy-Loaded-Content-From-Reload.html" rel="alternate" type="text/html" title="Prevent htmx Lazy Loaded Content From Reloading" /><published>2024-03-26T00:00:00-04:00</published><updated>2024-03-26T00:00:00-04:00</updated><id>https://www.binwang.me/Prevent-HTMX-Lazy-Loaded-Content-From-Reload</id><content type="html" xml:base="https://www.binwang.me/2024-03-26-Prevent-HTMX-Lazy-Loaded-Content-From-Reload.html"><![CDATA[<p>This is a short article about some tricks in <a href="https://htmx.org">htmx</a>. I have more to say about htmx but I’ll save that to another blog. In this one, I will skip the basics about htmx and assume you already know that.</p>

<h2 id="1-problem">1. Problem</h2>

<p>I’ll briefly introduce two features of htmx in order the explain the problem. You can go to official website for more details about the features.</p>

<h3 id="11-browser-history">1.1. Browser History</h3>

<p>htmx has <a href="https://htmx.org/docs/#history">a feature to interact with browser history</a>. Here is an example in the official document:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;a</span> <span class="na">hx-get=</span><span class="s">"/blog"</span> <span class="na">hx-push-url=</span><span class="s">"true"</span><span class="nt">&gt;</span>Blog<span class="nt">&lt;/a&gt;</span>
</code></pre></div></div>

<p>This will change the url in browser to <code class="language-plaintext highlighter-rouge">/blog</code> when you click the link and save a snapshot of current page into local storage. When you click back button in browser, htmx will try to find the cache in local storage, and swap it out so you don’t need to reload the whole page.</p>

<h3 id="12-lazy-load">1.2. Lazy Load</h3>

<p>htmx sends requests when an event is triggered on an element. The rule is defined by <a href="https://htmx.org/attributes/hx-trigger/">hx-trigger</a> attribute. There are some special events can be used for lazy loading:</p>

<ul>
  <li>load - triggered on load (useful for lazy-loading something).</li>
  <li>revealed - triggered when an element is scrolled into the viewport (also useful for lazy-loading).</li>
  <li>intersect - fires once when an element first intersects the viewport.</li>
</ul>

<p>However, when combined this with history support, the lazy loaded elements will be requested again when the pages are navigated in history. Here is an example:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;a</span> <span class="na">hx-get=</span><span class="s">"/page1"</span> <span class="na">hx-push-url=</span><span class="s">"true"</span> <span class="na">hx-target=</span><span class="s">"#content"</span><span class="nt">&gt;</span>page1<span class="nt">&lt;/a&gt;</span>
<span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">"content"</span> <span class="na">hx-get=</span><span class="s">"/content"</span> <span class="na">hx-trigger=</span><span class="s">"load"</span><span class="nt">&gt;&lt;/div&gt;</span>
</code></pre></div></div>

<p>When you click on <code class="language-plaintext highlighter-rouge">page1</code>, it will replace <code class="language-plaintext highlighter-rouge">#content</code> with the response from <code class="language-plaintext highlighter-rouge">/page1</code> and change the URL. However, when you click on back in browser, htmx will send a request to <code class="language-plaintext highlighter-rouge">/content</code> again even though it’s already in history cache, because technically, <code class="language-plaintext highlighter-rouge">#content</code> <strong>is</strong> loaded again so <code class="language-plaintext highlighter-rouge">hx-get</code> is triggered based on <code class="language-plaintext highlighter-rouge">hx-trigger</code> rule. This results a waste of resource and can sometimes make the webpage lost previous scroll position.</p>

<p>In this article, I’ll show some tricks to prevent this. They are very simple once you know them but sometimes it’s just hard to get when you are new to the framework.</p>

<h2 id="2-best-solution-swap-outer-html-instead-of-inner-html">2. Best Solution: Swap Outer HTML instead of Inner HTML</h2>

<p>I think this is the best solution. It’s so simple that I don’t know why I didn’t get it earlier. Anyway, that’s why I write this blog so that it can help more people like me.</p>

<p>By default, htmx swap the inner HTML of the element. So the <code class="language-plaintext highlighter-rouge">hx-trigger="load"</code> attribute is still there after the content is loaded and will be triggered again when load from history. The solution is to just let htmx swap the outer HTML instead. Using the same example, the code will be changed to this:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;a</span> <span class="na">hx-get=</span><span class="s">"/page1"</span> <span class="na">hx-push-url=</span><span class="s">"true"</span> <span class="na">hx-target=</span><span class="s">"#content"</span><span class="nt">&gt;</span>page1<span class="nt">&lt;/a&gt;</span>
<span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">"content"</span> <span class="na">hx-get=</span><span class="s">"/content"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;div</span> <span class="na">hx-get=</span><span class="s">"/content"</span> <span class="na">hx-trigger=</span><span class="s">"load"</span> <span class="na">hx-target=</span><span class="s">"this"</span> <span class="na">hx-swap=</span><span class="s">"outerHTML"</span><span class="nt">&gt;&lt;/div&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div></div>

<p>In the new implementation, we have another <code class="language-plaintext highlighter-rouge">div</code> tag inside <code class="language-plaintext highlighter-rouge">#content</code> to do the lazy load. After the response is loaded, it will swap out the whole <code class="language-plaintext highlighter-rouge">div</code> element so <code class="language-plaintext highlighter-rouge">hx-get</code> and <code class="language-plaintext highlighter-rouge">hx-trigger</code> are not there anymore when the snapshot is taken and loaded from history.</p>

<p>As I said, this is the best solution in my mind and I think it fits all the cases. So if you only care about the solution, you can stop reading here. I record the following solutions simply because I figured them out earlier than this one.</p>

<h2 id="3-solution-b-dont-snapshot-the-whole-body">3. Solution B: Don’t Snapshot the Whole Body</h2>

<p>The solution above removes the htmx attributes. The solution in section tackles the problem in another direction: it prevents the element from loading again when go back in history.</p>

<p>By default, htmx will take the snapshot of <code class="language-plaintext highlighter-rouge">body</code> and put it into history cache. That’s why when go back in history, the <code class="language-plaintext highlighter-rouge">load</code> event of the element is triggered again. To prevent it, we can let htmx only snapshot children of <code class="language-plaintext highlighter-rouge">#content</code>. <a href="https://htmx.org/docs/#specifying-history-snapshot-element">Here</a> is the official doc about how to do it. Using the same example, the code will be changed into:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;a</span> <span class="na">hx-get=</span><span class="s">"/page1"</span> <span class="na">hx-push-url=</span><span class="s">"true"</span> <span class="na">hx-target=</span><span class="s">"#content"</span><span class="nt">&gt;</span>page1<span class="nt">&lt;/a&gt;</span>
<span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">"content-load"</span> <span class="na">hx-get=</span><span class="s">"/content"</span> <span class="na">hx-trigger=</span><span class="s">"load"</span> <span class="na">hx-target=</span><span class="s">"#content"</span><span class="nt">&gt;&lt;/div&gt;</span>&gt;
<span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">"content"</span> <span class="na">hx-history-elt</span><span class="nt">&gt;&lt;/div&gt;</span>
</code></pre></div></div>

<p>Here we load the content with <code class="language-plaintext highlighter-rouge">#content-load</code> element. htmx will only swap out <code class="language-plaintext highlighter-rouge">#content</code> when we forward or go back in browser history since we added <code class="language-plaintext highlighter-rouge">hx-history-elt</code> on <code class="language-plaintext highlighter-rouge">#content</code>. This prevents <code class="language-plaintext highlighter-rouge">load</code> event from being triggered on <code class="language-plaintext highlighter-rouge">#content-load</code> so it will not send a new request.</p>

<p>But this solution has great limitations: you need to change the snapshot element which is not always possible.</p>

<h2 id="4-solution-c-remove-htmx-action-attributes-before-taking-snapshot">4. Solution C: Remove htmx Action Attributes Before Taking Snapshot</h2>

<p>This is a solution that could work in theory but I didn’t test it, because I came up with the best solution when thinking about it.</p>

<p>The idea is similar: we don’t want htmx action attributes like <code class="language-plaintext highlighter-rouge">hx-get</code> when we load the history. Other than swap the whole outerHTML, there is a htmx event you can catch in Javascript to remove the attribute before taking a snapshot:</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">htmx</span><span class="p">.</span><span class="nf">on</span><span class="p">(</span><span class="dl">'</span><span class="s1">htmx:beforeHistorySave</span><span class="dl">'</span><span class="p">,</span> <span class="kd">function</span><span class="p">()</span> <span class="p">{</span>
  <span class="nb">document</span><span class="p">.</span><span class="nf">getElementById</span><span class="p">(</span><span class="dl">'</span><span class="s1">#content</span><span class="dl">'</span><span class="p">).</span><span class="nf">removeAttributes</span><span class="p">(</span><span class="dl">"</span><span class="s2">hx-get</span><span class="dl">"</span><span class="p">))</span>
<span class="p">})</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="htmx" /><category term="web" /><category term="UI" /><category term="Javascript" /><summary type="html"><![CDATA[This is a short article about some tricks in htmx. I have more to say about htmx but I’ll save that to another blog. In this one, I will skip the basics about htmx and assume you already know that.]]></summary></entry><entry><title type="html">Travel Back to China</title><link href="https://www.binwang.me/2024-03-19-Travel-Back-to-China.html" rel="alternate" type="text/html" title="Travel Back to China" /><published>2024-03-19T00:00:00-04:00</published><updated>2024-03-19T00:00:00-04:00</updated><id>https://www.binwang.me/Travel-Back-to-China</id><content type="html" xml:base="https://www.binwang.me/2024-03-19-Travel-Back-to-China.html"><![CDATA[<p>After more than 4 years staying aboard without being able to go back to China because of Covid, I finally had the chance and spent this Chinese New Year at my hometown. Now I’ve come back to Toronto, it’s time to record it when my memory and feelings are still fresh.</p>

<h2 id="before-we-go">Before We Go</h2>

<p>Being able to travel back doesn’t mean it’s easy. The number of flights between China and other countries are still not recovered to pre Covid level. In order to keep our budget in a reasonable level, we need to fly through 2 stops, and then drive more than 3 hours to home from the airport. The flying time and the waiting time at airport combined is more than 24 hours. It’s a very long trip for any adult, not to mention traveling with a 6 months old baby. I was very nervous about that since our longest trip with the baby was taking her to the clinic. But since we haven’t been back for so many years, this is a travel that shouldn’t be delayed anymore. On the bright side, my sister in law will come to meet us at the first stop Tokyo. We will stay there for a few days for resting and sight seeing.</p>

<p>Things were not smooth before we go. First, our company had a bad outrage during the holiday season so I needed to work overtime. Following that was hot debates about the following steps to make our services better, which made me very frustrate for reasons I’d rather not to talk here. And during all these things, the whole family also caught cold and had fever for a few days. It’s the first time the baby is sick so it’s very stressful situation. The baby awoke every 2 hours at night while my wife and I were sick. When everyone finally recovered, we barely had the time to get new Covid vaccines and pack the baggages.</p>

<p>Anyway, we successfully handled everything before we go, took the 15 hours long flight and headed to our first stop, Tokyo.</p>

<h2 id="stay-in-tokyo">Stay in Tokyo</h2>

<p>I had been to Tokyo twice. But they are both many years ago and I didn’t have enough time to see the whole city. Even though it’s a relatively short visit again this time: just 3 - 4 days, it still got me excited to be there again.</p>

<p>We wanted to book a hotel near Asakusa (浅草) area since Sensō-ji is a must see site in Tokyo. We also want a subway station nearby. At last we found a place near Ueno (上野) station. It’s a traditional Japanese style hotel that has Tatamis, which is perfect for us: our baby doesn’t like to sleep in the crib anymore, so Tatamis is much safer since she cannot fall from it, and the mattress is also much firmer than the ones in regular hotels, which prevent the baby’s face from buried into the mattress. And she can also play on it during day time as well.</p>

<p>Only after I booked the hotel, I found out Ueno area is a place I wanted to visit but didn’t have enough time last time: when my wife and I visited Tokyo last time, we planned to take the train from Ueno station to the airport. We didn’t notice the schedule of that train is less frequent than subway. So we didn’t plan the time ahead and missed it. Disappointedly, we decided to have some food near Ueno station first. That’s when we found out the area around Ueno: there are many pedestrian streets filled with street food, outdoor eatings, restaurants, shops, and people. I was so fascinated by it and it was a shame that we didn’t have enough time to explore the area since we needed to head to Kyoto at that day. I forgot the name of that area since then because we were in such a hurry and we visited so many places in Japan after that. But when I was checking the surrounding area of the hotel on Google Maps, the Ueno station struck my memory and I was so excited that I had another opportunity to fully explore that area.</p>

<p>So Ueno and Asakusa are where we explored most when we were at Tokyo. We went to the Tokyo National Museum and enjoyed the ukiyo-e (浮世絵, wood block prints) exhibition that I wanted to see long before we went. We ate some delicious food at Ueno area. We also bought some electronic devices and manga books at Akihabara (秋葉原). What I didn’t expect was the experience at Asakusa: last time I only visited Sensō-ji and the street in the front of it. I didn’t know there is a larger area surrounding it that has lots of traditional Japanese style buildings, shops and restaurants. We found a shop by coincidence that sells high quality ukiyo-e prints. There are many places selling them in Tokyo, but they are either low quality or too expensive. I’m so glad to find a shop that sells lots of high quality prints in reasonable price range.</p>

<p>The whole experience in Tokyo is very positive. The mix of tradition Japanese and modern culture creates a very unique vibe. Because Japanese culture is largely impacted by Chinese culture in the past, I think I can appreciate more of the beauty of it. I have a fresh eye when I looked at the city after I explored more on the topic of city design in the past years: the non car centric culture, high quality public transit and high density of population makes it very different from North American cities. The city is more vibrant, much cleaner and safer, and have so many interesting places to explore. But unfortunately, Japan is a country better for visiting than long term living for foreigners because of its (almost non-exist of) immigration culture and stressful working environment.</p>

<p>The biggest happy surprise we got from Japan is our baby can sleep the whole night! It’s such a life changing improvement for my wife and me. After having the baby, I felt like there is nothing more important than being able to sleep a whole night. It’s so great to have that back!</p>

<p>On that happy note, we continued the trip back to home.</p>

<h2 id="back-to-hometown">Back to Hometown</h2>

<p>I thought there would be lots of feelings on the road to home. But there wasn’t. Maybe because of there are too many concrete things to worry about with a baby on the road so it left little room for feelings.</p>

<p>With the things happened in the past years in China: the lockdown of cities during Covid, the re-election of Xi which broke the political practice, the protests of both the re-eleciton and lockdowns, the broke of Evergrande Group, and the downhill of America-China relationship, you’d imagine China is in a pretty bad place. However, when I went back to home, I found things were not as bad as I thought. Yes, economic has gone bad: there are lots of unfinished buildings, small businesses are struggling, it’s harder to find a job for new grad, nationalism is on the rise and so on. But on another hand, at least from my limited experience, people’s life is still going on. I saw there is disapproval when people talk about the economic and government policies, but I saw little desperation feelings. When there are fewer ways to make life better, people continue to find new ways. For good or for bad, that’s the resilience of Chinese people. Maybe it looks better than normal because it’s holiday season: the malls and restaurants are packed with people. Beautiful decoration lights are everywhere. There are fireworks everyday.</p>

<p>Theoretically, fireworks have been banned for many years in China. However, with the lift of lockdown at the end of last year, there were lots of celebrations with fireworks at the new year (not the Chinese New Year) and created some conflicts between the crowd and the police in some cities. After that, the ban of fireworks still exists but is rarely enforced. Fireworks in the new year’s eve has been a tradition since ancient time. But in my opinion, the mixed feelings it brings represents the complexity of contemporary China perfectly: It’s believed to be able to dispel bad luck, which is much needed after the Covid and the following weak economic. It extends to some level of superstitious that some people believe it can cleanup the virus in the air. It also seems to be a subtle way to express disapproval of government policies because the ban is still in place. Of cause there is also pure excitement about the lights and sounds, the happiness about holiday, and wishes for a better year ahead.</p>

<p>Another reason of the weak economic not showing much trace may be my hometown is a small city so the trend is kind of lagged behind. It’s still benefiting from the development of the bigger cities in the past years: more and more big brands and chain stores are opening so there are more choices when buying things. Food delivery is more convenient. There are also more culture innovation products with better traditional Chinese aesthetic. If not considering education and healthcare system, the everyday life has little difference from big cities, or even better because of the less stressful working environment.</p>

<p>Not all things are good. Not mentioning the things that were already there before I left China, there is one new development that would trouble me a lot if I lived for a longer time: the lack of privacy both in the real world and in the cyber space. In the real world, cameras are everywhere. Lots of people start to use smart locks on the door that has a camera that you cannot avoid when you pass by. You must have scan the face in order to enter some residential compounds. Every crossing has high resolution cameras recording license numbers of cars and are able to recognize the drivers. In Beijing, face is recorded when entering every subway station. Even worse, when I was playing arcade games in a mall, the arcade machine has a camera that took a photo of me without reminding me first. On the cyber space side, there is little service you can use without installing an app and register an account that linked to your phone number and in turn linked to your ID. The worst experience I had is at a parking lot: there was no person at the exit and you need to scan the QR code to register an account, input personal details and pay the fee in order to leave. Again, there was nothing reminds you that before you actually try to leave and scan the QR code. I guess it’s not like there is no one in China cares about the privacy, it’s more like an already lost battle because the desire of surveillance from both the government and tech giants, and the lack of power to balance that.</p>

<p>Another thing I dislike is the trend of city planning. I think the city did a very good job in the past: reasonable density and mixed use was very well maintained. There are dedicated bike lanes, wide sidewalks and reasonable public transit coverage. However, with the widely adoption of cars, things got worth and the city seems just want to change things in the name of changing. That’s kind of understandable because there are more opportunity for corrupt when there are more projects. But at the end, parking lots replaced lots of green spaces on sidewalks. Roads has been re-designed with confusing turning lanes which replaced some bike lanes. Traffic lights replaced lots of roundabouts, and even worse, sometimes traffic lights are combined with roundabouts which is totally unnecessary. If the changes are limited because of the old foundation, then it’s not surprise that the worse place happens at the newly developed areas. It’s mostly all high rise residential buildings with little commercial uses. That’s kind of understandable as well since one of the main income of government is by selling land to developers. Seems like there were some commercial uses planned but the progress get delayed because of the real estate crisis. But the most ridiculous part is the roadway network design: there are many very wide roads. Many of them have 10 lanes! And some of them even have additional 2-3 lanes service road on each side. Be aware those are not highways. In a grid layout, those very wide roads are just beside residential buildings and are connected without skipping any crossing. It’s such a waste of resource because if there are so many cars that such wide roads are needed, then the non exist of road hierarchy doesn’t make any sense. Combined with the lack of commercial uses, it makes people rely more on cars and makes traffic very bad for commercial areas. Just go outside for a walk like the old days is not enjoyable anymore in the newly developed areas.</p>

<p>Despite all those things, it’s still a vacation at my hometown. So my mind was laid back even though I was very busy physically: my wife’s sister got married just days after we arrived. My wife and I also had the wedding that was planned years ago but got pushed because of Covid. I’m very happy how the wedding went considering we need to take care of the baby at the same time. If we were not preparing for the wedding, we took the baby to my parents and my in-law’s places. Between the gaps, I also needed to find some time to meet with friends that I haven’t seen for a long time. So it’s a very packed schedule but it’s so different (in a good way) to be close with family and friends again. Being aboard so many years and having a baby gave me a new perspective of the importance of family and friends.</p>

<p>However, I couldn’t stay there for long. I left half a month’s parental leave for the travel. But even combined with that, one month is basically the most I can have for a vacation and the company doesn’t allow work from China. So even we felt like we haven’t spent much time at home yet, we needed to go back. The trip back to Toronto has 3 stops on the way. So it’s another battle to fight. Our first stop is Beijing and we will stay at the airport for one night.</p>

<h2 id="one-night-in-beijing">One Night in Beijing</h2>

<p>I lived in Beijing for 8 years. It’s the second longest city I’ve lived in, just behind my hometown. It’s the longest if considering only the time of adulthood. So I have lots of memory and friends there. It’s unfortunate that I can only stay there for one night but it’s better than nothing.</p>

<p>Just before the day of leaving for Beijing, there was a snowstorm and most highways were closed as a result. We booked the train from a nearby city because the time of the train is better. But since the highway was closed, we changed the departure station to our hometown city. It was a very cold morning and we needed to leave for the railway station at 5:00am. When we were waiting at the station, there was an announcement that said the train was delayed. Following that, there were more announcements and the train was delayed longer and longer. Luckily, while debating if it’s better to go home instead of waiting in the station, the delay got shorter and we were finally able to aboard the train.</p>

<p>Things got better after this rocky start. We took the subway to the airport after arrived at Beijing since we didn’t have the baby’s car seat with us. It’s mostly underground on the way so I had little opportunity to see the city. It’s almost time for dinner when we arrived the hotel in the airport. If not because of the delay of train, we could arrive at noon. I made the plan to meet some friends and have dinner together. I left early from the hotel to walk around the city before I meet them.</p>

<p>I took the subway to Sanyuan Bridge (三元桥). It’s the northeast corner of the Third Ring Road and only one stop away from the airport by the airport express line. I went to a mall first. It’s still early for a weekday so it’s a little bit quite there. I decided to walked to a nearby subway station Liangma Bridge (亮马桥), which is a place surrounded by many embassies. I met my friends there and walked to the nearby restaurant together. Two of my jobs were at that area so I have lots of memory there. Walking along the streets at night, everything feels very familiar but also has a sense of distance. There are lots of restaurants and shops disappeared or changed owners, but the base layout is the same. While taking the subway, walking along the narrow road that has barriers to separate it from the Third Ring Road, going through the underground tunnel, I recognized the familiar feeling: Beijing is like a big machine or beast that doesn’t care about normal human beings. The city is not designed with human scale. Multiple ring road highways cut through the city with giant crossing bridges. But that doesn’t make driving easier because the traffic is still bad and only limited cars can be on the road at weekdays based on the license number. The public transit is wonderful compared to North American cities and most of the people use it. But the subway is usually packed with people during commute hours and stations have maze like paths for exits and connection to another station. It can suck all the remaining energy out after a work day. The pace of life is fast and people are busy. It’s not an enjoyable city to live. But it’s still the capital of China and is the biggest city of the north. There is no shortage of people live there with the hope of a better life. I was one of them and it gave me valuable adventures. I don’t love the city but I love the memory with it.</p>

<h2 id="back-to-toronto">Back to Toronto</h2>

<p>We left early next morning for the flight to Tokyo, then to Montreal, then to Toronto. We had the opportunity to fully explore the airports at Tokyo and Montreal because we left enough time in between of the flights. The longest flight from Tokyo to Montreal is full but fortunately the baby was able to sleep most of the time. We arrived at Toronto at night which means it’s another morning in China and it has been more than 24 hours since we took the first flight from Beijing.</p>

<p>The whole travel went much better than I thought. We not only see the family and friends after 4 years, we also have the experience of traveling a long distance with the baby so it opens so many possibilities in the future. It would be great if I can stay longer with family every year in the future.</p>]]></content><author><name></name></author><category term="life" /><category term="travel" /><category term="China" /><category term="Japan" /><category term="Beijing" /><category term="Tokyo" /><summary type="html"><![CDATA[After more than 4 years staying aboard without being able to go back to China because of Covid, I finally had the chance and spent this Chinese New Year at my hometown. Now I’ve come back to Toronto, it’s time to record it when my memory and feelings are still fresh.]]></summary></entry><entry><title type="html">Scala 2 Macro Tutorial</title><link href="https://www.binwang.me/2023-12-29-Scala-Macro-Tutorial.html" rel="alternate" type="text/html" title="Scala 2 Macro Tutorial" /><published>2023-12-29T00:00:00-05:00</published><updated>2023-12-29T00:00:00-05:00</updated><id>https://www.binwang.me/Scala-Macro-Tutorial</id><content type="html" xml:base="https://www.binwang.me/2023-12-29-Scala-Macro-Tutorial.html"><![CDATA[<p>Macros are powerful but complex. Especially when the language itself like Scala is already complex. The lack of learning resource and documents makes it more so. In this article, I’ll write down some of my learnings and hopefully it can help someone else who is new to it as well. I’ll keep the examples small and simple so it’s easier to understand. Since I’m still learning it, I may continue to update this article on the way, or write a new article if there is a big topic. Either way, I’ll make notes here so you know there are updates.</p>

<p>Scala’s macro syntax and APIs can be different from version to version. Especially it’s almost completely redesigned in Scala 3. This article only targets Scala 2 and I’ve only tested the examples on Scala 2.13.</p>

<h2 id="1-what-is-macro">1. What is Macro</h2>

<p>The basic idea of macro is to modify the code with code. For example, let’s imagine a macro <code class="language-plaintext highlighter-rouge">plusToMinus</code> that modifies all the plus operations of integers to minus:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plusToMinus</span> <span class="o">{</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">}</span>
</code></pre></div></div>

<p>This will be compiled to <code class="language-plaintext highlighter-rouge">1 - 1</code> and ends up as <code class="language-plaintext highlighter-rouge">0</code>.</p>

<p>Of cause this is not a practical example and not all the languages’ macro system can do it. But this demonstrate what macros can do where normal code cannot. Here is a more practical example: when we log something in different log levels, the API usually looks like this:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">v</span> <span class="k">=</span> <span class="o">...</span>
<span class="nv">logger</span><span class="o">.</span><span class="py">info</span><span class="o">(</span><span class="n">s</span><span class="s">"This is a info log. Value: $v"</span><span class="o">)</span>
<span class="nv">logger</span><span class="o">.</span><span class="py">warn</span><span class="o">(</span><span class="n">s</span><span class="s">"This is a warning. Value: $v"</span><span class="o">)</span>
</code></pre></div></div>

<p>However, with this kind of interface, the string <code class="language-plaintext highlighter-rouge">s"..."</code> need to be computed before passed in to the method, which is a waste since not all the strings need to be logged based on the log level configuration. Especially when <code class="language-plaintext highlighter-rouge">v.toString</code> needs a lot of resource to compute. So in language like Java, the values are usually passed in as separate parameters:</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">String</span> <span class="n">v</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">logger</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">"This is a info log. Value: {}"</span><span class="o">,</span> <span class="n">v</span><span class="o">);</span>
<span class="n">logger</span><span class="o">.</span><span class="na">warn</span><span class="o">(</span><span class="s">"This is a warning. Value: {}"</span><span class="o">,</span> <span class="n">v</span><span class="o">);</span>
</code></pre></div></div>

<p>Even though it resolves the problem, the interface is kind of awful. And not all the users know this kind of details so they may still just construct the string directly instead of pass in separate parameters. However, with the help of macros, you can still keep the logger interface in the intuitive way. As macros, <code class="language-plaintext highlighter-rouge">logger.info</code> and <code class="language-plaintext highlighter-rouge">logger.warn</code> can modify the code directly during the compile time. For example, it can modify the code like this:</p>

<p>From</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">logger</span><span class="o">.</span><span class="py">info</span><span class="o">(</span><span class="n">s</span><span class="s">"This is a info log. Value: $v"</span><span class="o">)</span>
</code></pre></div></div>

<p>To</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">if</span> <span class="o">(</span><span class="n">loggerLevel</span> <span class="o">&gt;=</span> <span class="nc">INFO</span><span class="o">)</span> <span class="o">{</span>
  <span class="nf">println</span><span class="o">(</span><span class="n">s</span><span class="s">"This is a info log. Value: $v"</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div></div>

<p>So that the actually string computation is not done unless log level is configured to print it.</p>

<h2 id="2-how-to-write-a-macro">2. How to Write a Macro</h2>

<p>Different languages have different syntaxes to write a macro. On the simpler side, macros in C can only do text substitution. On the powerful side, Lisp languages can modify the AST (abstract syntax tree) very easily because the code itself is written as a tree structure. The macro in Scala is on the powerful side since it is able to modify the AST even though it may not be as intuitive as Lisp. There are multiple ways to do it. But essentially, the process it to take the current AST as input and output a new AST. The APIs of reading AST input is very similar to reflection APIs (and in fact, sometimes they share some APIs). Generating a new AST part is more complex. In the following sections, we will walk through how to setup a SBT project to write macros, how to read an AST and how to generate a new AST.</p>

<h2 id="3-project-setup-with-sbt">3. Project Setup with SBT</h2>

<p>In Scala, the implementation of macros and the use of macros need to be compiled separately. So if you are using SBT, they need to be in different sub projects. Here is an example of <code class="language-plaintext highlighter-rouge">build.sbt</code>:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">lazy</span> <span class="k">val</span> <span class="nv">root</span> <span class="k">=</span> <span class="o">(</span><span class="n">project</span> <span class="n">in</span> <span class="nf">file</span><span class="o">(</span><span class="s">"."</span><span class="o">))</span>
  <span class="o">.</span><span class="py">aggregate</span><span class="o">(</span><span class="n">core</span><span class="o">,</span> <span class="n">coretest</span>
  <span class="o">.</span><span class="py">settings</span><span class="o">(</span>
    <span class="n">name</span> <span class="o">:=</span> <span class="s">"archmage"</span>
  <span class="o">)</span>

<span class="k">lazy</span> <span class="k">val</span> <span class="nv">core</span> <span class="k">=</span> <span class="o">(</span><span class="n">project</span> <span class="n">in</span> <span class="nf">file</span><span class="o">(</span><span class="s">"core"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">settings</span><span class="o">(</span>
    <span class="n">name</span> <span class="o">:=</span> <span class="s">"core"</span><span class="o">,</span>
    <span class="n">libraryDependencies</span> <span class="o">++=</span> <span class="nc">Seq</span><span class="o">(</span>
      <span class="s">"org.scala-lang"</span> <span class="o">%</span> <span class="s">"scala-reflect"</span> <span class="o">%</span> <span class="s">"2.13.12"</span><span class="o">,</span>
      <span class="s">"co.fs2"</span> <span class="o">%%</span> <span class="s">"fs2-core"</span> <span class="o">%</span> <span class="s">"3.9.3"</span><span class="o">,</span>
    <span class="o">)</span>
  <span class="o">)</span>

<span class="k">lazy</span> <span class="k">val</span> <span class="nv">coretest</span> <span class="k">=</span> <span class="o">(</span><span class="n">project</span> <span class="n">in</span> <span class="nf">file</span><span class="o">(</span><span class="s">"coretest"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">settings</span><span class="o">(</span>
    <span class="n">name</span> <span class="o">:=</span> <span class="s">"core-test"</span>
  <span class="o">)</span> <span class="n">dependsOn</span> <span class="n">core</span>
</code></pre></div></div>

<p>It creates two sub projects. You can implement the macros in <code class="language-plaintext highlighter-rouge">core</code> and use them in <code class="language-plaintext highlighter-rouge">coretest</code>.</p>

<p>If you want to debug the generated code from macros, add debug flags to Scala like this in <code class="language-plaintext highlighter-rouge">build.sbt</code>:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">ThisBuild</span> <span class="o">/</span> <span class="n">scalacOptions</span> <span class="o">+=</span> <span class="s">"-Ymacro-debug-lite"</span>
</code></pre></div></div>

<h2 id="4-how-to-read-ast">4. How to Read AST</h2>

<h3 id="41-read-macro-parameters">4.1 Read macro parameters:</h3>

<p>Here is the basic syntax of a macro. First, define a macro implementation:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">macroImpl</span><span class="o">(</span><span class="n">c</span><span class="k">:</span> <span class="kt">blackbox.Context</span><span class="o">)(</span><span class="n">s</span><span class="k">:</span> <span class="kt">c.Expr</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="k">:</span> <span class="kt">c.Expr</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="nf">println</span><span class="o">(</span><span class="nv">s</span><span class="o">.</span><span class="py">tree</span><span class="o">.</span><span class="py">symbol</span><span class="o">.</span><span class="py">fullName</span><span class="o">)</span>
  <span class="n">s</span>
<span class="o">}</span>
</code></pre></div></div>

<p>The first parameter <code class="language-plaintext highlighter-rouge">c: blackbox.Context</code> is a must have for a macro implementation. There is also a <code class="language-plaintext highlighter-rouge">whitebox.Context</code> but we will not cover it in this article. More details about whitebox can be found in <a href="https://docs.scala-lang.org/overviews/macros/blackbox-whitebox.html">the official document</a>.</p>

<p>The remaining parameters of the implementation method are parameters for the macro. For example, if you want to take a parameter of type <code class="language-plaintext highlighter-rouge">String</code> for the macro, then the implementation of macro will take <code class="language-plaintext highlighter-rouge">c.Expr[String]</code> as a parameter, which <code class="language-plaintext highlighter-rouge">c.Expr[String]</code> is the tree representation of the macro’s <code class="language-plaintext highlighter-rouge">String</code> parameter. The same applies to the return type of the macro. You can also  use <code class="language-plaintext highlighter-rouge">c.Tree</code> instead of <code class="language-plaintext highlighter-rouge">c.Expr[T]</code>. They can be converted between each other, which we will see in section 4.4.</p>

<p>This example prints out the variable name of the passed in parameter and return the parameter without modification. Note that the printing happens at compile time since that’s when the implementation of the macro is ran. Only the returned tree or <code class="language-plaintext highlighter-rouge">c.Expr</code> is used at run time. So this macro is not doing anything useful, it’s just a demo of how to read the input tree.</p>

<p>Once we have the macro implementation, we can define the macro like this:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">macroTest</span><span class="o">(</span><span class="n">arg</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="n">macro</span> <span class="n">macroImpl</span>
</code></pre></div></div>

<p>Then we can use it in another (sub) project so that the compilation is separated:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">a</span> <span class="k">=</span> <span class="s">"abc"</span>
<span class="nf">macroTest</span><span class="o">(</span><span class="n">a</span><span class="o">)</span>
</code></pre></div></div>

<p>It will print out the full name of <code class="language-plaintext highlighter-rouge">a</code> like this during compilation:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>me.binwang.archmage.coretest.MethodMetaTest.a
</code></pre></div></div>

<p>The API of <code class="language-plaintext highlighter-rouge">c.Expr</code> is very similar as reflection API. You can experiment with it by print out different things from it and see what you can get.</p>

<h3 id="42-read-type-parameters">4.2 Read type parameters:</h3>

<p>Macro can also take generic type as parameters. The example below takes a parameter of any type and print out its type at compile time.</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">macroImpl</span><span class="o">[</span><span class="kt">T:</span> <span class="kt">c.WeakTypeTag</span><span class="o">](</span><span class="n">c</span><span class="k">:</span> <span class="kt">blackbox.Context</span><span class="o">)(</span><span class="n">s</span><span class="k">:</span> <span class="kt">c.Expr</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="k">:</span> <span class="kt">c.Expr</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="nf">println</span><span class="o">(</span><span class="nv">c</span><span class="o">.</span><span class="py">weakTypeOf</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span>
  <span class="n">s</span>
<span class="o">}</span>

<span class="k">def</span> <span class="nf">macroTest</span><span class="o">[</span><span class="kt">T</span><span class="o">](</span><span class="n">s</span><span class="k">:</span> <span class="kt">T</span><span class="o">)</span><span class="k">:</span> <span class="kt">T</span> <span class="o">=</span> <span class="n">macro</span> <span class="n">macroImpl</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span>
</code></pre></div></div>

<p>Which can be used like this:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">macroTest</span><span class="o">(</span><span class="s">"abc"</span><span class="o">)</span>
<span class="nf">macroTest</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
</code></pre></div></div>

<p>The output during compilation will be:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>String
Int
</code></pre></div></div>

<h3 id="43-read-implicit-parameters">4.3 Read implicit parameters:</h3>

<p>Macro can have implicit parameters, but the macro implementation shouldn’t define them as implicit. Otherwise Scala compiler will give confusing errors. See <a href="https://github.com/scala/bug/issues/6494">this issue</a> for more details.</p>

<p>In the following example, <code class="language-plaintext highlighter-rouge">macroTest</code> takes an implicit double variable and return it as the new generated tree:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">macroImpl</span><span class="o">(</span><span class="n">c</span><span class="k">:</span> <span class="kt">blackbox.Context</span><span class="o">)(</span><span class="n">s</span><span class="k">:</span> <span class="kt">c.Expr</span><span class="o">[</span><span class="kt">String</span><span class="o">])(</span><span class="n">num</span><span class="k">:</span> <span class="kt">c.Expr</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span> <span class="k">:</span> <span class="kt">c.Expr</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="nf">println</span><span class="o">(</span><span class="n">s</span><span class="s">"Name of implicit num: ${num.tree.symbol.fullName}"</span><span class="o">)</span>
  <span class="n">num</span>
<span class="o">}</span>

<span class="k">def</span> <span class="nf">macroTest</span><span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">)(</span><span class="k">implicit</span> <span class="n">num</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="n">macro</span> <span class="n">macroImpl</span>
</code></pre></div></div>

<p>Note how <code class="language-plaintext highlighter-rouge">num</code> in <code class="language-plaintext highlighter-rouge">macroImpl</code> doesn’t have any <code class="language-plaintext highlighter-rouge">implicit</code> definition.</p>

<p>Then the test code:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">implicit</span> <span class="k">val</span> <span class="nv">num</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="mf">1.1</span>
<span class="nf">println</span><span class="o">(</span><span class="nf">macroTest</span><span class="o">(</span><span class="s">"abc"</span><span class="o">))</span>
</code></pre></div></div>

<p>It will print this during the compile time:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Name of implicit num: me.binwang.archmage.coretest.MethodMetaTest.num
</code></pre></div></div>

<p>And this during the run time:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.1
</code></pre></div></div>

<h3 id="44-read-code-block-with-by-name-parameter">4.4 Read code block with by-name parameter</h3>

<p>Macros can also take <a href="https://docs.scala-lang.org/tour/by-name-parameters.html">by-name parameter</a>. However, it needs to use <code class="language-plaintext highlighter-rouge">c.Tree</code> instead of <code class="language-plaintext highlighter-rouge">c.Expr</code> as parameter in the macro implementation:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">macroImpl</span><span class="o">(</span><span class="n">c</span><span class="k">:</span> <span class="kt">blackbox.Context</span><span class="o">)(</span><span class="n">s</span><span class="k">:</span> <span class="kt">c.Tree</span><span class="o">)</span> <span class="k">:</span> <span class="kt">c.Expr</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="nf">println</span><span class="o">(</span><span class="n">s</span><span class="o">)</span>
  <span class="nv">c</span><span class="o">.</span><span class="py">Expr</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="n">s</span><span class="o">)</span>
<span class="o">}</span>

<span class="k">def</span> <span class="nf">macroTest</span><span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="o">=&gt;</span> <span class="nc">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="n">macro</span> <span class="n">macroImpl</span>
</code></pre></div></div>

<p>See how <code class="language-plaintext highlighter-rouge">c.Tree</code> is converted to <code class="language-plaintext highlighter-rouge">c.Expr</code>. You can also convert <code class="language-plaintext highlighter-rouge">c.Expr</code> to <code class="language-plaintext highlighter-rouge">c.Tree</code> by using the <code class="language-plaintext highlighter-rouge">.tree</code> method, which we’ve seen in the examples above.</p>

<p>Test it with this code:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">macroTest</span> <span class="o">{</span>
  <span class="k">val</span> <span class="nv">a</span> <span class="k">=</span> <span class="s">"a"</span>
  <span class="k">val</span> <span class="nv">b</span> <span class="k">=</span> <span class="s">"b"</span>
  <span class="nf">println</span><span class="o">(</span><span class="s">"hello!"</span><span class="o">)</span>
  <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
<span class="o">}</span>
</code></pre></div></div>

<p>It will print out this during compile time:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
  val a: String = "a";
  val b: String = "b";
  scala.Predef.println("hello!");
  a.+(b)
}
</code></pre></div></div>

<h3 id="45-use-quasiquotes">4.5 Use Quasiquotes</h3>

<p><a href="https://docs.scala-lang.org/overviews/quasiquotes/intro.html">Quasiquotes</a>, or <code class="language-plaintext highlighter-rouge">q"..."</code>, is a very powerful syntax for Scala macro. It can both match a tree and generate a tree. For example, the following code can match different parts of a if else clause to <code class="language-plaintext highlighter-rouge">c.Tree</code> variables:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">macroImpl</span><span class="o">(</span><span class="n">c</span><span class="k">:</span> <span class="kt">blackbox.Context</span><span class="o">)(</span><span class="n">s</span><span class="k">:</span> <span class="kt">c.Tree</span><span class="o">)</span><span class="k">:</span> <span class="kt">c.Tree</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">import</span> <span class="nn">c.universe._</span>
  <span class="k">val</span> <span class="nv">q</span><span class="s">"if ($cond) $thenp else $elsep"</span> <span class="k">=</span> <span class="n">s</span>
  <span class="nf">println</span><span class="o">(</span><span class="n">cond</span><span class="o">)</span>
  <span class="nf">println</span><span class="o">(</span><span class="n">thenp</span><span class="o">)</span>
  <span class="nf">println</span><span class="o">(</span><span class="n">elsep</span><span class="o">)</span>
  <span class="n">q</span><span class="s">"$cond"</span>
<span class="o">}</span>

<span class="k">def</span> <span class="nf">macroTest</span><span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="o">=&gt;</span> <span class="nc">Any</span><span class="o">)</span><span class="k">:</span> <span class="kt">Any</span> <span class="o">=</span> <span class="n">macro</span> <span class="n">macroImpl</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">cond</code>, <code class="language-plaintext highlighter-rouge">thenp</code> and <code class="language-plaintext highlighter-rouge">elsep</code> are all matched parts from the input tree.</p>

<p><code class="language-plaintext highlighter-rouge">q"$cond"</code> generates a new tree using the matched condition part of the tree. We will see more details in how to use quasiquotes to generate trees in section 5.4.</p>

<p>Test it with this code:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">bigNum</span> <span class="k">=</span> <span class="mi">2</span>
<span class="k">val</span> <span class="nv">smallNum</span> <span class="k">=</span> <span class="mi">1</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="n">macroTest</span> <span class="o">{</span>
  <span class="nf">if</span> <span class="o">(</span><span class="n">bigNum</span> <span class="o">&gt;</span> <span class="n">smallNum</span><span class="o">)</span> <span class="o">{</span>
    <span class="s">"no surprise"</span>
  <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
    <span class="s">"surprise!"</span>
  <span class="o">}</span>
<span class="o">}</span>
<span class="nf">println</span><span class="o">(</span><span class="n">result</span><span class="o">)</span>
</code></pre></div></div>

<p>During the compile time, it will print out the different parts of the tree that we have asked it to match:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bigNum.&gt;(smallNum)
"no surprise"
"surprise!"
</code></pre></div></div>

<p>And during the run time, it will print out the value of condition instead of either if or else clause:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>true
</code></pre></div></div>

<p>More examples about how to match the tree can be found in <a href="https://docs.scala-lang.org/overviews/quasiquotes/syntax-summary.html">the document</a>. Click on each example to see more details.</p>

<h2 id="5-how-to-generate-tree">5. How to Generate Tree</h2>

<h3 id="51-construct-tree-directly-with-api">5.1 Construct Tree Directly with API</h3>

<p>An AST can be constructed from the classes that represent the tree. For example, a constant of string can be created by <code class="language-plaintext highlighter-rouge">Literal(Constant("I replaced you!"))</code>. The following example replace any string to <code class="language-plaintext highlighter-rouge">I replaced you</code>:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">macroImpl</span><span class="o">(</span><span class="n">c</span><span class="k">:</span> <span class="kt">blackbox.Context</span><span class="o">)(</span><span class="n">s</span><span class="k">:</span> <span class="kt">c.Expr</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="k">:</span> <span class="kt">c.Expr</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="k">import</span> <span class="nn">c.universe._</span>
  <span class="nv">c</span><span class="o">.</span><span class="py">Expr</span><span class="o">[</span><span class="kt">String</span><span class="o">](</span><span class="nc">Literal</span><span class="o">(</span><span class="nc">Constant</span><span class="o">(</span><span class="s">"I replaced you!"</span><span class="o">)))</span>
<span class="o">}</span>

<span class="k">def</span> <span class="nf">macroTest</span><span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="n">macro</span> <span class="n">macroImpl</span>
</code></pre></div></div>

<p>With the code below, it will print <code class="language-plaintext highlighter-rouge">I replaced you!</code> instead of <code class="language-plaintext highlighter-rouge">abc</code>:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">println</span><span class="o">(</span><span class="nf">macroTest</span><span class="o">(</span><span class="s">"abc"</span><span class="o">))</span>
</code></pre></div></div>

<p>This is a very simple example. When the tree becomes larger and larger , it’s more and more difficult to construct a tree with this approach. It’s like a much worse version of lisp. So in the following sections, we will see some easier ways to construct a tree.</p>

<h3 id="52-use-cparse">5.2 Use <code class="language-plaintext highlighter-rouge">c.parse</code>:</h3>

<p><code class="language-plaintext highlighter-rouge">c.parse</code> can parse a string as Scala code and generate an AST. For example, the following macro returns the variable name of a <code class="language-plaintext highlighter-rouge">String</code>:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">macroImpl</span><span class="o">(</span><span class="n">c</span><span class="k">:</span> <span class="kt">blackbox.Context</span><span class="o">)(</span><span class="n">s</span><span class="k">:</span> <span class="kt">c.Expr</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">c.Expr</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="nv">name</span> <span class="k">=</span> <span class="nv">s</span><span class="o">.</span><span class="py">tree</span><span class="o">.</span><span class="py">symbol</span><span class="o">.</span><span class="py">fullName</span>
  <span class="nv">c</span><span class="o">.</span><span class="py">Expr</span><span class="o">(</span><span class="nv">c</span><span class="o">.</span><span class="py">parse</span><span class="o">(</span><span class="n">s</span><span class="s">""" "Name of var is: $name" """</span><span class="o">))</span>
<span class="o">}</span>

<span class="k">def</span> <span class="nf">macroTest</span><span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="n">macro</span> <span class="n">macroImpl</span>
</code></pre></div></div>

<p>Then use it like this:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">a</span> <span class="k">=</span> <span class="s">"abc"</span>
<span class="nf">println</span><span class="o">(</span><span class="nf">macroTest</span><span class="o">(</span><span class="n">a</span><span class="o">))</span>
</code></pre></div></div>

<p>It will print out:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Name of var is: me.binwang.archmage.coretest.MethodMetaTest.a
</code></pre></div></div>

<p>Note the output is at run time instead of compile time like the examples in the last section, because we’ve replaced the tree with new code.</p>

<h3 id="53-use-reify">5.3 Use <code class="language-plaintext highlighter-rouge">reify</code></h3>

<p><code class="language-plaintext highlighter-rouge">c.parse</code> is easy to use and understand. But when generating more and more code with it, it can be pretty messy since it is just a string. There is no syntax checks in IDE. Even worse, you cannot get any run time information to use in the generated tree.</p>

<p><code class="language-plaintext highlighter-rouge">reify</code> is a much better option. You can write code as usual. The code in <code class="language-plaintext highlighter-rouge">reify</code> block is the code that will be generated. You can refer to another <code class="language-plaintext highlighter-rouge">Expr</code> (in the old tree) by using its <code class="language-plaintext highlighter-rouge">.splice</code> method. Here is an example to print out both the variable name and it’s value:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">macroImpl</span><span class="o">(</span><span class="n">c</span><span class="k">:</span> <span class="kt">blackbox.Context</span><span class="o">)(</span><span class="n">s</span><span class="k">:</span> <span class="kt">c.Expr</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">c.Expr</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="k">import</span> <span class="nn">c.universe._</span>
  <span class="k">val</span> <span class="nv">name</span> <span class="k">=</span> <span class="nv">c</span><span class="o">.</span><span class="py">Expr</span><span class="o">(</span><span class="nv">c</span><span class="o">.</span><span class="py">parse</span><span class="o">(</span><span class="s">"\""</span> <span class="o">+</span> <span class="nv">s</span><span class="o">.</span><span class="py">tree</span><span class="o">.</span><span class="py">symbol</span><span class="o">.</span><span class="py">fullName</span> <span class="o">+</span> <span class="s">"\""</span><span class="o">))</span>
  <span class="n">reify</span> <span class="o">{</span>
    <span class="n">s</span><span class="s">"${name.splice}: ${s.splice}"</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">macroTest</code> and the test code is the same above. Running the test code will get output like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>me.binwang.archmage.coretest.MethodMetaTest.a: abc
</code></pre></div></div>

<h3 id="54-use-quasiquotes">5.4 Use Quasiquotes</h3>

<p>As we’ve seen in section 4.5, <code class="language-plaintext highlighter-rouge">q"..."</code> can be used to match a tree. It can be used to generate a tree as well. For example, in the following code:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">macroImpl</span><span class="o">(</span><span class="n">c</span><span class="k">:</span> <span class="kt">blackbox.Context</span><span class="o">)(</span><span class="n">s</span><span class="k">:</span> <span class="kt">c.Tree</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
  <span class="k">import</span> <span class="nn">c.universe._</span>
  <span class="k">val</span> <span class="nv">q</span><span class="s">"if ($cond) $thenp else $elsep"</span> <span class="k">=</span> <span class="n">s</span>
  <span class="n">q</span><span class="s">"if ($cond) $elsep else $thenp"</span>
<span class="o">}</span>

<span class="k">def</span> <span class="nf">macroTest</span><span class="o">[</span><span class="kt">T</span><span class="o">](</span><span class="n">s</span><span class="k">:</span> <span class="kt">T</span><span class="o">)</span><span class="k">:</span> <span class="kt">T</span> <span class="o">=</span> <span class="n">macro</span> <span class="n">macroImpl</span>
</code></pre></div></div>

<p>It uses the parts that have been matched by <code class="language-plaintext highlighter-rouge">q"..."</code> and generates a new tree using those parts. It swaps the if and else clause. Run it with this test code:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">macroTest</span><span class="o">(</span><span class="nf">if</span> <span class="o">(</span><span class="kc">true</span><span class="o">)</span> <span class="nf">println</span><span class="o">(</span><span class="s">"a"</span><span class="o">)</span> <span class="k">else</span> <span class="nf">println</span><span class="o">(</span><span class="s">"b"</span><span class="o">))</span>
</code></pre></div></div>

<p>It will print <code class="language-plaintext highlighter-rouge">b</code> instead of <code class="language-plaintext highlighter-rouge">a</code>.</p>

<h3 id="55-avoid-name-conflict">5.5 Avoid Name Conflict</h3>

<p>When generating a new tree, we may generate some variables that have conflict names with the existing ones. Use <code class="language-plaintext highlighter-rouge">c.freshName</code> to get a unique name to avoid the conflict.</p>

<h3 id="56-type-checked-and-unchecked-tree">5.6 Type Checked and Unchecked Tree</h3>

<p>There are two kinds of AST in Scala’s internal compiler: type checked and unchecked. See more details in <a href="https://stackoverflow.com/questions/20936509/scala-macros-what-is-the-difference-between-typed-aka-typechecked-and-untyped">this Stack Overflow answer</a>. Some APIs can only accept either type checked or unchecked tree. And sometimes the compiler throws out weird errors if using the wrong type of tree. If that’s the case, try to use <code class="language-plaintext highlighter-rouge">c.untypecheck</code> and <code class="language-plaintext highlighter-rouge">c.typecheck</code> to covert trees.</p>

<p>For example, here is some code that cannot be compiled:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">macroImpl</span><span class="o">(</span><span class="n">c</span><span class="k">:</span> <span class="kt">blackbox.Context</span><span class="o">)(</span><span class="n">blockTree</span><span class="k">:</span> <span class="kt">c.Tree</span><span class="o">)</span> <span class="k">:</span> <span class="kt">c.Expr</span><span class="o">[</span><span class="kt">Seq</span><span class="o">[</span><span class="kt">String</span><span class="o">]]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="k">import</span> <span class="nn">c.universe._</span>
  <span class="k">val</span> <span class="nv">block</span> <span class="k">=</span> <span class="nv">c</span><span class="o">.</span><span class="py">Expr</span><span class="o">[</span><span class="kt">Seq</span><span class="o">[</span><span class="kt">String</span><span class="o">]](</span><span class="n">blockTree</span><span class="o">)</span>
  <span class="n">reify</span> <span class="o">{</span>
    <span class="nc">Seq</span><span class="o">(</span><span class="s">"a"</span><span class="o">).</span><span class="py">flatMap</span><span class="o">{</span><span class="k">_</span> <span class="k">=&gt;</span> <span class="nv">block</span><span class="o">.</span><span class="py">splice</span><span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>

<span class="k">def</span> <span class="nf">macroTest</span><span class="o">(</span><span class="n">blockTree</span><span class="k">:</span> <span class="o">=&gt;</span> <span class="nc">Seq</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="n">macro</span> <span class="n">macroImpl</span>

<span class="c1">// Testing code in another sub project:</span>
<span class="k">val</span> <span class="nv">s</span> <span class="k">=</span> <span class="s">"abc"</span>
<span class="n">macroTest</span> <span class="o">{</span>
  <span class="k">val</span> <span class="nv">a</span> <span class="k">=</span> <span class="n">s</span>
  <span class="nc">Seq</span><span class="o">(</span><span class="n">a</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div></div>

<p>The compiler will throw error:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[error] Error while emitting XXX.scala
[error] value a
[error] one error found
</code></pre></div></div>

<p>To fix this, we need to convert <code class="language-plaintext highlighter-rouge">blockTree</code> to unchecked tree:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">macroImpl</span><span class="o">(</span><span class="n">c</span><span class="k">:</span> <span class="kt">blackbox.Context</span><span class="o">)(</span><span class="n">blockTree</span><span class="k">:</span> <span class="kt">c.Tree</span><span class="o">)</span> <span class="k">:</span> <span class="kt">c.Expr</span><span class="o">[</span><span class="kt">Seq</span><span class="o">[</span><span class="kt">String</span><span class="o">]]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="k">import</span> <span class="nn">c.universe._</span>
  <span class="k">val</span> <span class="nv">cleanedBlock</span> <span class="k">=</span> <span class="nv">c</span><span class="o">.</span><span class="py">untypecheck</span><span class="o">(</span><span class="nv">blockTree</span><span class="o">.</span><span class="py">duplicate</span><span class="o">)</span>
  <span class="k">val</span> <span class="nv">block</span> <span class="k">=</span> <span class="nv">c</span><span class="o">.</span><span class="py">Expr</span><span class="o">[</span><span class="kt">Seq</span><span class="o">[</span><span class="kt">String</span><span class="o">]](</span><span class="n">cleanedBlock</span><span class="o">)</span>
  <span class="n">reify</span> <span class="o">{</span>
    <span class="nc">Seq</span><span class="o">(</span><span class="s">"a"</span><span class="o">).</span><span class="py">flatMap</span><span class="o">{</span><span class="k">_</span> <span class="k">=&gt;</span> <span class="nv">block</span><span class="o">.</span><span class="py">splice</span><span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="Scala" /><category term="macro" /><category term="meta programming" /><category term="AOP" /><summary type="html"><![CDATA[Macros are powerful but complex. Especially when the language itself like Scala is already complex. The lack of learning resource and documents makes it more so. In this article, I’ll write down some of my learnings and hopefully it can help someone else who is new to it as well. I’ll keep the examples small and simple so it’s easier to understand. Since I’m still learning it, I may continue to update this article on the way, or write a new article if there is a big topic. Either way, I’ll make notes here so you know there are updates.]]></summary></entry><entry><title type="html">ZFS Profiling on Arch Linux</title><link href="https://www.binwang.me/2023-12-14-ZFS-Profiling-on-Arch-Linux.html" rel="alternate" type="text/html" title="ZFS Profiling on Arch Linux" /><published>2023-12-14T00:00:00-05:00</published><updated>2023-12-14T00:00:00-05:00</updated><id>https://www.binwang.me/ZFS-Profiling-on-Arch-Linux</id><content type="html" xml:base="https://www.binwang.me/2023-12-14-ZFS-Profiling-on-Arch-Linux.html"><![CDATA[<p>I bought a new video game recently but found <code class="language-plaintext highlighter-rouge">z_rd_int</code> processes took almost all the CPU time when I was playing it. That doesn’t make much sense to me since I install games on a non compressed ZFS dataset. Even though I don’t have a powerful CPU, I don’t expect ZFS to use all of them and only reads about 60-70MiB/s from each of the NVME SSDs. To double check, I used <code class="language-plaintext highlighter-rouge">iostat -x 1</code> to confirm the iowait is very low. So disk IO is not the bottleneck.</p>

<p>Without finding any root cause from Internet, I decide to do some profiling by myself. From OpenZFS’ Github issues, people are using <a href="https://perf.wiki.kernel.org/index.php/Main_Page">perf</a> to do profiling. It is trivial enough to do it from a glance. But let <code class="language-plaintext highlighter-rouge">perf</code> showing debug symbols for ZFS spent me a lot of time. So in this article, I will document the steps to enable debug symbols for ZFS and hopefully it can help more people that facing difficulties to do it. After that, I will continue with how do I find the root cause and the solution. If you’ve seen my previous blog <a href="/2023-09-30-A-Boring-JVM-Memory-Profiling-Story.html">A Boring JVM Memory Profiling Story</a>, this is an even more boring profiling story. But the tool set is important. Use them efficiently and hopefully all the profiling stories become boring.</p>

<h2 id="1-enable-debug-info-for-zfs">1. Enable Debug Info for ZFS</h2>

<p>On Arch Linux, if you run <code class="language-plaintext highlighter-rouge">perf top</code>, you can see kernel has debug symbols attached like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2.95%  [kernel]                                        [k] entry_SYSCALL_64
</code></pre></div></div>

<p>But for some other processes like zfs ones, it only has an address like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2.65%  [zfs]                                           [k] 0x00000000002990cf
</code></pre></div></div>

<p>This is because perf cannot find debug info for zfs module. Let’s enable it now.</p>

<h3 id="11-use-dkms-package">1.1 Use DKMS Package</h3>

<p>First we need to use <a href="https://wiki.archlinux.org/title/Dynamic_Kernel_Module_Support">DKMS</a> package instead a pre compiled one so that we can control the compiling behaviour when build the zfs kernel module. In Arch Linux, the package name is <code class="language-plaintext highlighter-rouge">zfs-dkms</code> either in AUR or <a href="https://github.com/archzfs/archzfs">archzfs</a> repo. Be aware packages are different from those different repos even they have the same name. Personally I like archzfs repo more since it’s more well maintained and has better dependency management.</p>

<h3 id="12-enable-debuginfo-flags">1.2 Enable debuginfo Flags</h3>

<h4 id="tldr">TL;DR:</h4>

<p>Add these three lines to <code class="language-plaintext highlighter-rouge">/etc/sysconfig/zfs</code>, (re)install the zfs dkms package and reboot.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">ZFS_DKMS_ENABLE_DEBUG</span><span class="o">=</span>y
<span class="nv">ZFS_DKMS_ENABLE_DEBUGINFO</span><span class="o">=</span>y
<span class="nv">ZFS_DKMS_DISABLE_STRIP</span><span class="o">=</span>y
</code></pre></div></div>

<p>Decompress the installed ko file.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>unzstd /lib/modules/&lt;your kernel version&gt;/updates/dkms/zfs.ko.zst
</code></pre></div></div>

<p>Now you should be able to see zfs symbols in <code class="language-plaintext highlighter-rouge">perf top</code>.</p>

<p>Remember to cleanup the files after profiling.</p>

<p>If you care about the reason behind these changes, continue reading. Otherwise you can skip the remaining of this section.</p>

<h4 id="what-is-etcsysconfigzfs">What is <code class="language-plaintext highlighter-rouge">/etc/sysconfig/zfs</code>?</h4>

<p>The package <code class="language-plaintext highlighter-rouge">zfs-dkms</code> only installs the code that will be compiled by dkms to <code class="language-plaintext highlighter-rouge">/usr/src/zfs-&lt;zfs-version&gt;</code>. (I learned this by reading <code class="language-plaintext highlighter-rouge">PKGBUILD</code> of the aur package). Then when <code class="language-plaintext highlighter-rouge">dkms</code> commands are run, <code class="language-plaintext highlighter-rouge">dkms</code> copies the files to <code class="language-plaintext highlighter-rouge">/var/lib/dkms/zfs/&lt;zfs-version&gt;/build</code> to build it and then install the built ko files to <code class="language-plaintext highlighter-rouge">/lib/modules/&lt;your kernel version&gt;/updates/dkms</code>. So in order to build zfs module with debug symbols, we need to let dkms uses correct compile flags.</p>

<p>Under <code class="language-plaintext highlighter-rouge">/usr/src/zfs-&lt;zfs-version&gt;</code>, there is <code class="language-plaintext highlighter-rouge">dkms.conf</code> that tells DKMS how to use the source code to build and install modules. We can find some key information there:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">PRE_BUILD</span><span class="o">=</span><span class="s2">"configure
  --prefix=/usr
  --with-config=kernel
  --with-linux=</span><span class="se">\$</span><span class="s2">(
    if [ -e "</span><span class="se">\$</span><span class="o">{</span>kernel_source_dir/%build/source<span class="o">}</span><span class="s2">" ]
    then
      echo "</span><span class="se">\$</span><span class="o">{</span>kernel_source_dir/%build/source<span class="o">}</span><span class="s2">"
    else
      echo "</span><span class="se">\$</span><span class="o">{</span>kernel_source_dir<span class="o">}</span><span class="s2">"
    fi
  )
  --with-linux-obj="</span><span class="se">\$</span><span class="o">{</span>kernel_source_dir<span class="o">}</span><span class="s2">"
  </span><span class="se">\$</span><span class="s2">(
    [[ -n </span><span class="se">\"\$</span><span class="s2">{ICP_ROOT}</span><span class="se">\"</span><span class="s2"> ]] &amp;&amp; </span><span class="se">\\</span><span class="s2">
    {
      echo --with-qat=</span><span class="se">\"\$</span><span class="s2">{ICP_ROOT}</span><span class="se">\"</span><span class="s2">
    }
  )
  </span><span class="se">\$</span><span class="s2">(
    [[ -r </span><span class="se">\$</span><span class="s2">{PACKAGE_CONFIG} ]] </span><span class="se">\\</span><span class="s2">
    &amp;&amp; source </span><span class="se">\$</span><span class="s2">{PACKAGE_CONFIG} </span><span class="se">\\</span><span class="s2">
    &amp;&amp; shopt -q -s extglob </span><span class="se">\\</span><span class="s2">
    &amp;&amp; </span><span class="se">\\</span><span class="s2">
    {
      if [[ </span><span class="se">\$</span><span class="s2">{ZFS_DKMS_ENABLE_DEBUG,,} == @(y|yes) ]]
      then
        echo --enable-debug
      fi
      if [[ </span><span class="se">\$</span><span class="s2">{ZFS_DKMS_ENABLE_DEBUGINFO,,} == @(y|yes) ]]
      then
        echo --enable-debuginfo
      fi
    }
  )
"</span>
</code></pre></div></div>

<p>There is <code class="language-plaintext highlighter-rouge">--enable-debug</code> and <code class="language-plaintext highlighter-rouge">--enable-debuginfo</code>. Run <code class="language-plaintext highlighter-rouge">./configure --help</code> shows the meaning of these two flags:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  --enable-debug          Enable compiler and code assertions [default=no]
  --enable-debuginfo      Force generation of debuginfo [default=no]
</code></pre></div></div>

<p>So if those two flags are enabled, the zfs module should be built with debug info. The code above checks <code class="language-plaintext highlighter-rouge">ZFS_DKMS_ENABLE_DEBUG</code> and <code class="language-plaintext highlighter-rouge">ZFS_DKMS_ENABLE_DEBUGINFO</code> in file <code class="language-plaintext highlighter-rouge">${PACKAGE_CONFIG}</code>. If they are <code class="language-plaintext highlighter-rouge">y</code> or <code class="language-plaintext highlighter-rouge">yes</code>, the corresponding flags are enabled. At the beginning of <code class="language-plaintext highlighter-rouge">dkms.conf</code> we can find <code class="language-plaintext highlighter-rouge">PACKAGE_CONFIG</code> is defined as <code class="language-plaintext highlighter-rouge">/etc/sysconfig/zfs</code>.</p>

<p>However, only defining <code class="language-plaintext highlighter-rouge">ZFS_DKMS_ENABLE_DEBUG</code> and <code class="language-plaintext highlighter-rouge">ZFS_DKMS_ENABLE_DEBUGINFO</code> is not enough. I learnt it the hard way. Checking <code class="language-plaintext highlighter-rouge">dkms.conf</code> more closely, we can see these code below:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>STRIP[0]<span class="o">=</span><span class="s2">"</span><span class="se">\$</span><span class="s2">(
  [[ -r </span><span class="se">\$</span><span class="s2">{PACKAGE_CONFIG} ]] </span><span class="se">\\</span><span class="s2">
  &amp;&amp; source </span><span class="se">\$</span><span class="s2">{PACKAGE_CONFIG} </span><span class="se">\\</span><span class="s2">
  &amp;&amp; shopt -q -s extglob </span><span class="se">\\</span><span class="s2">
  &amp;&amp; [[ </span><span class="se">\$</span><span class="s2">{ZFS_DKMS_DISABLE_STRIP,,} == @(y|yes) ]] </span><span class="se">\\</span><span class="s2">
  &amp;&amp; echo -n no
)"</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">man dkms</code> shows the meaning of <code class="language-plaintext highlighter-rouge">STRIP</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>STRIP[#]=
       By default strip is considered to be "yes". If set to  "no",  DKMS  will
       not  run strip -g against your built module to remove debug symbols from
       it.  STRIP[0] is used as the default for any unset entries in the  STRIP
       array.
</code></pre></div></div>

<p>If <code class="language-plaintext highlighter-rouge">STRIP</code> is not set to <code class="language-plaintext highlighter-rouge">no</code>, <code class="language-plaintext highlighter-rouge">dkms</code> will stripe the debug info! So we also need to set <code class="language-plaintext highlighter-rouge">ZFS_DKMS_DISABLE_STRIP</code> in <code class="language-plaintext highlighter-rouge">/etc/sysconfig/zfs</code> to <code class="language-plaintext highlighter-rouge">y</code> or <code class="language-plaintext highlighter-rouge">yes</code> so that <code class="language-plaintext highlighter-rouge">STRIP[0]</code> will be <code class="language-plaintext highlighter-rouge">no</code>.</p>

<h4 id="why-unzstd">Why unzstd?</h4>

<p>In my system, the dkms modules are compressed with zstd when installing. But it seems <code class="language-plaintext highlighter-rouge">perf</code> is not able to read the compressed module file in order to find the debug symbols, so we need to uncompress it at the same location.</p>

<h2 id="2-profiling-zfs">2. Profiling ZFS</h2>

<p><code class="language-plaintext highlighter-rouge">perf top</code> can show the CPU usage for each function in real time. But in order to analysis it better, we can record it with <code class="language-plaintext highlighter-rouge">perf record -g -p &lt;pid&gt;</code>. It should generate <code class="language-plaintext highlighter-rouge">perf.data</code> file in the current directory. Press <code class="language-plaintext highlighter-rouge">Ctrl + C</code> to stop the recording and flush the file.</p>

<p>Then use <code class="language-plaintext highlighter-rouge">sudo perf report</code> to show the report of the recording. Mine is like this (press <code class="language-plaintext highlighter-rouge">+</code> to extend a row of interest in <code class="language-plaintext highlighter-rouge">perf report</code>):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Samples: 277K of event 'cycles:P', Event count (approx.): 244633155596
Children      Self  Command   Shared Object     Symbol
+   96.59%     0.01%  z_rd_int  [zfs]             [k] zio_do_crypt_uio
+   96.58%     0.00%  z_rd_int  [zfs]             [k] crypto_decrypt
+   96.57%     0.01%  z_rd_int  [zfs]             [k] aes_decrypt_atomic
+   75.53%     8.17%  z_rd_int  [zfs]             [k] aes_encrypt_block
+   49.76%     0.00%  z_rd_int  [zfs]             [k] crypto_update_uio
+   49.76%     0.00%  z_rd_int  [zfs]             [k] aes_decrypt_contiguous_blocks
+   49.76%     4.52%  z_rd_int  [zfs]             [k] ccm_mode_decrypt_contiguous_blocks
+   46.42%     2.08%  z_rd_int  [zfs]             [k] ccm_decrypt_final
+   42.15%     6.94%  z_rd_int  [zfs]             [k] aes_aesni_encrypt
-   24.72%    24.36%  z_rd_int  [zfs]             [k] kfpu_end
     24.36% ret_from_fork_asm
        ret_from_fork
        kthread
        0xffffffffc02b15eb
        zio_execute
        zio_done
        zio_pop_transforms
        zio_decrypt
        spa_do_crypt_abd
        zio_do_crypt_data
        zio_do_crypt_uio
        crypto_decrypt
      + aes_decrypt_atomic
-   21.20%    20.96%  z_rd_int  [zfs]             [k] kfpu_begin
     20.96% ret_from_fork_asm
        ret_from_fork
        kthread
        0xffffffffc02b15eb
        zio_execute
        zio_done
        zio_pop_transforms
        zio_decrypt
        spa_do_crypt_abd
        zio_do_crypt_data
        zio_do_crypt_uio
        crypto_decrypt
      + aes_decrypt_atomic
+   14.42%    14.21%  z_rd_int  [zfs]             [k] aes_encrypt_intel
+    7.36%     7.14%  z_rd_int  [zfs]             [k] aes_xor_block
+    6.31%     6.16%  z_rd_int  [zfs]             [k] aes_copy_block
+    1.27%     0.03%  z_rd_int  [zfs]             [k] arc_read_done
+    1.17%     0.02%  z_rd_int  [zfs]             [k] zio_vdev_io_done
+    1.14%     0.00%  z_rd_int  [zfs]             [k] abd_iterate_func
</code></pre></div></div>

<h2 id="3-find-root-cause">3. Find Root Cause</h2>

<p>From the profiling report, we can easily see that the CPU is mostly used on decrypting the content on ZFS. That makes some sense because decryption do need CPU power. But there is no reason it uses so much CPU at that throughput. In fact found some performance issues related encryption and did something to rule out some causes:</p>

<ol>
  <li>I made sure the AES hardware acceleration is enabled for my CPU by checking <code class="language-plaintext highlighter-rouge">lscpu | grep aes</code>.</li>
  <li>My system can decrypt and encrypt at a much higher speed (2000+ MB/s) by running <code class="language-plaintext highlighter-rouge">cryptsetup benchmark</code>.</li>
</ol>

<p>That’s why I need the profiling to confirm where the bottleneck comes from.</p>

<p>Even though the code path is related to decryption, the hotspot is at <code class="language-plaintext highlighter-rouge">kfpu_begin</code> and <code class="language-plaintext highlighter-rouge">kfpu_end</code>. I read the code and have totally no idea what they are doing. I asked ChatGPT and it explains to me that it’s saving and restoring FPU state. I don’t know if its answer is correct or not, but that at least gave me some direction to search issues. At last I found this Github issue <a href="https://github.com/openzfs/zfs/pull/9749">ICP: Improve AES-GCM performance</a>. It says exactly that there is performance issue with saving FPU state when doing encryption. And the PR improves it for AES-GCM algorithm. It states AES-CCM can benifit from similar fix but the performance improvement will not be as great. So in the discussion of the PR, they decide to change the default encryption algorithm to AES-GCM instead of AES-CCM.</p>

<p>I <a href="/2020-01-28-Migrate-Arch-Linux-to-Zfs.html">started use zfs</a> before this PR. So I checked the encryption algorithm on my system by <code class="language-plaintext highlighter-rouge">zfs get all &lt;dataset&gt; | grep encryption</code>. And it is indeed using AES-CCM. In order to confirm it is causing performance issue, I did some benchmark on AES-CCM, AES-GCM and not encrypted datasets.</p>

<p>First, created the datasets:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>zfs create <span class="nt">-o</span> <span class="nv">encryption</span><span class="o">=</span>aes-256-ccm <span class="nt">-o</span> <span class="nv">compression</span><span class="o">=</span>off <span class="nt">-o</span> <span class="nv">atime</span><span class="o">=</span>off zroot/root/ccm-test
<span class="nb">sudo </span>zfs create <span class="nt">-o</span> <span class="nv">encryption</span><span class="o">=</span>aes-256-gcm <span class="nt">-o</span> <span class="nv">compression</span><span class="o">=</span>off <span class="nt">-o</span> <span class="nv">atime</span><span class="o">=</span>off zroot/root/gcm-test
<span class="nb">sudo </span>zfs create <span class="nt">-o</span> <span class="nv">encryption</span><span class="o">=</span>off <span class="nt">-o</span> <span class="nv">compression</span><span class="o">=</span>off <span class="nt">-o</span> <span class="nv">atime</span><span class="o">=</span>off zroot/local_steam_unencrypt
</code></pre></div></div>

<p>Then I write a script to benchmark it:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="nb">set</span> <span class="nt">-e</span>

<span class="k">function </span>print_cputime<span class="o">()</span> <span class="o">{</span>
	<span class="nv">pname</span><span class="o">=</span><span class="nv">$1</span>
	<span class="k">for </span>pid <span class="k">in</span> <span class="sb">`</span>pgrep <span class="nv">$pname</span><span class="sb">`</span> <span class="p">;</span> <span class="k">do
		</span>ps <span class="nt">-p</span> <span class="nv">$pid</span> <span class="nt">-o</span> cputime,etime
	<span class="k">done</span>
<span class="o">}</span>


<span class="k">function </span>benchmark <span class="o">{</span>
	<span class="nv">test_name</span><span class="o">=</span><span class="nv">$1</span>
	<span class="nv">test_file</span><span class="o">=</span><span class="nv">$2</span>

	<span class="nv">file_size</span><span class="o">=</span><span class="s2">"20480"</span>

	<span class="nb">echo</span> <span class="s2">"### Start benchmark </span><span class="nv">$test_name</span><span class="s2">"</span>

	<span class="nb">echo</span> <span class="s2">"### Print z_wr_iss cpu time before the write test"</span>
	print_cputime z_wr_iss
	<span class="nb">echo</span> <span class="s2">"### Start write test"</span>
	<span class="nb">time dd </span><span class="k">if</span><span class="o">=</span>/dev/random <span class="nv">of</span><span class="o">=</span><span class="nv">$test_file</span> <span class="nv">bs</span><span class="o">=</span>1M <span class="nv">count</span><span class="o">=</span><span class="nv">$file_size</span> <span class="nv">oflag</span><span class="o">=</span>direct
	<span class="nb">echo</span> <span class="s2">"### Pring z_wr_iss cpu time afte the write test"</span>
	print_cputime z_wr_iss

	<span class="nb">echo</span> <span class="s2">"### Print z_rd_int cpu time before the read test"</span>
	print_cputime z_rd_int
	<span class="nb">echo</span> <span class="s2">"### Start read test"</span>
	<span class="nb">time dd </span><span class="k">if</span><span class="o">=</span><span class="nv">$test_file</span> <span class="nv">of</span><span class="o">=</span>/dev/null <span class="nv">bs</span><span class="o">=</span>1M <span class="nv">count</span><span class="o">=</span><span class="nv">$file_size</span>
	<span class="nb">echo</span> <span class="s2">"### Print z_rd_int cpu time before the read test"</span>
	print_cputime z_rd_int
<span class="o">}</span>

benchmark ccm-test /ccm-test/test-file
benchmark gcm-test /gcm-test/test-file
benchmark non-encrypt-test /data/local_steam/test-file
</code></pre></div></div>

<p>My ZFS cache is set to 8GB. So I write and read files with 20GB. It uses dd to write and read a file. Before the read and write, it uses <code class="language-plaintext highlighter-rouge">ps -o cputime,etime</code> to print out CPU time and wall time used by each related ZFS processes.</p>

<p>Running this script creates lots of output. The full output can be found in the appendix at the end. Here are the key lines:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>### Start benchmark ccm-test
// ... output omitted ...
21474836480 bytes (21 GB, 20 GiB) copied, 107.307 s, 200 MB/s
// ... output omitted ...
### Start benchmark gcm-test
// ... output omitted ...
21474836480 bytes (21 GB, 20 GiB) copied, 13.7417 s, 1.6 GB/s
// ... output omitted ...
### Start benchmark non-encrypt-test
// ... output omitted ...
21474836480 bytes (21 GB, 20 GiB) copied, 9.03496 s, 2.4 GB/s
// ... output omitted ...
</code></pre></div></div>

<p>During the test, AES-CCM makes <code class="language-plaintext highlighter-rouge">z_rd_int</code> takes all CPU time as observed before. For AES-GCM, it’s much better, <code class="language-plaintext highlighter-rouge">z_rd_int</code> takes less than 50% and for non encrypted it’s less than 20%. The testing output prints the CPU time and wall time for each of the <code class="language-plaintext highlighter-rouge">z_rd_int</code> processes before and after the test. So you can count the percentage.</p>

<p>From the test result, we can see AES-CCM indeed affect read performance a lot. It’s even slower than writes. We can confirm this is the root cause for our problem.</p>

<h2 id="4-solution-and-workaround">4. Solution and Workaround</h2>

<p>The solution is obvious: just change the encryption from AES-CCM to AES-GCM. But it cannot be done without migrating the dataset to another place and then move it back. It takes time. At the mean time, I moved my Steam library to a non encrypted dataset since I have enough disk space to do the migration. It doesn’t have sensitive information. Yes it exposes the machine to <a href="https://en.wikipedia.org/wiki/Evil_maid_attack">evil maid attack</a>, but my setup on the machine doesn’t prevent it anyway. See my previous blog <a href="/2021-09-19-Personal-ZFS-Offsite-Online-Backup-Solution.html">Personal ZFS Offsite Backup Solution</a> for more information on putting a machine into a not trusted environment.</p>

<p>I’ll do the migration from AES-CCM to AES-GCM in the future and report back how it works. Stay tuned!</p>

<h2 id="5-appendix">5. Appendix</h2>

<p>Here is the full output from the benchmark script:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>### Start benchmark ccm-test
### Print z_wr_iss cpu time before the write test
    TIME     ELAPSED
00:47:56  3-03:39:21
    TIME     ELAPSED
00:22:34  3-03:39:21
    TIME     ELAPSED
00:47:54  3-03:39:21
    TIME     ELAPSED
00:47:55  3-03:39:21
    TIME     ELAPSED
00:00:01  3-03:39:17
    TIME     ELAPSED
00:00:00  3-03:39:17
    TIME     ELAPSED
00:04:50    15:30:06
    TIME     ELAPSED
00:04:49    15:29:57
    TIME     ELAPSED
00:04:51    15:29:56
    TIME     ELAPSED
00:04:51    15:29:18
    TIME     ELAPSED
00:00:00    10:07:30
    TIME     ELAPSED
00:00:00       55:49
### Start write test
20480+0 records in
20480+0 records out
21474836480 bytes (21 GB, 20 GiB) copied, 91.4066 s, 235 MB/s

real	1m31.414s
user	0m0.059s
sys	0m53.252s
### Pring z_wr_iss cpu time afte the write test
    TIME     ELAPSED
00:49:23  3-03:40:53
    TIME     ELAPSED
00:22:34  3-03:40:53
    TIME     ELAPSED
00:49:21  3-03:40:53
    TIME     ELAPSED
00:49:22  3-03:40:53
    TIME     ELAPSED
00:00:01  3-03:40:49
    TIME     ELAPSED
00:00:00  3-03:40:49
    TIME     ELAPSED
00:04:50    15:31:38
    TIME     ELAPSED
00:04:50    15:31:28
    TIME     ELAPSED
00:04:51    15:31:28
    TIME     ELAPSED
00:04:51    15:30:50
    TIME     ELAPSED
00:00:00    10:09:01
    TIME     ELAPSED
00:00:00       57:21
### Print z_rd_int cpu time before the read test
    TIME     ELAPSED
00:24:46  3-03:40:53
    TIME     ELAPSED
00:00:02  3-03:40:49
    TIME     ELAPSED
00:01:50       06:47
    TIME     ELAPSED
00:01:49       06:47
### Start read test
20480+0 records in
20480+0 records out
21474836480 bytes (21 GB, 20 GiB) copied, 107.307 s, 200 MB/s

real	1m47.372s
user	0m0.060s
sys	0m8.091s
### Print z_rd_int cpu time after the read test
    TIME     ELAPSED
00:26:24  3-03:42:41
    TIME     ELAPSED
00:00:02  3-03:42:37
    TIME     ELAPSED
00:03:28       08:34
    TIME     ELAPSED
00:03:27       08:34
### Start benchmark gcm-test
### Print z_wr_iss cpu time before the write test
    TIME     ELAPSED
00:49:35  3-03:42:41
    TIME     ELAPSED
00:22:34  3-03:42:41
    TIME     ELAPSED
00:49:33  3-03:42:41
    TIME     ELAPSED
00:49:33  3-03:42:41
    TIME     ELAPSED
00:00:01  3-03:42:37
    TIME     ELAPSED
00:00:00  3-03:42:37
    TIME     ELAPSED
00:04:50    15:33:26
    TIME     ELAPSED
00:04:50    15:33:16
    TIME     ELAPSED
00:04:51    15:33:16
    TIME     ELAPSED
00:04:51    15:32:38
    TIME     ELAPSED
00:00:00    10:10:49
    TIME     ELAPSED
00:00:00       59:08
### Start write test
20480+0 records in
20480+0 records out
21474836480 bytes (21 GB, 20 GiB) copied, 56.9529 s, 377 MB/s

real	0m56.960s
user	0m0.045s
sys	0m53.566s
### Pring z_wr_iss cpu time afte the write test
    TIME     ELAPSED
00:49:42  3-03:43:38
    TIME     ELAPSED
00:22:35  3-03:43:38
    TIME     ELAPSED
00:49:39  3-03:43:38
    TIME     ELAPSED
00:49:39  3-03:43:38
    TIME     ELAPSED
00:00:01  3-03:43:34
    TIME     ELAPSED
00:00:00  3-03:43:34
    TIME     ELAPSED
00:04:51    15:34:23
    TIME     ELAPSED
00:04:50    15:34:14
    TIME     ELAPSED
00:04:52    15:34:13
    TIME     ELAPSED
00:04:52    15:33:35
    TIME     ELAPSED
00:00:00    10:11:46
    TIME     ELAPSED
00:00:00    01:00:06
### Print z_rd_int cpu time before the read test
    TIME     ELAPSED
00:26:24  3-03:43:38
    TIME     ELAPSED
00:00:02  3-03:43:34
    TIME     ELAPSED
00:00:00       00:05
    TIME     ELAPSED
00:00:00       00:05
### Start read test
20480+0 records in
20480+0 records out
21474836480 bytes (21 GB, 20 GiB) copied, 13.7417 s, 1.6 GB/s

real	0m13.743s
user	0m0.071s
sys	0m11.215s
### Print z_rd_int cpu time after the read test
    TIME     ELAPSED
00:26:31  3-03:43:52
    TIME     ELAPSED
00:00:02  3-03:43:48
    TIME     ELAPSED
00:00:07       00:19
    TIME     ELAPSED
00:00:07       00:19
### Start benchmark non-encrypt-test
### Print z_wr_iss cpu time before the write test
    TIME     ELAPSED
00:49:42  3-03:43:52
    TIME     ELAPSED
00:22:35  3-03:43:52
    TIME     ELAPSED
00:49:40  3-03:43:52
    TIME     ELAPSED
00:49:39  3-03:43:52
    TIME     ELAPSED
00:00:01  3-03:43:48
    TIME     ELAPSED
00:00:00  3-03:43:48
    TIME     ELAPSED
00:04:51    15:34:37
    TIME     ELAPSED
00:04:50    15:34:28
    TIME     ELAPSED
00:04:52    15:34:28
    TIME     ELAPSED
00:04:52    15:33:49
    TIME     ELAPSED
00:00:00    10:12:01
    TIME     ELAPSED
00:00:00    01:00:20
### Start write test
20480+0 records in
20480+0 records out
21474836480 bytes (21 GB, 20 GiB) copied, 56.0508 s, 383 MB/s

real	0m56.052s
user	0m0.042s
sys	0m53.060s
### Pring z_wr_iss cpu time afte the write test
    TIME     ELAPSED
00:49:46  3-03:44:49
    TIME     ELAPSED
00:22:35  3-03:44:49
    TIME     ELAPSED
00:49:44  3-03:44:49
    TIME     ELAPSED
00:49:43  3-03:44:49
    TIME     ELAPSED
00:00:01  3-03:44:44
    TIME     ELAPSED
00:00:00  3-03:44:44
    TIME     ELAPSED
00:04:51    15:35:33
    TIME     ELAPSED
00:04:50    15:35:24
    TIME     ELAPSED
00:04:52    15:35:24
    TIME     ELAPSED
00:04:52    15:34:46
    TIME     ELAPSED
00:00:00    10:12:57
    TIME     ELAPSED
00:00:00    01:01:16
### Print z_rd_int cpu time before the read test
    TIME     ELAPSED
00:26:31  3-03:44:49
    TIME     ELAPSED
00:00:02  3-03:44:45
    TIME     ELAPSED
00:00:07       01:16
    TIME     ELAPSED
00:00:07       01:16
### Start read test
20480+0 records in
20480+0 records out
21474836480 bytes (21 GB, 20 GiB) copied, 9.03496 s, 2.4 GB/s

real	0m9.036s
user	0m0.032s
sys	0m8.207s
### Print z_rd_int cpu time after the read test
    TIME     ELAPSED
00:26:33  3-03:44:58
    TIME     ELAPSED
00:00:02  3-03:44:54
    TIME     ELAPSED
00:00:09       01:25
    TIME     ELAPSED
00:00:09       01:25
</code></pre></div></div>]]></content><author><name></name></author><category term="ZFS" /><category term="Linux" /><category term="Profiling" /><category term="dkms" /><category term="kernel" /><summary type="html"><![CDATA[I bought a new video game recently but found z_rd_int processes took almost all the CPU time when I was playing it. That doesn’t make much sense to me since I install games on a non compressed ZFS dataset. Even though I don’t have a powerful CPU, I don’t expect ZFS to use all of them and only reads about 60-70MiB/s from each of the NVME SSDs. To double check, I used iostat -x 1 to confirm the iowait is very low. So disk IO is not the bottleneck.]]></summary></entry><entry><title type="html">Introduce K3s, CephFS and MetalLB to My High Avaliable Cluster</title><link href="https://www.binwang.me/2023-11-28-Introduce-K3s-CephFS-and-MetalLB-to-My-High-Avaliable-Cluster.html" rel="alternate" type="text/html" title="Introduce K3s, CephFS and MetalLB to My High Avaliable Cluster" /><published>2023-11-28T00:00:00-05:00</published><updated>2023-11-28T00:00:00-05:00</updated><id>https://www.binwang.me/Introduce-K3s-CephFS-and-MetalLB-to-My-High-Avaliable-Cluster</id><content type="html" xml:base="https://www.binwang.me/2023-11-28-Introduce-K3s-CephFS-and-MetalLB-to-My-High-Avaliable-Cluster.html"><![CDATA[<p>In a previous blog <a href="/2023-03-13-Infrastructure-Setup-for-High-Availability.html">Infrastructure Setup for High Availability</a>, I talked about how I setup a cluster infrastructure for high availability applications. I have made a few changes since then. This blog is to talk about them in details.</p>

<h2 id="updated-architecture-overview">Updated Architecture Overview</h2>

<p><img src="/static/images/2023-11-28-Introduce-K3s-CephFS-and-MetalLB-to-My-High-Avaliable-Cluster/ha-cluster-infrastructure-k3s.png" alt="arch-diagram" /></p>

<p>Comparing the diagram with the one in <a href="/2023-03-13-Infrastructure-Setup-for-High-Availability.html">Infrastructure Setup for High Availability</a>, the overall structure remains the same, with a few modifications:</p>

<ul>
  <li>Not shown in the graph, but replaced official Kubernetes with K3s.</li>
  <li>Replaced GlusterFS with CephFS.</li>
  <li>Included cert-manager to get SSL certificates.</li>
  <li>Replaced Keepalived on each node with MetalLB.</li>
</ul>

<h2 id="replace-kubernetes-with-k3s">Replace Kubernetes with K3s</h2>

<p>I didn’t know <a href="https://k3s.io/">K3s</a> back when I setup my Kubernetes cluster for the first time. But since then I heard a lot of good things about it at various places. However, the complexity of migration and its installation method through a script from Internet instead of an OS package made me think twice before adopt it. But after I watched the video <a href="https://www.youtube.com/watch?v=k58WnbKmjdA">Talk About K3s Internals from Darren Shepherd</a>, I realized how simple k3s is compared to Kubernetes. I highly recommend everyone who is interested in K3s watch this video.</p>

<p>In short, K3s is a distribution of Kubernetes instead of a fork. It does these things with a few patches: combined the components of Kubernetes into one binary and process, and removed some components not needed in a bare metal environment. By doing so, it makes its binary size and memory footprint smaller than Kubernetes, and makes it easier to deploy and manage. It only needs a binary <code class="language-plaintext highlighter-rouge">k3s</code> and a configuration file under <code class="language-plaintext highlighter-rouge">/etc/rancher/k3s/config.yaml</code> to start, and all of its content is under <code class="language-plaintext highlighter-rouge">/var/lib/rancher/k3s</code>. The official install script adds a little bit more than just the binary file: it has a few scripts to kill and uninstall k3s. It also includes systemd file to start/stop k3s through systemd. So even though it’s not packaged into a standard OS package, I think the complexity is manageable so I started to experiment with it.</p>

<p>It’s very easy to config K3s since all it needs is a configuration file on each machine. I created a virtual machine cluster with Vagrant in the project <a href="https://github.com/wb14123/k3s-vm-cluster">k3s-vm-cluster</a> to experiment with it. Feel free to play with it to get a feel with it before go all in. The setup is based on the official guide for <a href="https://docs.k3s.io/datastore/ha-embedded">High Availability Embedded etcd</a>. It’s the easiest way to setup a high available K3s cluster.</p>

<p>No load balancer setup is needed if no external Kubernetes API server HA is needed. That means, you can access to Kubernetes API server within the cluster if any of the machine fails. But if you still want to access it outside of the cluster during a failure, check <a href="https://docs.k3s.io/datastore/cluster-loadbalancer">this doc</a>. Alternatively, I think load balancer like MentalLB can also do it, but I don’t need it so I didn’t experiment with it.</p>

<h2 id="distributed-storage-system-glusterfs-to-cephfs">Distributed Storage System: GlusterFS to CephFS</h2>

<p>The biggest motivation drives this migration is the deprecation of GlusterFS. I’m using distributed file system for a few use cases:</p>

<ul>
  <li>Configuration files: this can be migrated to <a href="https://kubernetes.io/docs/concepts/configuration/configmap/">Kubernetes ConfigMaps</a>.</li>
  <li>Logs: this can be migrated to a centralized log management system like ElasticSearch. But some of them like <a href="https://grafana.com/oss/loki/">Loki</a> in turn depends on another distributed storage.</li>
  <li>Data files: this is most complex one. Some of the services support saving files into S3 compatible systems. But some of them don’t. (I cannot control the services since I only self host them instead of developed them). One option is to not having HA and just bind those services into a specific host and use local storage.</li>
  <li>Docker registry: this belongs to the point “Data files” above, but this is very import so I separate into another point. I’m using <a href="https://www.sonatype.com/products/sonatype-nexus-repository">Sonatype Nexus</a> as the docker registry. It supports to put packages into S3 but still pretty tricky to get rid of all the local files. This is a service that absolutely needs HA if I want to have a HA cluster. Or I can change to another Docker registry implementation, but I feel pretty comfortable using it so I don’t want to change it.</li>
</ul>

<p>So it basically comes down to these 2 options:</p>

<ol>
  <li>Use a S3 compatible storage like <a href="https://min.io/">MinIO</a> but do a lot of work to configure services to store files into that, and make services cannot do that not HA anymore.</li>
  <li>Go ahead and uses a real distributed file system like CephFs or <a href="https://longhorn.io/">Longhorn</a>.</li>
</ol>

<p><em>Update: I also explored <a href="https://linbit.com/">LINBIT</a> which I forgot to write it here. It got more and more complex when I went into the rabbit hole. But its architecture looks very interesting to me. So I may explore it more in the future for other use cases.</em></p>

<p>Option 1 sounds appealing to me at first since I really don’t want to deal with the complexity of setting up CephFS. But as I go into the rabbit hole, I found configuring the services to use S3 may be a more complex process and less portable than just setup CephFS. So at the end I decide to go option 2.</p>

<p>I’ve heard of CephFS long time ago but decided to use GlusterFS at previous setups because of the level of user friendly. So CephFS seems like a nature choice after GlusterFs is deprecated. Especially when I found other than the distributed block device, it also supports file system and S3 compatible storage system. It’s also easier to install than before because of <a href="https://rook.io/">Rook</a>. Longhorn is another choice I looked a little bit but because of wider adoption of CephFS and more features of it, I decide to use CephFS at the end.</p>

<p>The way I use it is mainly <a href="https://rook.io/docs/rook/v1.11/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage/">Ceph Filesystem</a>, so it’s easier to share volumes between pods. Again, the project <a href="https://github.com/wb14123/k3s-vm-cluster">k3s-vm-cluster</a> has an example about it. Try to play it if you are interested in it. Along the way I actually contributed to Rook project by improving doc (<a href="https://github.com/rook/rook/pull/13045">#13045</a>) and its error message (<a href="https://github.com/rook/rook/pull/13046">#13046</a>).</p>

<h2 id="network-gateway">Network Gateway</h2>

<p>In the previous article, I talked about using Cloudflare tunnel, or NodePort and Keepalived to expose services to the Internet. But there are some other things a network gateway can do other than just expose the service: it can also do things like terminate SSL encryption and so on. Cloudflare tunnel support terminate SSL at their end so I don’t need to worry about that. But for some services, I don’t want Cloudflare to see the traffic, so I need to terminate SSL and expose service by myself.</p>

<p>As I said, expose service part was done by NodePort and Keepalived, which is not very elegant but works. For the terminate SSL part, I was using Nginx as reverse proxy. But updating SSL certificates is a little bit more complex. I don’t want to talk it in details here because the setup is pretty complex and explaining it will be very lengthy. The point is, with this migration, I want to revisit this part to make it simpler and more elegant.</p>

<p>Kubernetes has a concept of <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress</a>, and newer but less mature, <a href="https://gateway-api.sigs.k8s.io/">Gateway</a>. What they are doing is essentially reverse proxy like Nginx. In fact, Nginx Ingress is a thing. The advantage is that you don’t need to configure all the services in a single place like Nginx’s configuration files. You can create Kubernetes resources for each of the service. So that the deployment and configuration of each service is totally self contained. This is a very good feature, especially for a company: when I first started to use Kubernetes at 2015 in a previous company, I felt the pain of not having it. But the feature of Ingress is pretty limited. For example, it can only bind to 443. It cannot modify the http content, and so on. So that I may still need a layer of Nginx for my use cases. The design of gateway is too complex and the features don’t really meet all my requirements as well.</p>

<p>There are some players like <a href="https://traefik.io/">Traefik</a>(shipped with K3s by default) and <a href="https://istio.io/">Istio</a> which overcome the limitations by having their own custom resources. But Traefik cannot get new certificates from Let’s Encrypt with a HA setup. Istio is just too complex and include features like service mesh that I don’t need. I can see how service mesh can be useful in big companies, but I prefer not to have another layer on my own service. At the end, I don’t think the complexity worth it.</p>

<p>But while I exploring Traefik and Istio, I found <a href="https://cert-manager.io/">cert-manager</a>, which can be deployed into Kubernetes. It can get certificates from Let’s Encrypt and put them into Kubernetes secrets, which then can be mount into each pods. It supports Cloudflare DNS API for <a href="https://letsencrypt.org/docs/challenge-types/#dns-01-challenge">ACME DNS challenge</a>, so I don’t need to export a http service for Let’s Encrypt to verify the ownership of the domain name. With all of this features, I decided to use it and mount the certificates into Nginx pods. It resolves the problem of update certificates from Let’s Encrypt.</p>

<p>For the other problem of exposing the services to Internet in a HA way, I want to use a more Kubernetes native way instead of setup Keepalived outside of the Kubernetes cluster. Kubernetes supports <a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/">external load balancers</a>. But most of the load balancers it supports are from cloud. Then I found <a href="https://metallb.org/">MetalLB</a>, which supports creating a HA load balancer without special hardware in a bare metal cluster. I use it with <a href="https://metallb.org/concepts/layer2/">layer 2 mode</a>, which creates a virtual IP like keepalived and can failover to another node.</p>

<h2 id="deploy-services-with-code">Deploy Services with Code</h2>

<p>What I didn’t talk in the previous blog is, I define the deployment of my services as code instead yaml files. It gives lots of advantages: first, you can create models for your own deployment pattern so that you can avoid lots of redundant code. Traditionally it’s hard to define the deployment as code. There are lots of frameworks to do it but none of them is easy to use. But with Kubernetes, all you need is generating a resource object for Kubernetes to use at the end. You can construct it in any way with your favorite language, and either output a YAML or call Kubernetes API directly. It’s using a high level language instead of writing machine code directly. It’s much more elegant and the maintenance is much easier. Be aware: use a real language instead of some template language. Why limit your power to do things?</p>

<p>This approach works so well especially during this migration. For example, I abstracted all the storage layer for my services, so that when I migrated from GlusterFS to CephFS, I just need to change the storage class to define the CephFS volume, and the code for services don’t need to change much.</p>

<p>Hope you enjoy my experience of setting up a HA cluster. Happy hacking and have fun with your own cluster!</p>]]></content><author><name></name></author><category term="Kubernetes" /><category term="CephFS" /><category term="MetalLB" /><category term="k3s" /><category term="Infrastructure" /><category term="devops" /><summary type="html"><![CDATA[In a previous blog Infrastructure Setup for High Availability, I talked about how I setup a cluster infrastructure for high availability applications. I have made a few changes since then. This blog is to talk about them in details.]]></summary></entry><entry><title type="html">Update on RSS Brain to Find Related Articles with Machine Learning</title><link href="https://www.binwang.me/2023-11-14-Update-On-RSS-Brain-to-Find-Related-Articles-with-Machine-Learning.html" rel="alternate" type="text/html" title="Update on RSS Brain to Find Related Articles with Machine Learning" /><published>2023-11-14T00:00:00-05:00</published><updated>2023-11-14T00:00:00-05:00</updated><id>https://www.binwang.me/Update-On-RSS-Brain-to-Find-Related-Articles-with-Machine-Learning</id><content type="html" xml:base="https://www.binwang.me/2023-11-14-Update-On-RSS-Brain-to-Find-Related-Articles-with-Machine-Learning.html"><![CDATA[<p>In the previous blog about RSS, <a href="/2022-12-03-How-RSS-Brain-Shows-Related-Articles.html">How RSS Brain Shows Related Articles</a>, I talked about how RSS Brain finds the related articles. I’ve updated the algorithm recently. This blog is about the details about the update. The basic idea is to replace tf-idf algorithm with text embeddings to represent the articles as vectors, and use ElastcSearch to store and query those vectors.</p>

<h2 id="the-disadvantages-of-previous-algorithm">The Disadvantages of Previous Algorithm</h2>

<p>First let’s do a quick revisit on the algorithm before the update: it’s using tf-idf algorithm. Which is basically an algorithm to represent each document as a vector by using the words’ frequency in it. It’s an algorithm that is easy to understand, and works well enough in practice to power lots of searching engines for a long time. However, it has a few shortcomings:</p>

<p>First, it doesn’t understand the meaning of the word. A word can mean different things based on context, order, combinations and so on. Different words can also have the same meaning. Word frequency along doesn’t catch that.</p>

<p>Second, “word” needs to be defined. Which is a relatively easy task for languages like English, since it has a built-in word separator character (space). However, for languages like Chinese, there is no obvious way to separate the words. The performance of tf-idf algorithm largely depends on the performance of word separating algorithm, which itself is much more complex than tf-idf and often involves machine learning as well. Even for languages like English, in order to minimize the first disadvantage above, the words are usually broke down so that some similar words can be matched.</p>

<p>Last, which is an extension of the first disadvantage: it’s hard to do multi language matches. Word frequency along doesn’t know that different words in different languages can mean the same thing. Of course you can translate the document to other languages and index the translated documents, but it doesn’t scale well when you need to support more and more languages. And translation algorithms are usually much more complex than tf-idf, and mostly use machine learning too.</p>

<h2 id="word-and-document-embeddings">Word and Document Embeddings</h2>

<p>With the advancement of machine learning, a new method to represent words as vectors has been developed in the paper <a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a>. The vector is called word embedding. Then based on the idea, <a href="https://arxiv.org/abs/1405.4053">Distributed Representations of Sentences and Documents</a> explores representing paragraphs as a vectors. Without go into the details, the basic idea is to get a layer from neural network for a NLP task.</p>

<p>For example, if we have a neural network to predict the nth word given previous words, then we may have a neural network like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>word[1]   --&gt; vector[1]
word[2]   --&gt; vector[2]    --&gt; layer2 --&gt; ... -&gt; classifier -&gt; output
...
word[n-1] --&gt; vector[n-1]
</code></pre></div></div>

<p>Words are mapped to vectors at the first layer, with something like</p>

\[v = w * W + b\]

<p>Which \(v\) is the vector, \(w\) is the one-hot encoded word. And matrix \(W\) and \(b\) is the trained parameters. There are many other parameters in the later layers of the neural network but we don’t care. We only take \(W\) and \(b\) so that we can compute the vector for any word. With this method, the represented vectors can measure similarities between words by computing similarity of the vectors. Also surprisingly, quoted from the paper <a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a>: “To find a word that is similar to small in the same sense as biggest is similar to big, we can simply compute vector \(X = vector(biggest) − vector(big) + vector(small)\).” What a beautiful result!</p>

<p>I was aware of this research not long after it came out. I believe some commercial search engines started to use it since then. But the ecosystem like models, tools, databases really picked up since GPT3 came out. So recently, I decided to use it in RSS Brain because how easy to do it nowadays.</p>

<h2 id="select-a-model-to-use">Select a Model to Use</h2>

<p>The first step is to select a model to use. I think OpenAI may have the best model that is available to public. You cannot access the real model but there are APIs you can call to use the model. But I don’t like it for 2 reasons: First, I don’t like OpenAI as a company: it presents itself as a non-profit organization first with the goal to make AI accessible to everyone, then stopped publish models or even the algorithm details. Second, I don’t want vendor lock-in.</p>

<p>There is also Llama. But it’s not really a multilingual model. I see some attempts to train it on some other languages, but the result are not that good in my experience. The license of the model is not commercial friendly as well. And there is no easy to use API to get the embeddings.</p>

<p>At the end I found <a href="https://www.sbert.net/index.html">SentenceTransformers</a>. There are lots of <a href="https://www.sbert.net/docs/pretrained_models.html in the project">pretrained models</a>. After all I selected the model <code class="language-plaintext highlighter-rouge">paraphrase-multilingual-mpnet-base-v2</code> since it’s a multilingual model. But it’s called “sentence” transformers for a reason: there is a size limit on the length of document that you can feed in to the models. I ended up to just get the embeddings for the article title. I think it’s a good enough for my use case.</p>

<h2 id="implementation-details-for-model-server">Implementation Details for Model Server</h2>

<p>The library SentenceTransformer is very easy to use. However it’s implemented in Python so it needs a way to communicate with RSS Brain server, which is written in Scala. Since this is a computation heavy task, the first though is to have a buffer queue in between so that the Python program can process the articles in a speed it can handle. Kafka is a good choice for external task queue but I don’t think it worth the complexity to import another component into the system. So I created buffer queue at both end to avoid creating too many requests while maintain some parallelism. Here is what the whole architecture looks like:</p>

<p><img src="/static/images/2023-11-14-Update-On-RSS-Brain-to-Find-Related-Articles-with-Machine-Learning/article-embedding-arch.png" alt="embedding-arch" /></p>

<p>The green parts in the diagram means the workers in them can work concurrently. On the Scala side, it follows the pattern I experimented in <a href="/2023-08-27-Compare-Task-Processing-Approaches-in-Scala.html">Compare Task Processing Approaches in Scala</a>. On the Python side, it’s more tricky since Python’s async handling is far worth than Scala’s plain old Future, not to mention effect systems like Cats Effect. I may write another blog in the future about it.</p>

<p>The reason I go great detail into this relatively simple problem is that it represents a category of problems: problems that need Python to do some async work because of the library supports. For example, in the future, Python server may have more features like fetching Youtube transcriptions. The architecture to integrate it into RSS Brain would be the same.</p>

<h2 id="database-to-store-and-query-embeddings">Database to Store and Query Embeddings</h2>

<p>There are a few vector databases that can store vectors and query nearest vectors if given one. ElasticSearch added vector fields support at 7.0 and approximate nearest neighbor search (ANN) at 8.0. Since RSS Brain is already using ElasticSearch heavily for searching, I can just use it without add another database into the dependency. It also supports machine learning models so that you don’t need to insert the embedding vectors from the outside world, but I find it’s not as flexible.</p>

<p>Once the vectors are inserted into ElastiSearch, it’s just an API call to get the most similar documents. The details of vector insert and query are in the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html">ElasticSearch KNN search document</a>. One tricky part is that even though ElasticSearch supports <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html#_combine_approximate_knn_with_other_features">combining ANN search with other features like term searches (tf-idf algorithm)</a> by using a boost factor, it doesn’t work well unless you are willing to tune it. That’s because the embedding vector and term vector mean different things, and the similarity score is not really comparable. So I ended up enable vector search only for finding related articles, instead of combining with term searches.</p>

<h2 id="result">Result</h2>

<p>It’s actually hard to have some metrics for the performance of finding related articles. I don’t believe metrics like click rate, since it doesn’t necessarily show the articles are related. I think the only way for me is to review the results manually and compute the score based on it. But I don’t think it has much value since supporting multiple language along would make it much better than the previous algorithm. But if you are using RSS Brain, you can see the results yourself and let me know what you think about the new algorithm!</p>]]></content><author><name></name></author><category term="RSS Brain" /><category term="Machine Learning" /><category term="Nerual Network" /><category term="Embeddings" /><category term="Python" /><summary type="html"><![CDATA[In the previous blog about RSS, How RSS Brain Shows Related Articles, I talked about how RSS Brain finds the related articles. I’ve updated the algorithm recently. This blog is about the details about the update. The basic idea is to replace tf-idf algorithm with text embeddings to represent the articles as vectors, and use ElastcSearch to store and query those vectors.]]></summary></entry><entry><title type="html">Add Index Sidebar to My Blog</title><link href="https://www.binwang.me/2023-11-10-Index-Sidebar-on-My-Blog.html" rel="alternate" type="text/html" title="Add Index Sidebar to My Blog" /><published>2023-11-10T00:00:00-05:00</published><updated>2023-11-10T00:00:00-05:00</updated><id>https://www.binwang.me/Index-Sidebar-on-My-Blog</id><content type="html" xml:base="https://www.binwang.me/2023-11-10-Index-Sidebar-on-My-Blog.html"><![CDATA[<p>In a previous blog <a href="/2021-10-31-Add-Index-to-My-Blog.html">Add Index to My Blog</a>, I talked about how I added an index page to my blog that put all the articles into categories. I always wanted the index to be a sidebar instead of a single page, but I guess I didn’t wrap my head around about how to implement so I gave up at last. But recently, when I started to use <a href="https://obsidian.md/">Obsidian</a> and checked some demos of <a href="https://obsidian.md/publish">Obsidian Publish</a>, I found having a sidebar is so useful and beautiful so I decide I should implement it.</p>

<p>You can see the result right now: if you are on a big screen device, the index is on the left side of the page. If you are on a small screen device like a mobile phone, it will show a menu button at the top left corner instead. Clicking it will take you to the index.</p>

<p>When I implement it, I want to keep it simple and stupid. That means:</p>

<ul>
  <li>I want to be as simple as possible as long as it has the function: show articles in nested categories.</li>
  <li>I want to use as little Javascript as possible so people can still use it with Javascript disabled.</li>
</ul>

<p>I found the design of Obsidian Publish is very good. So I copied lots of details from them with some modifications: I didn’t implement showing/hiding sub items when click on the index entry since I think it’s not necessary, and I like how it looks when all the articles are listed there: feels like I’ve written lots of things. The categories are sorted by alphabet order and the posts are ordered by publish date. I also added the publish year for each article entry: some articles can look outdated but if people noticed the published year they can understand the context.</p>

<p>Since I’m using Jeykyll, I can generate plain HTML when possible to avoid the usage of Javascript. So the sidebar is generated for each page instead of using Javascript to keep the sidebar and replace the article content on the fly. Javascript is only used for 2 features:</p>

<ol>
  <li>Remember the position of the sidebar when jump pages.</li>
  <li>Scroll the sidebar to show the entry for the current page if it’s not in the viewpoint.</li>
</ol>

<p>Both of the features are not that important so the sidebar is still usable without Javascript. Even for the menu button on small screens, it’s not popping up a dialog. It just jumps to a new static page that has all the index so no Javascript is needed.</p>

<p>The previous implementation of the index page uses recursive templates: Since the nested index is a tree, rendering the content in a recursive manner is a nature thought. However, I made that mistake to put the complex logic into the template engine. So this time, I traverse the tree with Ruby code and generates a list for the template to render. It has all the information like entry type, the depth of the entry and so on. It makes the template code much simpler so it’s easier to implement other features on top of it.</p>

<p>If you want to checkout the detailed implementation, go to my <a href="https://github.com/wb14123/blog">Github repo for the blog</a> and check <a href="https://github.com/wb14123/blog/blob/master/jekyll/_plugins/Index.rb"><code class="language-plaintext highlighter-rouge">jekyll/_plugins/Index.rb</code></a> and <a href="https://github.com/wb14123/blog/blob/master/jekyll/_includes/index_menu.html"><code class="language-plaintext highlighter-rouge">jekyll/_includes/index_menu.html</code></a>.</p>]]></content><author><name></name></author><category term="blog" /><category term="Jekyll" /><category term="Javascript" /><category term="desgin" /><summary type="html"><![CDATA[In a previous blog Add Index to My Blog, I talked about how I added an index page to my blog that put all the articles into categories. I always wanted the index to be a sidebar instead of a single page, but I guess I didn’t wrap my head around about how to implement so I gave up at last. But recently, when I started to use Obsidian and checked some demos of Obsidian Publish, I found having a sidebar is so useful and beautiful so I decide I should implement it.]]></summary></entry><entry><title type="html">How to Cleanup Ceph Filesystem for Deleted Kubernetes Persistent Volume</title><link href="https://www.binwang.me/2023-11-04-How-to-Cleanup-Ceph-Filesystem-for-Deleted-Kubernetes-Persistent-Volume.html" rel="alternate" type="text/html" title="How to Cleanup Ceph Filesystem for Deleted Kubernetes Persistent Volume" /><published>2023-11-04T00:00:00-04:00</published><updated>2023-11-04T00:00:00-04:00</updated><id>https://www.binwang.me/How-to-Cleanup-Ceph-Filesystem-for-Deleted-Kubernetes-Persistent-Volume</id><content type="html" xml:base="https://www.binwang.me/2023-11-04-How-to-Cleanup-Ceph-Filesystem-for-Deleted-Kubernetes-Persistent-Volume.html"><![CDATA[<p><a href="https://docs.ceph.com">Ceph</a> is a distributed file system. <a href="https://rook.io/">Rook</a> is a project to deploy it with Kubernetes. I recently replaced GlusterFS in my Kubernetes cluster with Ceph. I will write a blog (or a series of blogs) for the migration. But in this article, I will just talk about a problem I encountered, just in case I forget it.</p>

<p>Once Rook is deployed in Kubernetes, you can create a <a href="https://rook.io/docs/rook/v1.11/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage/">Ceph Filesystem</a> and use it to <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes">persistent volume (PV)</a>. Each PV’s data will be stored in a folder in the filesystem. If the PV’s reclaiming policy is set to <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retain">retain</a>, the data will not be deleted after the persistent volume is manually deleted. It’s safer in this way. But what could you do if you want to cleanup the data? Normally you should change the PV’s reclaim policy before you delete the PV, then Rook’s operator will auto reclaim the storage in Ceph. But what if you forget or didn’t know that (like me), and want to cleanup the data after?</p>

<p>First, we need to the folder/subvolume names in Ceph that store’s each PV’s data. We an get that by using <code class="language-plaintext highlighter-rouge">kubectl describe pv &lt;pv-name&gt;</code> and look for the field <code class="language-plaintext highlighter-rouge">subvolumeName</code>. But since the PV is deleted, we need to find the mappings for existing PVs and compare that with the folders/subvolumes in Ceph. This is the command to show all of the existing ones:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get pv -o yaml | grep subvolumeName  | sort
</code></pre></div></div>

<p>Then we need to find all the existing folders/subvolumes in Ceph’s filesystem: Start a Ceph toolbox pod based on the <a href="https://rook.github.io/docs/rook/v1.11/Troubleshooting/ceph-toolbox/?h=toolbox">doc</a>. Then go into the pod and find the filesystem’s name first:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ceph fs ls
</code></pre></div></div>

<p>After getting the filesystem’s name, get all the subvolumegroup from it:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ceph fs subvolume ls &lt;fs-name&gt; csi | grep 'name' | sort
</code></pre></div></div>

<p>Compare this list with the list above, you should be able to find a subvolume that exists in Ceph but not shown in Kubernetes’ PV mapping. Use this command to check its info:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ceph fs subvolume info &lt;fs-name&gt; &lt;subvolume-name&gt; csi
</code></pre></div></div>

<p>If you are sure this is the folder you want to delete, use this command to delete it:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ceph fs subvolume rm &lt;fs-name&gt; &lt;subvolume-name&gt; csi
</code></pre></div></div>]]></content><author><name></name></author><category term="Kubernetes" /><category term="Ceph" /><category term="Distributed file system" /><summary type="html"><![CDATA[Ceph is a distributed file system. Rook is a project to deploy it with Kubernetes. I recently replaced GlusterFS in my Kubernetes cluster with Ceph. I will write a blog (or a series of blogs) for the migration. But in this article, I will just talk about a problem I encountered, just in case I forget it.]]></summary></entry><entry><title type="html">Linux Full Disk Encryption with Yubikey</title><link href="https://www.binwang.me/2023-10-22-Full-Disk-Encryption-with-Yubikey.html" rel="alternate" type="text/html" title="Linux Full Disk Encryption with Yubikey" /><published>2023-10-22T00:00:00-04:00</published><updated>2023-10-22T00:00:00-04:00</updated><id>https://www.binwang.me/Full-Disk-Encryption-with-Yubikey</id><content type="html" xml:base="https://www.binwang.me/2023-10-22-Full-Disk-Encryption-with-Yubikey.html"><![CDATA[<h2 id="background">Background</h2>

<p>As mentioned in a previous blog <a href="/2023-03-13-Infrastructure-Setup-for-High-Availability.html">Infrastructure Setup for High Availability</a>, I’ve setup a high available cluster that has 3 machines. But one of them is on my laptop. I feel like I need a dedicated machine for my personal usage, especially I’m planning some travels. So I need to remove the laptop from the cluster. Its disk space is also very limited. With migrating Gluster to Ceph (more blogs to come on that) and not be able to use a disk partition with Ceph’s encryption, I need another machine with more disks. So I repurposed another small form factor machine to join the cluster.</p>

<p>I want full disk encryption on it but I don’t want to input password every time it boots: this machine is put into a closet and it’s very inconvenient to plug/unplug keyboard and monitor. In another blog <a href="/2021-09-19-Personal-ZFS-Offsite-Online-Backup-Solution.html">Personal ZFS Offsite Backup Solution</a>, I talked about a solution to boot encrypted Linxu without input password by setting up TPM. However, old machines only have TPM 1.x chips instead of newer TPM 2.0 chips, which is very tricky to setup and  with very limited support from Linux distros. I don’t want to do it again if not necessary. The thread model is also different since this machine is supposed to be at my home all the time. So this time, I found a new solution to use Yubikey to decrypt disks: I just need to keep Yubikey plugged in during the boot process and press it at the proper time. I can also fallback to the password method if there is anything wrong with Yubikey decryption.</p>

<p>There are some great tutorials and wiki pages describe how to do it. I must give the credit to <a href="https://0pointer.net/blog/unlocking-luks2-volumes-with-tpm2-fido2-pkcs11-security-hardware-on-systemd-248.html">this article</a> that helped me a lot. But all of them are missing some details so I though it would be great to write down my setup so that it may help someone else. My setup is on Arch Linux but the steps should be portable to other Linux distros.</p>

<p><strong>Warning: the steps may make your system not be able to boot if not setup properly. Make sure to back it up or have a recovery CD available to fix it if things went south.</strong></p>

<h2 id="install-linux-with-luks2">Install Linux with LUKS2</h2>

<p>First we need to install Linux with our root partition encrypted. If you are using an installer, most likely there is an option to encryption the disk. If so, select that option and input a passphrase for it. Even though we are using Yubikey to decrypt the disks, it’s always good to have a passphrase to decrypt it in case something goes wrong. However, if your threat model needs a solution that doesn’t involve a passphrase, I believe you can remove it later after setup Yubikey, though I’ve never tried it myself.</p>

<p>Some installers will use LUKS1 instead of LUKS2 to encrypt the disk. Don’t worry, use <code class="language-plaintext highlighter-rouge">cryptsetup convert --type=LUKS2 &lt;device&gt;</code> to convert it to a LUKS2 setup after the OS is installed.</p>

<p>Note: do not encrypt boot partition. It usually doesn’t have sensitive information and encrypting it doesn’t prevent evil maid attack anyway. If you want it to be more secure, considering setup secure boot, which is also mentioned in my previous blog <a href="/2021-09-19-Personal-ZFS-Offsite-Online-Backup-Solution.html">Personal ZFS Offsite Backup Solution</a>.</p>

<h2 id="enroll-yubikey-to-key-slot">Enroll Yubikey to Key Slot</h2>

<p>We can enroll a FIDO2 (which is a protocol Yubikey supports) device by using <code class="language-plaintext highlighter-rouge">systemd-cryptenroll</code>.</p>

<p>Plug in the Yubikey. You can use this command first to list all the FIDO2 devices to make sure the Yubikey is recognized:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemd-cryptenroll --fido2-device=auto list
</code></pre></div></div>

<p>Note: You may need to install <code class="language-plaintext highlighter-rouge">libfido2</code>.</p>

<p>After confirm the Yubikey is recognized, use this command to enroll it:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemd-cryptenroll --fido2-device=auto &lt;disk-device&gt;
</code></pre></div></div>

<p>It will show hint about you may need to press the Yubikey during the process. So <strong>watch the Yubikey: when its LED flashes, press it to continue.</strong></p>

<h2 id="setup-crypttab">Setup crypttab</h2>

<p>Put a line like this into <code class="language-plaintext highlighter-rouge">/etc/crypttab.initramfs</code>. It will be copied to initramfs by mkinitcpio as <code class="language-plaintext highlighter-rouge">/etc/crypttab</code> so that your root partition can be decrypted before it is mounted:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>myvolume &lt;disk-device&gt; - fido2-device=auto
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">&lt;disk-device&gt;</code> can be something like <code class="language-plaintext highlighter-rouge">/dev/sda1</code> or using UUID format <code class="language-plaintext highlighter-rouge">UUID=&lt;disk-uuid&gt;</code>.</p>

<p>If it’s not root partition, you can put it in <code class="language-plaintext highlighter-rouge">/etc/crypttab</code> so it will be used after root partition is mounted.</p>

<h2 id="setup-mkinitcpio">Setup mkinitcpio</h2>

<p><code class="language-plaintext highlighter-rouge">mkinitcpio</code> is a tool to generate initramfs. <code class="language-plaintext highlighter-rouge">/etc/crypttab.initramfs</code> only works with it. So if your distro comes with other tools like <code class="language-plaintext highlighter-rouge">dracut</code>, you may need to uninstall it and install <code class="language-plaintext highlighter-rouge">mkinitcpio</code> instead.</p>

<p>Once making sure <code class="language-plaintext highlighter-rouge">mkinitcpio</code> is installed, we need to configure the hooks to make it read <code class="language-plaintext highlighter-rouge">crypttab</code> to decrypt the disks. We also need to make sure we are using systemd init instead of busybox init.</p>

<p>Open <code class="language-plaintext highlighter-rouge">/etc/mkinitcpio.conf</code>, and find the line with <code class="language-plaintext highlighter-rouge">HOOKS=(...)</code>. Refer to <a href="https://wiki.archlinux.org/title/Mkinitcpio#Common_hooks">this wiki page</a> about the common hooks and replace busybox hooks with systemd ones. For example, in my setup, I replaced <code class="language-plaintext highlighter-rouge">udev</code> with <code class="language-plaintext highlighter-rouge">systemd</code> and <code class="language-plaintext highlighter-rouge">keymap</code> with <code class="language-plaintext highlighter-rouge">sd-vconsole</code>. Then add <code class="language-plaintext highlighter-rouge">sd-encrypt</code> to the hooks. The order matters: usually it comes after <code class="language-plaintext highlighter-rouge">sd-vconsole</code>.</p>

<p>Then use <code class="language-plaintext highlighter-rouge">mkinitcpio -P</code> to regenerate initramfs images.</p>

<h2 id="test-and-finish">Test and Finish!</h2>

<p>Okay, now we have already setup everything. We can boot the system and test. Make sure Yubikey is plugged in before the boot. And watch for its LED light to flash and press it when it does! This little detail spent me lots of time to figure it out.</p>

<p>You can also use password to decrypt the disk <strong>without Yubikey plugged in</strong>. Wait it for 30 seconds and it will prompt you to input the password. The time can be configured in <code class="language-plaintext highlighter-rouge">/etc/crypttab</code> (or <code class="language-plaintext highlighter-rouge">/etc/crypttab.initramfs</code>) by setting up <a href="https://man.archlinux.org/man/crypttab.5">token-timeout=</a>.</p>]]></content><author><name></name></author><category term="Yubikey" /><category term="Linux" /><category term="Encryption" /><summary type="html"><![CDATA[Background]]></summary></entry><entry><title type="html">A Boring JVM Memory Profiling Story</title><link href="https://www.binwang.me/2023-09-30-A-Boring-JVM-Memory-Profiling-Story.html" rel="alternate" type="text/html" title="A Boring JVM Memory Profiling Story" /><published>2023-09-30T00:00:00-04:00</published><updated>2023-09-30T00:00:00-04:00</updated><id>https://www.binwang.me/A-Boring-JVM-Memory-Profiling-Story</id><content type="html" xml:base="https://www.binwang.me/2023-09-30-A-Boring-JVM-Memory-Profiling-Story.html"><![CDATA[<h2 id="background-of-memory-leakings">Background of Memory Leakings</h2>

<p>I encountered another memory leak problem recently. I’ve debugged a few of memory leak problems in the past, including <a href="https://github.com/splicemachine/spliceengine/pull/2260">the one</a> in Splice Machine, an open source distributed SQL engine based on HBase but was sadly discontinued. The memory leak problems are interesting because it’s challenging to find the root cause. However, I’ve never written a blog about it. Memory leak problems are not so usual, so when I encountered a new one, I kind of need to remember what tools I’ve used. So this time, even though not as interesting as some other memory leak problems I’ve debugged in the past, I decide to write it down as a note for my own reference in the future. The tool set I used this time is relatively simple. I guess I can write more when I use others in the future. This is more like a dev log instead of a tutorial. The “boring” in the title means it’s a pretty standard process and the problem is not that hard to find this time.</p>

<p>Most of the memory leak bugs are very easy to fix once found the root cause, but the part of finding the root cause is tricky. First of all, it’s hard to reproduce: sometime it only happens on production environment. Without knowing the cause, it’s hard to reproduce locally. Even it can be reproduced consistently, it may take some time to let the memory accumulate so the debugging loop can be time consuming sometimes. Last of all, unlike some other bugs that you have an exception and a nice stack trace to help you identify which code causes the problem, it’s almost impossible to find the root cause without the help of a profiler, which itself has challenging parts depending on the platform.</p>

<p>Luckily, JVM has good profilers. That’s one of the reasons Scala, a JVM based language, is my favorite language. (The criteria of a good production language for me is not only the language itself, but also the ecosystem like library, IDE and profilers. JVM based language makes lots of the criteria easy to meet.) This time I uses a very popular profiler <a href="https://www.ej-technologies.com/products/jprofiler/overview.html">JProfiler</a>. Other popular choices that I have used are <a href="https://visualvm.github.io/">VisulaVM</a> and <a href="https://www.oracle.com/java/technologies/jdk-mission-control.html">Java Msission Control</a>. But I found JProfiler is both powerful and easy to use. The only downside is you need to buy a license. It has free trail and open source license. So if you have an open source project or just need to use it for a few days, you can still use it for free.</p>

<h2 id="the-problem">The Problem</h2>

<p>Okay, enough of the background. Let’s dive into the memory leaking problem I encountered this time. As mentioned in the previous blog <a href="http://localhost:4001/2023-09-23-Migrate-Scala2Grpc-to-Cats-Effect-3.html">Migrate Scala2grpc to Cats Effect 3</a>, I migrated one of my side projects to Cats Effect 3 as well, with a lot of other dependencies. This side project is <a href="https://www.rssbrain.com/">RSS Brain</a>. There are two parts on the backend: one for serving client requests with gRPC and gRPC web, another one for fetching RSS feeds. The fetcher gets the RSS feeds that haven’t been fetched for a while from the database with the help of <a href="https://tpolecat.github.io/doobie/">doobie</a> and <a href="https://zio.dev/zio-quill/">quill</a>, and fetch them in parallel with the help of fs2 stream and Cats Effect.</p>

<p>After the mass upgrades, I looked into the metrics to make sure everything is okay. Then I found the fetcher’s memory starts to increase slowly. Looks like a memory leak problem to me. Here is the memory usage graph:</p>

<p><img src="/static/images/2023-09-30-A-Boring-JVM-Memory-Profiling-Story/memory-usage.png" alt="memory usage" /></p>

<p>Seems eventually the JVM will run out of memory but I didn’t wait for it. It’s good to try if force a full gc will reclaim the memory or not, in my case full gc doesn’t help much.</p>

<p>Another metrics to look at is the GC metrics. Only after I shipped the fix, I realized the GC didn’t look normal when there was this memory problem:</p>

<p><img src="/static/images/2023-09-30-A-Boring-JVM-Memory-Profiling-Story/gc.png" alt="gc" /></p>

<p>Before the blue line starts, GCs are all “Copy” and “MarkSweepCompact”, which means the memory are mostly being moved around instead of reclaimed. After the blue line starts, which was when the fix was shipped, we starts to see normal young and old generation GC.</p>

<p>So these metrics indicates that we may have a memory leak issue. Let’s starts to debug it.</p>

<h2 id="setup-profiler">Setup Profiler</h2>

<p>In this case I’m using JProfiler. But as I mentioned above, VisualVM or Java Mission Control should also be able to do the job.</p>

<p>JProfiler has a nice wizard to let you setup the profiler. In my case, since I run the service in Kubernetes, I need to select remote server profiling and go through the wizard. We are going to use <code class="language-plaintext highlighter-rouge">kubectl</code> to forward the debugging port to local, so that we can just use <code class="language-plaintext highlighter-rouge">localhost:8849</code> as the remote address. At the end of the setup wizard, it will prompt you to download the profiler agent and include it with a Java command line argument. Since the service is running in container, I added the following lines to the Dockerfile in order to include the agent in it:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RUN apt update -y &amp;&amp; apt install -y wget
RUN cd /opt &amp;&amp; \
        wget -c 'https://download.ej-technologies.com/jprofiler/jprofiler_agent_linux-x86_14_0.tar.gz' &amp;&amp; \
        tar -xf jprofiler_agent_linux-x86_14_0.tar.gz
</code></pre></div></div>

<p>Also add this flag to Java command line when starting the service:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-agentpath:/opt/jprofiler14/bin/linux-x64/libjprofilerti.so=port=8849,nowait
</code></pre></div></div>

<p>After the new container is deployed, we can port forward 8849 from the service to our localhost with kubectl:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl port-forward &lt;service-pod-name&gt; 8849:8849
</code></pre></div></div>

<h2 id="memory-comparison">Memory Comparison</h2>

<p>Since it’s a memory leaking problem, we want to find out what objects are increasing. First let’s restart the JVM, connect JProfiler to it and take a snapshot of all objects in live memory:</p>

<p><img src="/static/images/2023-09-30-A-Boring-JVM-Memory-Profiling-Story/objects-start.png" alt="objects-start" /></p>

<p>We can see <code class="language-plaintext highlighter-rouge">byte[]</code> takes the most memory but it doesn’t mean it’s responsible for memory leak, since we need to look at the increase of the memory.</p>

<p>So we need to wait for a while for the memory problem starts to happen. In my case, obvious memory increase can be occurred after the JVM run for about 12 hours. Normally if this is a work related thing, I may want to make it faster by increasing the work load. In this case, the code is fetching RSS feeds, so I could make the interval shorter so that it makes more requests. But since this is only a side project, I don’t need to continues working on it, and I also don’t quite like the idea to increase the requests to target RSS websites to increase their load. So I decide just let the JVM run during the night and take another look next day:</p>

<p><img src="/static/images/2023-09-30-A-Boring-JVM-Memory-Profiling-Story/objects-end.png" alt="objects-end" /></p>

<p>Okay, obviously <code class="language-plaintext highlighter-rouge">scala.collection.mutable.LinkedHashMap$LinkedEntry</code> increased a lot. But is there anything else? Conveniently, JProfiler has the feature to compare 2 snapshots. Just go to “Session” -&gt; “Start Center” -&gt; “Open Snapshots” -&gt; “Compare Multiple Snapshots”. After open those 2 snapshots, select both of them on the left and then compare memory:</p>

<p><img src="/static/images/2023-09-30-A-Boring-JVM-Memory-Profiling-Story/objects-compare.png" alt="objects-compare" /></p>

<p>We can see <code class="language-plaintext highlighter-rouge">LinkedEntry</code> indeed increased the most by instance count. However, if we sort by size, we fill find <code class="language-plaintext highlighter-rouge">byte[]</code> increased the most by memory size.</p>

<h2 id="a-false-root-cause">A False Root Cause</h2>

<p>Since <code class="language-plaintext highlighter-rouge">byte[]</code> increased the most by memory size, I’d like to start there. By using “Allocation Call Tree”, we can check which code allocates <code class="language-plaintext highlighter-rouge">byte[]</code> the most. After profiling for a while, we get the following result:</p>

<p><img src="/static/images/2023-09-30-A-Boring-JVM-Memory-Profiling-Story/allocation-tree-bytes.png" alt="allocation-tree-bytes" /></p>

<p>Okay, the top allocation goes to my own code <code class="language-plaintext highlighter-rouge">me.binwang.rss.parser.SourceParser</code>. It’s the class that parse the xml from RSS feeds. So I looked into it if it has any code that can cause memory leak and I found this:</p>

<pre><code class="language-Scala">object SourceParser {

  def parse(url: String, content: Resource[IO, InputStream]): IO[(Source, Seq[Try[FullArticle]])] = {
    content.use { c =&gt;
      // ...
      throw new RuntimeException(s"Error to parse source xml, unrecognized label $label")
      // ...
    }
  }
</code></pre>

<p>So there is an exception thrown in a <code class="language-plaintext highlighter-rouge">Resource.use</code>. <code class="language-plaintext highlighter-rouge">Resource.use</code> makes sure to cleanup the resource when the <code class="language-plaintext highlighter-rouge">use</code> scope is over. But what will happen if it throws an example in there? I thought it will cause <code class="language-plaintext highlighter-rouge">use</code> to not handle the cleanup properly. So I changed it to use <code class="language-plaintext highlighter-rouge">IO.raiseError</code> instead of throw it directly.</p>

<p>However, while I deploying the code, I thought I should really test it. So I wrote a piece of simple code to see whether <code class="language-plaintext highlighter-rouge">Resource</code> will still be cleaned up if there is any exception thrown in <code class="language-plaintext highlighter-rouge">use</code>, and the answer is yes. So this shouldn’t be the root cause. And the deployment result also confirms that: the memory kept increasing with this fix.</p>

<h2 id="the-real-root-cause">The Real Root Cause</h2>

<p>Maybe <code class="language-plaintext highlighter-rouge">byte[]</code> just happened to uses more memory because it’s parsing a large xml at that time. It’s okay that it isn’t the real root cause since we have another lead: <code class="language-plaintext highlighter-rouge">scala.collection.mutable.LinkedHashMap$LinkedEntry</code>. From the profiling, its allocation tree looks like this:</p>

<p><img src="/static/images/2023-09-30-A-Boring-JVM-Memory-Profiling-Story/allocation-tree-linkedlist.png" alt="allocation-tree-linkedlist" /></p>

<p>Okay, so seems most of them come from quill. quill is a library that compiles Scala DSL to SQL queries. It is fairly complex since it uses macros. I checked the code in the allocation tree and couldn’t find out what is wrong.</p>

<p>Then I tried to check the object reference to see which instances are pointed to the these LinkedEntry:</p>

<p><img src="/static/images/2023-09-30-A-Boring-JVM-Memory-Profiling-Story/object-refer.png" alt="object-refer" /></p>

<p>No surprise, they are basically all from quill as well. However, I couldn’t understand the internal AST represents of quill and not sure where are they coming from.</p>

<p>It’s time to search the Internet to see if there is any known issue in quill about memory leak. Maybe I didn’t have the right query, I didn’t find proper results from Internet.</p>

<p>After struggle for a while, I went to its Github repo to search “Memory leak” directly and found 3 issues. That’s good! And there is <a href="https://github.com/zio/zio-quill/issues/2484">one</a> describes the exact problem we have. If we see the allocation tree above, we can find there is a call from <code class="language-plaintext highlighter-rouge">NormalizeCaching</code> (at the bottom of the tree in the picture), which is the class that the issue describes. I guess I didn’t go that far enough to check this class. I’m glad someone else did and found the issue! Basically the root cause is there is a map in the caching doesn’t have any bound. So the cache triggered by dynamic queries never got expired and is growing more and more:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>private val cache = new ConcurrentHashMap[Ast, Ast] 
</code></pre></div></div>

<h2 id="fix-the-memory-leak">Fix the Memory Leak</h2>

<p>The issue is pretty old and is related to a core feature. I’m surprised it’s not fixed yet. As I said, once we found the root cause, the fix should be easy. We just need a way to make the cache expire. I replaced the cache implementation with Guava’s cache, and after the suggestion of maintainer changed it to <a href="https://github.com/ben-manes/caffeine">Caffeine</a>’s cache implementation. <a href="https://github.com/zio/zio-quill/pull/2878">Here is the PR</a>.</p>

<p>I built quill with the fix locally and tested with RSS Brain. The memory leak is indeed fixed! How exciting it is!</p>

<h2 id="conclusion">Conclusion</h2>

<p>Let’s review the process of fixing the memory leak in this case:</p>

<ul>
  <li>Setup profiler.</li>
  <li>Run full GC cannot resolve the memory issue.</li>
  <li>Compare the snapshots between when JVM first started and when the memory increases. See which classes increased most.</li>
  <li>Using allocation tree to find out which part of the code is creating the instances.</li>
  <li>Using references in heap walker to check which classes holds references of those instances.</li>
  <li>Check the identified code and classes.</li>
  <li>If it’s a third party library and we cannot find the root cause, check if the issue is reported. Otherwise report the issue.</li>
  <li>Fix the memory leak based on the root cause.</li>
</ul>]]></content><author><name></name></author><category term="Java" /><category term="JVM" /><category term="Memory Leak" /><category term="Scala" /><category term="JProfiler" /><category term="Profiling" /><summary type="html"><![CDATA[Background of Memory Leakings]]></summary></entry><entry><title type="html">Jekyll Plugin to Load Asciinema Recordings Locally</title><link href="https://www.binwang.me/2023-09-24-Jekyll-Plugin-to-Load-Asciicast-Locally.html" rel="alternate" type="text/html" title="Jekyll Plugin to Load Asciinema Recordings Locally" /><published>2023-09-24T00:00:00-04:00</published><updated>2023-09-24T00:00:00-04:00</updated><id>https://www.binwang.me/Jekyll-Plugin-to-Load-Asciicast-Locally</id><content type="html" xml:base="https://www.binwang.me/2023-09-24-Jekyll-Plugin-to-Load-Asciicast-Locally.html"><![CDATA[<p><a href="https://asciinema.org/">Asciinema</a> is a wonderful tool to record Linux terminal. It saves the records as a text format called Asciicast. However, it has a strong integration with its website. Especially if you want to embed the recordings into the web page use some simple JS code like this:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"https://asciinema.org/a/14.js"</span> <span class="na">id=</span><span class="s">"asciicast-14"</span> <span class="na">async</span><span class="nt">&gt;&lt;/script&gt;</span>
</code></pre></div></div>

<p>You need to share the recordings to Asciinema’s website and need to link an account with the recordings, otherwise they will be deleted after 7 days, which I just found out yesterday. I don’t want my blog to rely on some third party website for core content, so I need a way to load the recordings from my website itself.</p>

<p>Lucky, the <a href="https://github.com/asciinema/asciinema-player">Asciinema Javascript player</a> is open source and support to load recordings from a url out of box. First you need to import the CSS:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;link</span> <span class="na">rel=</span><span class="s">"stylesheet"</span> <span class="na">type=</span><span class="s">"text/css"</span> <span class="na">href=</span><span class="s">"/asciinema-player.css"</span> <span class="nt">/&gt;</span>
</code></pre></div></div>

<p>This is no big deal since this can be put in Jekyll’s template. Then you need some JS code like this:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">"demo"</span><span class="nt">&gt;&lt;/div&gt;</span>
 ...
<span class="nt">&lt;script </span><span class="na">src=</span><span class="s">"/asciinema-player.min.js"</span><span class="nt">&gt;&lt;/script&gt;</span>
<span class="nt">&lt;script&gt;</span>
  <span class="nx">AsciinemaPlayer</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span><span class="dl">'</span><span class="s1">/demo.cast</span><span class="dl">'</span><span class="p">,</span> <span class="nb">document</span><span class="p">.</span><span class="nf">getElementById</span><span class="p">(</span><span class="dl">'</span><span class="s1">demo</span><span class="dl">'</span><span class="p">));</span>
<span class="nt">&lt;/script&gt;</span>
</code></pre></div></div>

<p>It’s a little bit too much for embedding a terminal recording in a blog. However, with the powerful Jekyll plugin system, We can write a plugin to make it simpler so that we can just use a tag to include it:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
{% asciicast &lt;id&gt; %}

</code></pre></div></div>

<p>Here is the implementation, it’s also in <a href="https://github.com/wb14123/blog/blob/master/jekyll/_plugins/Asciicast.rb">my blog’s Github repo</a>:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">module</span> <span class="nn">Jekyll</span>
  <span class="k">class</span> <span class="nc">RenderAsciicastTag</span> <span class="o">&lt;</span> <span class="no">Liquid</span><span class="o">::</span><span class="no">Tag</span>

    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="n">tag_name</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span>
      <span class="k">super</span>
      <span class="vi">@text</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="nf">strip</span>
    <span class="k">end</span>

    <span class="k">def</span> <span class="nf">render</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
      <span class="s2">"&lt;div id=</span><span class="se">\"</span><span class="s2">cast-</span><span class="si">#{</span><span class="vi">@text</span><span class="si">}</span><span class="se">\"</span><span class="s2">&gt;&lt;/div&gt;"</span> <span class="p">\</span>
      <span class="s1">'&lt;script src="/static/js/asciinema-player.min.js"&gt;&lt;/script&gt;'</span> <span class="p">\</span>
      <span class="s2">"&lt;script&gt;AsciinemaPlayer.create('/static/asciicasts/</span><span class="si">#{</span><span class="vi">@text</span><span class="si">}</span><span class="s2">.cast', document.getElementById('cast-</span><span class="si">#{</span><span class="vi">@text</span><span class="si">}</span><span class="s2">'), {rows: 10, autoPlay: true});&lt;/script&gt;"</span>
    <span class="k">end</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="no">Liquid</span><span class="o">::</span><span class="no">Template</span><span class="p">.</span><span class="nf">register_tag</span><span class="p">(</span><span class="s1">'asciicast'</span><span class="p">,</span> <span class="no">Jekyll</span><span class="o">::</span><span class="no">RenderAsciicastTag</span><span class="p">)</span>
</code></pre></div></div>

<p>It will find the recordings under <code class="language-plaintext highlighter-rouge">/static/asciicasts/{id}.cast</code> and load from there.</p>

<p>Put this file under <code class="language-plaintext highlighter-rouge">_plugins</code> and happy hacking!</p>]]></content><author><name></name></author><category term="Jekyll" /><category term="Blog" /><category term="command line" /><category term="Asciicast" /><summary type="html"><![CDATA[Asciinema is a wonderful tool to record Linux terminal. It saves the records as a text format called Asciicast. However, it has a strong integration with its website. Especially if you want to embed the recordings into the web page use some simple JS code like this:]]></summary></entry><entry><title type="html">Migrate Scala2grpc to Cats Effect 3</title><link href="https://www.binwang.me/2023-09-23-Migrate-Scala2Grpc-to-Cats-Effect-3.html" rel="alternate" type="text/html" title="Migrate Scala2grpc to Cats Effect 3" /><published>2023-09-23T00:00:00-04:00</published><updated>2023-09-23T00:00:00-04:00</updated><id>https://www.binwang.me/Migrate-Scala2Grpc-to-Cats-Effect-3</id><content type="html" xml:base="https://www.binwang.me/2023-09-23-Migrate-Scala2Grpc-to-Cats-Effect-3.html"><![CDATA[<p><a href="https://github.com/wb14123/scala2grpc">Scala2grpc</a> is a library and SBT plugin I wrote so that you can integrate gRPC to Scala code in a non-invasive way. In a previous <a href="/2022-05-02-A-Library-to-Make-It-Easier-to-Use-Scala-with-GRPC.html">blog post</a>, I talked about the motivation behind it.</p>

<p>The library requires each service method to return Cats Effect’s <code class="language-plaintext highlighter-rouge">IO</code> or fs2 stream. However, it’s still using Cats Effect 2.x version. There are big changes in Cats Effect 3 and almost all up to date libraries already support it. So it’s time to migrate it to Cats Effect 3 as well.</p>

<h2 id="replace-akka-grpc-with-fs2-grpc">Replace Akka gRPC with fs2-grpc</h2>

<p>This library was using <a href="https://doc.akka.io/docs/akka-grpc/current/index.html">akka-grpc</a>. But I want to replace it for a few reasons:</p>

<ul>
  <li>It uses <a href="https://github.com/krasserm/streamz">streamz</a> to convert between Akka streams and fs2 streams. This library doesn’t support Cats Effect 3 and hasn’t been updated for a while. This is the biggest reason that I need to migrate from akka immediately.</li>
  <li>Akka changed its open source license to a very expensive one which I didn’t know at the time writing this library. Even the license doesn’t cost anything if the revenue doesn’t reach a certain point, I don’t want it to be a liability.</li>
  <li>It’s good to have a gRPC library that supports for Cats Effect and fs2 streams natively.</li>
</ul>

<p>So in the newer version, I replaced Akka gRPC with <a href="https://github.com/typelevel/fs2-grpc">fs2-grpc</a>, a library under Typelevel umbrella and natively supports Cats Effect and fs2. The document is not as good and I spent quite some time to figure out how to actually use it, but I’m happy it finally worked out.</p>

<h2 id="add-hooks-to-grpc-calls">Add Hooks to gRPC Calls</h2>

<p>In the previous version of Scala2grpc, there is a feature to log every request. But it is a pretty hacky implementation: I just throw the logging logic into the generated code. Since this version is a breaking change, it’s a good opportunity to revisit the approach and see how to add generic hooks before and after each gRPC calls.</p>

<p>My implementation wraps the gRPC response into a context in the generated code. The context includes the response with type of <code class="language-plaintext highlighter-rouge">IO</code> or <code class="language-plaintext highlighter-rouge">fs2.Stream</code>. Because of the referential transparency, you can take the response and add hooks before or after it. More detailed document is in <a href="https://github.com/wb14123/scala2grpc#4-optional-define-custom-grpc-hook">this section of readme</a>.</p>

<h2 id="non-invasive-nature-of-the-library">Non-invasive Nature of the Library</h2>

<p>The migration brings lots of breaking changes, so it is a good test for the non-invasive nature of the library. I have 2 side projects that are using this library and I migrated one of them recently. Since it’s also using Cats Effect, there are some migration steps unrelated to this library. But regarding of the related parts, the migration process is very smooth: I don’t need to change any implementation code of the services. The generated gPRC protocol files are also not changed either. The only thing I need to change is the single object that implements <code class="language-plaintext highlighter-rouge">GRPCGenerator</code> to <a href="https://github.com/wb14123/scala2grpc#2-create-an-object-to-implement-grpcgenerator">pass in some new parameters</a>. It is impossible if I used akka gRPC directly and changed it to fs2-grpc since their interface are all different.</p>

<p>I’m so happy with the result: it adds gRPC to pure Scala code so easily without ever touch it. It saves me so much time to build a service and keeps the code clean at the same time. It continue to be a must have for my future Scala gRPC projects.</p>]]></content><author><name></name></author><category term="Programming" /><category term="Scala" /><category term="gRPC" /><category term="Cats" /><category term="Functional Programming" /><summary type="html"><![CDATA[Scala2grpc is a library and SBT plugin I wrote so that you can integrate gRPC to Scala code in a non-invasive way. In a previous blog post, I talked about the motivation behind it.]]></summary></entry><entry><title type="html">Build a Linux Virtual Machine for Windows Apps</title><link href="https://www.binwang.me/2023-09-01-Build-a-Linux-Virtual-Machine-for-Windows-Apps.html" rel="alternate" type="text/html" title="Build a Linux Virtual Machine for Windows Apps" /><published>2023-09-01T00:00:00-04:00</published><updated>2023-09-01T00:00:00-04:00</updated><id>https://www.binwang.me/Build-a-Linux-Virtual-Machine-for-Windows-Apps</id><content type="html" xml:base="https://www.binwang.me/2023-09-01-Build-a-Linux-Virtual-Machine-for-Windows-Apps.html"><![CDATA[<h2 id="background">Background</h2>

<p>Linux is great. However sometimes you just need to run some Windows only applications to collaborate with other people, especially if it’s impossible to let the other party to change the software. Luckily I rarely run into that situation in the past 10+ years. The only recent exceptions I can remember are filling some government forms (which uses pdf with XFA form. Yes Firefox can fill that now but it’s still incompatible with Adobe Reader from time to time), use IM and video meeting software with a previous Chinese client for some consultant work.</p>

<p>Even it’s rarely used, it’s handy to have a VM to run Windows applications. But Windows is becoming more and more bloat, adding more and more tracking and ads, basically more holistic to users. The only good parts in Windows XP and Windows 7 have long gone. Currently I have a Windows 10 VM, but I’m not sure if I ever want to login to Windows 11 if Windows 10’s life is end. So it’s good to have a backup plan to run Windows apps without Windows. Practise reasons aside, it’s just fun to play with Linux distros. Linux and its desktop environments are so diverse and configurable, I spent such a great time to explore what are the possibilities.</p>

<p>There is <a href="https://www.winehq.org/">Wine</a> to run Windows applications on Linux. It’s not perfect. Some apps need to be tweaked a lot in order to run with wine and some are just nearly impossible. Even the software can be run with wine, I don’t want to run it on my OS since wine is just a compatibility layer, not a sandbox. Which means the typical malware like behaviours in Windows applications are still effective under Wine. So I need to run it in a virtual machine. It also gives us an opportunity to select a distro to focus more on this specific task.</p>

<p>I have tried a few distros in the last few days. At the end I find <a href="https://www.deepin.org">Deepin Linux</a> is the best one for this use case. Especially you want to run Chinese Windows apps.</p>

<h2 id="a-little-history-of-deepin">A Little History of Deepin</h2>

<p>Deepin’s root is in Windows. It first started as a Windows online forum and then started to customize and piracy Windows XP. The year was 2006. Almost no one bought Windows in China back in the days. I don’t know if business or even Universities ever bought Windows licenses or not, but even they do, it’s a very common practise to install piracy Windows in those environment because the popular ones are so user friendly. Computer sellers would ask the buyers if they want to change the stock Windows to a piracy one, and most of the time they do. Ironically, the only time I’ve inputted a (legitimately obtained) Windows key is when I was working at Redhat and setting up a Windows server for testing Samba and nfs. The popular piracy versions are really impressive: the installation is fast and easy, they are more beautiful, they include things like system backup and recovery, they have common drivers pre-installed and application to find drivers (not like the driver finder on Windows, this one actually works), the system was cut down to a very small size and so on. However, if it sounds sketchy to you, you are not wrong. Even though the user experience maybe superior, there is no shortage of back doors and things like that. The popular versions even have their own more sketchy piracy versions. However, that era is wild west for computer security and just one more vulnerability didn’t really matter that much in my mind.</p>

<p>Anyway, Deepin was one of the most popular among them. But things didn’t last for long. Around 2008, the year China hosted its first Olympic Game, the person behind a popular piracy version got arrested. Even the common practise of using piracy Windows in China lasted a long time after that, the big ones felt the risk and stopped making them. Some of them started to make Linux distros instead. Again, Deepin became one of the most popular. <a href="https://en.wikipedia.org/wiki/YLMF_Computer_Technology_Co.,_Ltd.">雨林沐风 (YLMF)</a> is another very popular one which is in famous of its clean and beautify theme in the Windows piracy era. It started to make Linux distro (<a href="https://en.wikipedia.org/wiki/StartOS">YLMF OS</a>) around the same time. It is the first Linux distro I’ve ever used and introduced the whole world of Linux to me.</p>

<p>It’s no wonder Deepin Linux has good support for Windows applications: Windows users are its earliest user base. It’s still true nowadays: even with some failed attempts, Chinese government never stopped exploring to use Linux instead of Windows on government devices. After these years, because of factors like applications are more web based instead of native, and the better experience of Linux desktop, Chinese government actually replaced a large amount of their devices with Linux. From what I know, they are using <a href="https://en.wikipedia.org/wiki/Ubuntu_Kylin">Ubuntu Kylin</a> instead of Deepin or UOS (commercial version of Deepin), but the market is large enough to motivate Deepin to continue maintaining Windows app supports.</p>

<h2 id="deepin-linux-101">Deepin Linux 101</h2>

<p>We’ve talked enough history. Let’s look at Deepin Linux at nowadays. It has it’s own desktop environment called DDE and includes lots of its own apps like browser, video player, mail client and so on. But it’s not my taste and the DE is pretty resource hungry on my machine. Luckily, Deepin is based on Debian stable, so you can basically customize to whatever you like using Debian packages, which we will do later.</p>

<p>The main thing we want in Deepin Linux is its app store. It has lots of Windows applications supported by default. Deepin actually has its own wine version deepin-wine to support those Windows apps better. No matter what tweaks we are going to do with the system, make sure app store works after that.</p>

<p>It also ships with Android support with UEngine, which is a fork of Anbox. There are some officially supported Android apps in the package repo but seems they are not findable in app store. You can use <code class="language-plaintext highlighter-rouge">apt search uengine</code> to find them in terminal. I’ve never had good experience with Android in VM and this time it’s no exception: I tried to install an app from apt and it couldn’t start because of uengine startup timeout. I’m not sure if it will be better on a physical machine but I don’t bother to try it.</p>

<p>Overall, I have double feelings about Deepin. On one hand, it’s pretty impressive on the technical side about what they have done and the community they’ve built. On the other hand, it always feels a little bit sketchy. Even after the Windows piracy era, the Linux distro is still less trustworthy in my mind because of some telemetry its app store collects, not straightforward removable stock apps, the connection (or the intention to connect) with Chinese government and so on. In addition of all the Windows apps I’m going to install, I will not have any personal or important data on it.</p>

<p>After understanding the basics of Deepin Linux, let’s go ahead to install and tweak it. To have a taste of what it will look like, let me show a screenshot of my setup:</p>

<p><img src="/static/images/2023-08-31-Build-a-Linux-VM-for-Windows-Apps/screenshot.png" alt="screnshot" /></p>

<h2 id="installation">Installation</h2>

<p>The installation is pretty straightforward, but make sure to make these tweaks:</p>

<ul>
  <li>During the installation, it will detect that you are in a VM and prompt to use “performance” mode. Make sure to select it so that it will be faster. Even though we will replace the DE later so I don’t think it matters that much, but it doesn’t hurt anyway.</li>
  <li>By default it will create a recovery partition, which is a waste of storage since we are using a VM. We can take snapshots through the VM software if we backup and recovery. So make sure to manually partition the file system and not using recovery partition.</li>
</ul>

<h2 id="replace-dde-with-xfce">Replace DDE with XFCE</h2>

<p>As I said, I don’t like the default DDE Deepin ships. And with limited time I don’t find it’s very configurable as well. So we will replace it with the less resource hungry and highly customizable XFCE. In order to install XFCE, run this in the terminal:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo apt install xfce4 xfce4-goodies
</code></pre></div></div>

<p>Then logout and select <code class="language-plaintext highlighter-rouge">xfce</code> in the login screen.</p>

<h2 id="configure-lightdm">Configure lightdm</h2>

<p>Deepin is using <code class="language-plaintext highlighter-rouge">lightdm</code> as its display manager. To match our simple xfce feeling, I’d like to change the login screen to a simpler and reto look. Open <code class="language-plaintext highlighter-rouge">/etc/ligthdm/lightdm.conf</code> and apply these changes:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- greeter-session=lightdm-deepin-greeter
+ greeter-session=lightdm-gtk-greeter
- user-session=deepin
+ user-sesion=xfce
</code></pre></div></div>

<h2 id="install-windows-95-theme">Install Windows 95 theme</h2>

<p>Another important reason of choosing XFCE the <a href="https://github.com/grassmunk/Chicago95">Chicago95 theme</a>, which makes XFCE looks like Windows 95. If we are having an OS for running Windows programs, what’s a better look than Windows 95/98/2000 era theme?</p>

<p>Go to <a href="https://github.com/grassmunk/Chicago95">Chicago95’s Github repo</a> and follow the instructions to install it. Since it’s made for XUbuntu, which is based on Ubuntu which is in turn based on Debian, the installation script works perfectly. After installation, read the popped up txt file so tweak remaining things to make it more Windows 95 like if you want.</p>

<p>If you also want to make the login screen Windows 95 like, you need to use <code class="language-plaintext highlighter-rouge">lightdm-webkit-greeter</code> instead of the <code class="language-plaintext highlighter-rouge">lightdm-gtk-greeter</code> above and change the theme. See <a href="https://github.com/grassmunk/Chicago95/tree/5670fde8ce33b33d37622b888278aa9cdbe5eea2/Lightdm/Chicago95">the doc</a> for more details. However, <code class="language-plaintext highlighter-rouge">lightdm-webkit-greeter</code> is not in Debian or Deepin’s package repo by default and I find the trouble to install it manually doesn’t worth it, so I didn’t make the change.</p>

<p>For Firefox, there is a <a href="https://addons.mozilla.org/en-US/firefox/addon/windows-98-se/?utm_source=addons.mozilla.org&amp;utm_medium=referral&amp;utm_content=search">Windows 98 SE</a> theme I find fit into the system theme the best.</p>

<h2 id="other-xfce-tweaks">Other XFCE tweaks</h2>

<ul>
  <li>Disable compositor in Settings -&gt; Window Manager Tweaks -&gt; Compositor. Some windows will have a black frame after this, so enable/disable it based on your preference.</li>
  <li>I’d also like to disable auto session save:
    <ul>
      <li>Uncheck all the boxes in Settings -&gt; Session and Startup -&gt; General (I find it doesn’t prompt and auto save if <code class="language-plaintext highlighter-rouge">prompt on logout</code> is checked)</li>
      <li>Remove all the existing sessions: <code class="language-plaintext highlighter-rouge">rm -r ~/.cache/sessions/*</code></li>
    </ul>
  </li>
</ul>

<h2 id="remove-stock-deepin-apps">Remove Stock Deepin Apps</h2>

<p>There are lots of stock apps made by Deepin. Even though I like the effort, I still prefer the familiar ones and the ones XFCE has. So I need to uninstall the stock apps. However, you cannot uninstall them through Deepin’s app store, so we need to use <code class="language-plaintext highlighter-rouge">apt</code> to find and remove them.</p>

<p>Use <code class="language-plaintext highlighter-rouge">apt search deepin | grep installed</code> to find installed deepin packages and remove the ones you don’t want. Then use <code class="language-plaintext highlighter-rouge">sudo apt autoremove</code> and <code class="language-plaintext highlighter-rouge">sudo apt autoclean</code> to cleanup the not needed dependencies. Make sure app store is still working after this since it’s the whole point of using Deepin.</p>

<h2 id="disable-deepin-services">Disable Deepin Services</h2>

<p>I didn’t remove all the deepin related packages since some of them are needed by the App Store. However, I don’t think some of them are needed to run as deamon even if I left them on the machine. So type <code class="language-plaintext highlighter-rouge">systemctl status deepin-</code> and press tab for autocomplete to see the systemd services related to deepin, and use <code class="language-plaintext highlighter-rouge">systemctl disable &lt;service&gt;</code> to disable the ones you don’t need.</p>]]></content><author><name></name></author><category term="Linux" /><category term="Windows" /><category term="Virtual Machine" /><category term="Deepin" /><category term="Wine" /><summary type="html"><![CDATA[Background]]></summary></entry><entry><title type="html">Compare Task Processing Approaches in Scala</title><link href="https://www.binwang.me/2023-08-27-Compare-Task-Processing-Approaches-in-Scala.html" rel="alternate" type="text/html" title="Compare Task Processing Approaches in Scala" /><published>2023-08-27T00:00:00-04:00</published><updated>2023-08-27T00:00:00-04:00</updated><id>https://www.binwang.me/Compare-Task-Processing-Approaches-in-Scala</id><content type="html" xml:base="https://www.binwang.me/2023-08-27-Compare-Task-Processing-Approaches-in-Scala.html"><![CDATA[<p><em>All the source code mentioned in this blog can be found in <a href="https://github.com/wb14123/scala-stream-demo">my Github repo</a>.</em></p>

<h2 id="task-processing">Task Processing</h2>

<p>There is a common problem in computer science and I’ve met it again recently: how to generate and process tasks efficiently? Use my recent project <a href="https://www.rssbrain.com">RSS Brain</a> as an example: it needs to find the RSS feeds that haven’t been updated for a while in a database, and fetch the newest data from network.</p>

<p>The easiest way to do it is producing and consuming the tasks in a sequence, for example:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">feeds</span> <span class="k">=</span> <span class="nf">getPendingFeeds</span><span class="o">()</span> <span class="c1">// produce the tasks</span>
<span class="nv">feeds</span><span class="o">.</span><span class="py">foreach</span><span class="o">(</span><span class="n">fetchFromNetwork</span><span class="o">)</span> <span class="c1">// consume the dtasks</span>
</code></pre></div></div>

<p>However, it is unnecessarily slow. Network request doesn’t take lots of CPU and we can send multiple requests at the same time. Even if <code class="language-plaintext highlighter-rouge">fetchFromNetwork</code> is a CPU bound task, it can be parallelized if there are multiple CPU cores on a machine.</p>

<p>In this article, we will explore ways to do it more efficiently with <a href="https://typelevel.org/cats-effect/">Cats Effect</a> and <a href="https://fs2.io">FS2</a> in a functional programming fashion.</p>

<p><em>You may wonder why not using AKKA stream? Other than it’s using a different programming paradigm (not functional programming), it’s also because <a href="https://www.lightbend.com/blog/why-we-are-changing-the-license-for-akka">AKKA has changed its license</a> with a ridiculous price.</em></p>

<h2 id="introducing-cats-effect-and-fs2">Introducing Cats Effect and FS2</h2>

<p>To make <code class="language-plaintext highlighter-rouge">processTask</code> async, there is <code class="language-plaintext highlighter-rouge">Future</code> in Scala’s standard library. However, the side effect will happen when you create a <code class="language-plaintext highlighter-rouge">Future</code> instance. For example:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">processTask</span><span class="o">(</span><span class="n">task</span><span class="k">:</span> <span class="kt">Task</span><span class="o">)</span><span class="k">:</span> <span class="kt">Future</span><span class="o">[</span><span class="kt">Unit</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Future</span><span class="o">(</span><span class="nf">println</span><span class="o">(</span><span class="n">task</span><span class="o">))</span>

<span class="k">val</span> <span class="nv">runTask1</span> <span class="k">=</span> <span class="nf">processTask</span><span class="o">(</span><span class="n">task1</span><span class="o">)</span> <span class="c1">// this will start the async task</span>
</code></pre></div></div>

<p>I assume the readers have a basic understanding of functional programming, so I’ll not explain why we want to avoid side effects. While Scala is not a pure functional language, a popular Scala library <a href="https://typelevel.org/cats-effect/">Cats Effect</a> provides convenient ways to wrap side effects. With the help of its <code class="language-plaintext highlighter-rouge">IO</code> type, we can define an async task like this:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">processTask</span><span class="o">(</span><span class="n">task</span><span class="k">:</span> <span class="kt">Task</span><span class="o">)</span><span class="k">:</span> <span class="kt">IO</span><span class="o">[</span><span class="kt">Unit</span><span class="o">]</span> <span class="k">=</span> <span class="nc">IO</span><span class="o">(</span><span class="nf">println</span><span class="o">(</span><span class="n">task</span><span class="o">))</span>

<span class="c1">// this will not start the task, so no side effect</span>
<span class="k">val</span> <span class="nv">runTask1</span> <span class="k">=</span> <span class="nf">processTask</span><span class="o">(</span><span class="n">task1</span><span class="o">)</span>

<span class="c1">// out of pure functional world and starts the side effect</span>
<span class="nv">runTask1</span><span class="o">.</span><span class="py">unsafeRunSync</span><span class="o">()</span>
</code></pre></div></div>

<p>Then there is <a href="https://fs2.io">fs2</a> that is a stream library that can be used with cats effect. It will be very handy when resolving our problem as we can see later.</p>

<p><em>Cat Effect has some big changes in version 3.x. In this article, we are using version 2.x. But I may upgrade the version in the future.</em></p>

<h2 id="testing-setup">Testing Setup</h2>

<p>In order to test which approach is the best under different scenarios, we need some basic setup. In <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/TestRunner.scala">TestRunner.scala</a>, I defined some functions to generate tasks. Here are their signatures:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Produce a sequence of tasks represented by `Int`</span>
<span class="k">def</span> <span class="nf">produce</span><span class="o">(</span><span class="n">start</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">end</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="kt">IO</span><span class="o">[</span><span class="kt">Seq</span><span class="o">[</span><span class="kt">Int</span><span class="o">]]</span>

<span class="c1">// Process a task</span>
<span class="k">def</span> <span class="nf">consume</span><span class="o">(</span><span class="n">x</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="kt">IO</span><span class="o">[</span><span class="kt">Unit</span><span class="o">]</span>

<span class="c1">// Produce tasks as a stream</span>
<span class="k">def</span> <span class="nf">def</span> <span class="nf">produceStream</span><span class="o">(</span><span class="n">start</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">end</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span><span class="k">:</span> <span class="kt">fs2.Stream</span><span class="o">[</span><span class="kt">IO</span>, <span class="kt">Int</span><span class="o">]</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">produce</code> simply produces tasks as <code class="language-plaintext highlighter-rouge">int</code>, and <code class="language-plaintext highlighter-rouge">consume</code> just print characters. In each of the functions, I use <code class="language-plaintext highlighter-rouge">IO.sleep</code> to create some delay to simulate the real world non-blocking IO. They also print characters <code class="language-plaintext highlighter-rouge">P</code> (produce) or <code class="language-plaintext highlighter-rouge">C</code> (consume) (based on the width of terminal, some of the <code class="language-plaintext highlighter-rouge">C</code> outputs may be skipped to fit the width) when being invoked, so that we can have an intuitive view of how quick tasks are produced and consumed.</p>

<p>Then there is <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/TestConfig.scala">TestConfig.scala</a> for configuring the test:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">trait</span> <span class="nc">TestConfig</span> <span class="o">{</span>
  <span class="k">val</span> <span class="nv">testName</span><span class="k">:</span> <span class="kt">String</span>
  <span class="k">val</span> <span class="nv">produceDelay</span><span class="k">:</span> <span class="kt">FiniteDuration</span>
  <span class="k">val</span> <span class="nv">minConsumeDelayMillis</span><span class="k">:</span> <span class="kt">Long</span>
  <span class="k">val</span> <span class="nv">maxConsumeDelayMillis</span><span class="k">:</span> <span class="kt">Long</span>
  <span class="k">val</span> <span class="nv">batchSize</span> <span class="k">=</span> <span class="mi">100</span>  <span class="c1">// consume batch size</span>
  <span class="k">val</span> <span class="nv">totalSize</span> <span class="k">=</span> <span class="mi">1000</span> <span class="c1">// how many tasks to generate</span>
<span class="o">}</span>
</code></pre></div></div>

<p>By setting up produce and consume delays, we can test scenarios when producer is slower, consumer is slower, or producer and consumer speed is almost the same. Here are the configurations we are going to use in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/Main.scala">Main.scala</a></p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">configs</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span>
  <span class="k">new</span> <span class="nc">TestConfig</span> <span class="o">{</span>
    <span class="k">override</span> <span class="k">val</span> <span class="nv">testName</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="s">"slow-producer"</span>
    <span class="k">override</span> <span class="k">val</span> <span class="nv">produceDelay</span><span class="k">:</span> <span class="kt">FiniteDuration</span> <span class="o">=</span> <span class="mf">1000.</span><span class="n">millis</span>
    <span class="k">override</span> <span class="k">val</span> <span class="nv">minConsumeDelayMillis</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="k">override</span> <span class="k">val</span> <span class="nv">maxConsumeDelayMillis</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">100</span>
  <span class="o">},</span>
  <span class="k">new</span> <span class="nc">TestConfig</span> <span class="o">{</span>
    <span class="k">override</span> <span class="k">val</span> <span class="nv">testName</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="s">"balanced"</span>
    <span class="k">override</span> <span class="k">val</span> <span class="nv">produceDelay</span><span class="k">:</span> <span class="kt">FiniteDuration</span> <span class="o">=</span> <span class="mf">1005.</span><span class="n">millis</span>
    <span class="k">override</span> <span class="k">val</span> <span class="nv">minConsumeDelayMillis</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="k">override</span> <span class="k">val</span> <span class="nv">maxConsumeDelayMillis</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">2000</span>
  <span class="o">},</span>
  <span class="k">new</span> <span class="nc">TestConfig</span> <span class="o">{</span>
    <span class="k">override</span> <span class="k">val</span> <span class="nv">testName</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="s">"slow-consumer"</span>
    <span class="k">override</span> <span class="k">val</span> <span class="nv">produceDelay</span><span class="k">:</span> <span class="kt">FiniteDuration</span> <span class="o">=</span> <span class="mf">10.</span><span class="n">millis</span>
    <span class="k">override</span> <span class="k">val</span> <span class="nv">minConsumeDelayMillis</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="k">override</span> <span class="k">val</span> <span class="nv">maxConsumeDelayMillis</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">1000</span>
  <span class="o">}</span>
<span class="o">)</span>
</code></pre></div></div>

<h2 id="approach-1-batch-consuming">Approach 1: Batch Consuming</h2>

<p>The first approach is to make the consuming side parallel. We can consume a batch of tasks concurrently, like in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/BatchIOApp.scala">BatchIOApp.scala</a>.</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loop</span><span class="o">(</span><span class="n">start</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="kt">IO</span><span class="o">[</span><span class="kt">Unit</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="nf">if</span> <span class="o">(</span><span class="n">start</span> <span class="o">&gt;=</span> <span class="nv">config</span><span class="o">.</span><span class="py">totalSize</span><span class="o">)</span> <span class="o">{</span>
    <span class="nv">IO</span><span class="o">.</span><span class="py">unit</span>
  <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
    <span class="nf">produce</span><span class="o">(</span><span class="n">start</span><span class="o">,</span> <span class="n">start</span> <span class="o">+</span> <span class="nv">config</span><span class="o">.</span><span class="py">batchSize</span><span class="o">)</span>
      <span class="o">.</span><span class="py">flatMap</span><span class="o">{</span><span class="nv">_</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">consume</span><span class="o">).</span><span class="py">parSequence</span><span class="o">}</span>
      <span class="o">.</span><span class="py">flatMap</span><span class="o">(</span><span class="k">_</span> <span class="k">=&gt;</span> <span class="nf">loop</span><span class="o">(</span><span class="n">start</span> <span class="o">+</span> <span class="nv">config</span><span class="o">.</span><span class="py">batchSize</span><span class="o">))</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<p>However, this only makes a batch of tasks run in parallel. It needs to wait the whole batch to be finished in order to start next batch. This is very obvious when we run this approach and see the output of characters (download <a href="https://github.com/wb14123/scala-stream-demo">Github repo</a> and run <code class="language-plaintext highlighter-rouge">sbt "run -n BatchIOApp"</code>). See how it paused after each batch even when consumer is slower than producer:</p>

<div id="cast-tmpg4x5_bn7-ascii"></div>
<script src="/static/js/asciinema-player.min.js"></script>
<script>AsciinemaPlayer.create('/static/asciicasts/tmpg4x5_bn7-ascii.cast', document.getElementById('cast-tmpg4x5_bn7-ascii'), {rows: 10, autoPlay: true});</script>

<h2 id="approach-2-use-blocking-queue-to-buffer-tasks">Approach 2: Use Blocking Queue to Buffer Tasks</h2>

<p>We need a way to let producers not waiting for consumers, and also let consumers not wait for a batch to finish in order to start next batch. A very common solution is to use a queue between producers and consumers. Producers put tasks into the queue, and consumers get tasks for the queue. If the queue is thread safe, then both producers and consumers can work on their own without care about each other. In order to not let producer put unlimited tasks into the queue to blowup the memory, we need the queue to have a capacity. When the queue is full, the producer should be blocked. And when the queue is empty, the consumers should be blocked as well.</p>

<p>In Java, <code class="language-plaintext highlighter-rouge">BlockingQueue</code> meets our requirements. We can use an implementation <code class="language-plaintext highlighter-rouge">LinkedBlockingQueue</code>. However, <code class="language-plaintext highlighter-rouge">BlockingQueue</code> will block the whole thread instead of a single <code class="language-plaintext highlighter-rouge">IO</code>. Let’s not worry about it for now and see how to use a queue to implement producing and consuming in parallel. The implementation is in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/BlockingQueueApp.scala">BlockingQueueApp.scala</a>:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">val</span> <span class="nv">queue</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">LinkedBlockingQueue</span><span class="o">[</span><span class="kt">Option</span><span class="o">[</span><span class="kt">Int</span><span class="o">]](</span><span class="nv">config</span><span class="o">.</span><span class="py">batchSize</span> <span class="o">*</span> <span class="mi">2</span><span class="o">)</span>

<span class="k">override</span> <span class="k">def</span> <span class="nf">work</span><span class="o">()</span><span class="k">:</span> <span class="kt">IO</span><span class="o">[</span><span class="kt">Unit</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="nc">Seq</span><span class="o">(</span>
    <span class="o">(</span><span class="nf">produceStream</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="py">map</span><span class="o">(</span><span class="nc">Some</span><span class="o">(</span><span class="k">_</span><span class="o">))</span> <span class="o">++</span> <span class="nv">fs2</span><span class="o">.</span><span class="py">Stream</span><span class="o">.</span><span class="py">emit</span><span class="o">(</span><span class="nc">None</span><span class="o">))</span>
			<span class="o">.</span><span class="py">evalMap</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="nc">IO</span><span class="o">(</span><span class="nv">queue</span><span class="o">.</span><span class="py">put</span><span class="o">(</span><span class="n">x</span><span class="o">))).</span><span class="py">compile</span><span class="o">.</span><span class="py">drain</span><span class="o">,</span>
    <span class="nf">dequeueStream</span><span class="o">().</span><span class="py">unNoneTerminate</span><span class="o">.</span><span class="py">parEvalMap</span><span class="o">(</span><span class="nv">config</span><span class="o">.</span><span class="py">batchSize</span><span class="o">)(</span><span class="n">consume</span><span class="o">).</span><span class="py">compile</span><span class="o">.</span><span class="py">drain</span><span class="o">,</span>
  <span class="o">).</span><span class="py">parSequence</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="k">_</span> <span class="k">=&gt;</span> <span class="o">())</span>
<span class="o">}</span>

<span class="k">private</span> <span class="k">def</span> <span class="nf">dequeueStream</span><span class="o">()</span><span class="k">:</span> <span class="kt">fs2.Stream</span><span class="o">[</span><span class="kt">IO</span>, <span class="kt">Option</span><span class="o">[</span><span class="kt">Int</span><span class="o">]]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="nv">fs2</span><span class="o">.</span><span class="py">Stream</span><span class="o">.</span><span class="py">eval</span><span class="o">(</span><span class="nc">IO</span><span class="o">(</span><span class="nv">queue</span><span class="o">.</span><span class="py">take</span><span class="o">()))</span> <span class="o">++</span> <span class="nf">dequeueStream</span><span class="o">()</span>
<span class="o">}</span>
</code></pre></div></div>

<p>Here we have two IOs run in parallel with <code class="language-plaintext highlighter-rouge">parSequence</code>: the first one creates a task stream by <code class="language-plaintext highlighter-rouge">produceStream</code>, and append <code class="language-plaintext highlighter-rouge">None</code> at the end so that the consumer knows it should end processing. Another stream <code class="language-plaintext highlighter-rouge">dequeueStream</code> gets the tasks from the queue then consumes it in parallel with <code class="language-plaintext highlighter-rouge">parEvalmap(config.batchSize)(consume)</code>.</p>

<p>When run it with <code class="language-plaintext highlighter-rouge">sbt "run -n BlockingQueueApp"</code>, we can see it’s much faster when the consumer is faster or has the same speed as the producer. Especially when the consumer is slow, it prints multiple <code class="language-plaintext highlighter-rouge">P</code> at first, which means the producers doesn’t wait all the consumers to finish in order to produce tasks.</p>

<div id="cast-tmp6dx2heu_-ascii"></div>
<script src="/static/js/asciinema-player.min.js"></script>
<script>AsciinemaPlayer.create('/static/asciicasts/tmp6dx2heu_-ascii.cast', document.getElementById('cast-tmp6dx2heu_-ascii'), {rows: 10, autoPlay: true});</script>

<p>Back to the blocking the whole thread problem: it doesn’t seem to be a problem in this case, right? It’s only because we are lucky! In this setup, we are using two fixed threads as the thread pool of running IO in <code class="language-plaintext highlighter-rouge">Main.scala</code>:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">private</span> <span class="k">val</span> <span class="nv">executor</span> <span class="k">=</span> <span class="nv">Executors</span><span class="o">.</span><span class="py">newFixedThreadPool</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="o">(</span><span class="n">r</span><span class="k">:</span> <span class="kt">Runnable</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span>
  <span class="k">val</span> <span class="nv">back</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Thread</span><span class="o">(</span><span class="n">r</span><span class="o">)</span>
  <span class="nv">back</span><span class="o">.</span><span class="py">setDaemon</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
  <span class="n">back</span>
<span class="o">})</span>

<span class="k">implicit</span> <span class="k">override</span> <span class="k">def</span> <span class="nf">executionContext</span><span class="k">:</span> <span class="kt">ExecutionContext</span> <span class="o">=</span> <span class="nv">ExecutionContext</span><span class="o">.</span><span class="py">fromExecutor</span><span class="o">(</span><span class="n">executor</span><span class="o">)</span>

<span class="k">implicit</span> <span class="k">override</span> <span class="k">def</span> <span class="nf">timer</span><span class="k">:</span> <span class="kt">Timer</span><span class="o">[</span><span class="kt">IO</span><span class="o">]</span> <span class="k">=</span> <span class="nv">IO</span><span class="o">.</span><span class="py">timer</span><span class="o">(</span><span class="n">executionContext</span><span class="o">)</span>

<span class="k">implicit</span> <span class="k">override</span> <span class="k">def</span> <span class="nf">contextShift</span><span class="k">:</span> <span class="kt">ContextShift</span><span class="o">[</span><span class="kt">IO</span><span class="o">]</span> <span class="k">=</span> <span class="nv">IO</span><span class="o">.</span><span class="py">contextShift</span><span class="o">(</span><span class="n">executionContext</span><span class="o">)</span>
</code></pre></div></div>

<p>If 2 consumers with empty queue happens to be scheduled on these 2 threads separately, it will block. If we change our <code class="language-plaintext highlighter-rouge">BlockingQueueApp</code> to the code in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/RealBlockingQueueApp.scala">RealBlockingQueueApp</a>:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">override</span> <span class="k">def</span> <span class="nf">work</span><span class="o">()</span><span class="k">:</span> <span class="kt">IO</span><span class="o">[</span><span class="kt">Unit</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="nc">Seq</span><span class="o">(</span>
    <span class="nf">dequeueStream</span><span class="o">().</span><span class="py">unNoneTerminate</span><span class="o">.</span><span class="py">parEvalMap</span><span class="o">(</span><span class="nv">config</span><span class="o">.</span><span class="py">batchSize</span><span class="o">)(</span><span class="n">consume</span><span class="o">).</span><span class="py">compile</span><span class="o">.</span><span class="py">drain</span><span class="o">,</span>
    <span class="nf">dequeueStream</span><span class="o">().</span><span class="py">unNoneTerminate</span><span class="o">.</span><span class="py">parEvalMap</span><span class="o">(</span><span class="nv">config</span><span class="o">.</span><span class="py">batchSize</span><span class="o">)(</span><span class="n">consume</span><span class="o">).</span><span class="py">compile</span><span class="o">.</span><span class="py">drain</span><span class="o">,</span>
    <span class="o">(</span><span class="nf">produceStream</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="py">map</span><span class="o">(</span><span class="nc">Some</span><span class="o">(</span><span class="k">_</span><span class="o">))</span> <span class="o">++</span> <span class="nv">fs2</span><span class="o">.</span><span class="py">Stream</span><span class="o">.</span><span class="py">emit</span><span class="o">(</span><span class="nc">None</span><span class="o">)).</span><span class="py">evalMap</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="nc">IO</span><span class="o">(</span><span class="nv">queue</span><span class="o">.</span><span class="py">put</span><span class="o">(</span><span class="n">x</span><span class="o">))).</span><span class="py">compile</span><span class="o">.</span><span class="py">drain</span><span class="o">,</span>
  <span class="o">).</span><span class="py">parSequence</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="k">_</span> <span class="k">=&gt;</span> <span class="o">())</span>
<span class="o">}</span>
</code></pre></div></div>

<p>Here we started two dequeue stream at first. Now the whole program will block when run it with <code class="language-plaintext highlighter-rouge">sbt "run -b"</code> .</p>

<p>The lesson learned here is that there is a big risk if any operation blocks the whole thread in cats effect. Even it doesn’t block the whole program, it may make a whole thread unavailable.</p>

<p>Actually in <a href="https://typelevel.org/cats-effect/docs/thread-model">Cats Effect’s thread model</a>, there is another thread pool for blocking tasks if we mark it explicitly. In <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/AsyncConsole.scala">AsyncConsole.scala</a>, I use this exact block mode to run console output so that it won’t effect other non blocking IO operations:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">asyncPrintln</span><span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">)(</span>
    <span class="k">implicit</span> <span class="n">cs</span><span class="k">:</span> <span class="kt">ContextShift</span><span class="o">[</span><span class="kt">IO</span><span class="o">],</span> <span class="n">blocker</span><span class="k">:</span> <span class="kt">Blocker</span><span class="o">)</span><span class="k">:</span> <span class="kt">IO</span><span class="o">[</span><span class="kt">Unit</span><span class="o">]</span> <span class="k">=</span> <span class="nv">blocker</span><span class="o">.</span><span class="py">blockOn</span><span class="o">(</span><span class="nc">IO</span><span class="o">(</span><span class="nf">println</span><span class="o">(</span><span class="n">s</span><span class="o">)))</span>
</code></pre></div></div>

<p>However, if a thread is blocked in this pool, it will start another thread for the next operation. Based on the document, there is no limit on how many threads will be created. So if the producer is much slower than consumer, there will be more and more consume operations blocked on dequeue, so it will generate a large amount of threads, which is not ideal and eventually even will blow up the memory.</p>

<h2 id="approach-3-use-cats-effect-friendly-queue">Approach 3: Use Cats Effect Friendly Queue</h2>

<p>What if we have a queue that only block the dequeue <code class="language-plaintext highlighter-rouge">IO</code> when empty instead of blocking the whole thread? Luckily, FS2 provides such a queue. (Cats Effect 3.x also provides such a queue). The implementation is basically the same as above (code in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/StreamQueueApp.scala">StreamQueueApp.scala</a>):</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">fs2.concurrent.Queue</span>

<span class="k">def</span> <span class="nf">work</span><span class="o">()</span><span class="k">:</span> <span class="kt">IO</span><span class="o">[</span><span class="kt">Unit</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="k">for</span> <span class="o">{</span>
    <span class="n">queue</span> <span class="k">&lt;-</span> <span class="nv">Queue</span><span class="o">.</span><span class="py">bounded</span><span class="o">[</span><span class="kt">IO</span>, <span class="kt">Option</span><span class="o">[</span><span class="kt">Int</span><span class="o">]](</span><span class="nv">config</span><span class="o">.</span><span class="py">batchSize</span> <span class="o">*</span> <span class="mi">2</span><span class="o">)</span>
    <span class="k">_</span> <span class="k">&lt;-</span> <span class="nc">Seq</span><span class="o">(</span>
      <span class="o">(</span><span class="nf">produceStream</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="py">map</span><span class="o">(</span><span class="nc">Some</span><span class="o">(</span><span class="k">_</span><span class="o">))</span> <span class="o">++</span> <span class="nv">fs2</span><span class="o">.</span><span class="py">Stream</span><span class="o">.</span><span class="py">emit</span><span class="o">(</span><span class="nc">None</span><span class="o">)).</span><span class="py">through</span><span class="o">(</span><span class="nv">queue</span><span class="o">.</span><span class="py">enqueue</span><span class="o">).</span><span class="py">compile</span><span class="o">.</span><span class="py">drain</span><span class="o">,</span>
      <span class="nv">queue</span><span class="o">.</span><span class="py">dequeue</span><span class="o">.</span><span class="py">unNoneTerminate</span><span class="o">.</span><span class="py">parEvalMap</span><span class="o">(</span><span class="nv">config</span><span class="o">.</span><span class="py">batchSize</span><span class="o">)(</span><span class="n">consume</span><span class="o">).</span><span class="py">compile</span><span class="o">.</span><span class="py">drain</span><span class="o">,</span>
    <span class="o">).</span><span class="py">parSequence</span>
  <span class="o">}</span> <span class="nf">yield</span> <span class="o">()</span>
<span class="o">}</span>
</code></pre></div></div>

<p>Run <code class="language-plaintext highlighter-rouge">sbt "run -n StreamAppQueue"</code> to see how it performs.</p>

<h2 id="approach-4-use-fs2-stream-directly">Approach 4: Use FS2 Stream Directly</h2>

<p>FS2 actually provides some advanced stream operations that makes it possible to combine the producing stream and consume stream, like the code in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/StreamApp.scala">StreamApp.scala</a>:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">produceStream</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="py">parEvalMap</span><span class="o">(</span><span class="nv">config</span><span class="o">.</span><span class="py">batchSize</span><span class="o">)(</span><span class="n">consume</span><span class="o">).</span><span class="py">compile</span><span class="o">.</span><span class="py">drain</span>
</code></pre></div></div>

<p>Here we map <code class="language-plaintext highlighter-rouge">consume</code> in parallel on <code class="language-plaintext highlighter-rouge">produce</code> stream. However, if you try to run <code class="language-plaintext highlighter-rouge">sbt "run -n StreamApp"</code> vs <code class="language-plaintext highlighter-rouge">sbt "run -n StreamQueueApp"</code>, you will find this is slower than before. This is because <code class="language-plaintext highlighter-rouge">produceStream</code> will give the next batch when the downstream asks. If we can prepare at least one batch before the downstream is free, we can save more time. Luckily, it’s very easy to do in fs2. As we can see in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/PrefetchStreamApp.scala">PrefetchStreamApp.scala</a>, we can add <code class="language-plaintext highlighter-rouge">prefetch</code> after the <code class="language-plaintext highlighter-rouge">produceStream</code>:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">produceStream</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="py">prefetch</span><span class="o">.</span><span class="py">parEvalMap</span><span class="o">(</span><span class="nv">config</span><span class="o">.</span><span class="py">batchSize</span><span class="o">)(</span><span class="n">consume</span><span class="o">).</span><span class="py">compile</span><span class="o">.</span><span class="py">drain</span>
</code></pre></div></div>

<p>It will prefetch a <a href="https://fs2.io/#/guide?id=chunks">chunk</a> of elements. Use <code class="language-plaintext highlighter-rouge">prefetchN</code> if you want to prefetch N chunks.</p>

<p>Then run this with <code class="language-plaintext highlighter-rouge">sbt "run -n PrefetchStreamApp"</code>, you will find the performance is similar as the queued approach.</p>

<p>Actually if you check the source code of <code class="language-plaintext highlighter-rouge">prefetch</code>, you will find the implementation is almost the same as ours:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">prefetch</span><span class="o">[</span><span class="kt">F2</span><span class="o">[</span><span class="kt">x</span><span class="o">]</span> <span class="k">&gt;:</span> <span class="kt">F</span><span class="o">[</span><span class="kt">x</span><span class="o">]</span><span class="kt">:</span> <span class="kt">Concurrent</span><span class="o">]</span><span class="k">:</span> <span class="kt">Stream</span><span class="o">[</span><span class="kt">F2</span>, <span class="kt">O</span><span class="o">]</span> <span class="k">=</span> <span class="n">prefetchN</span><span class="o">[</span><span class="kt">F2</span><span class="o">](</span><span class="mi">1</span><span class="o">)</span>

<span class="k">def</span> <span class="nf">prefetchN</span><span class="o">[</span><span class="kt">F2</span><span class="o">[</span><span class="kt">x</span><span class="o">]</span> <span class="k">&gt;:</span> <span class="kt">F</span><span class="o">[</span><span class="kt">x</span><span class="o">]</span><span class="kt">:</span> <span class="kt">Concurrent</span><span class="o">](</span><span class="n">n</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="kt">Stream</span><span class="o">[</span><span class="kt">F2</span>, <span class="kt">O</span><span class="o">]</span> <span class="k">=</span>
  <span class="nv">Stream</span><span class="o">.</span><span class="py">eval</span><span class="o">(</span><span class="nv">Queue</span><span class="o">.</span><span class="py">bounded</span><span class="o">[</span><span class="kt">F2</span>, <span class="kt">Option</span><span class="o">[</span><span class="kt">Chunk</span><span class="o">[</span><span class="kt">O</span><span class="o">]]](</span><span class="n">n</span><span class="o">)).</span><span class="py">flatMap</span> <span class="o">{</span> <span class="n">queue</span> <span class="k">=&gt;</span>
    <span class="nv">queue</span><span class="o">.</span><span class="py">dequeue</span><span class="o">.</span><span class="py">unNoneTerminate</span>
      <span class="o">.</span><span class="py">flatMap</span><span class="o">(</span><span class="nv">Stream</span><span class="o">.</span><span class="py">chunk</span><span class="o">(</span><span class="k">_</span><span class="o">))</span>
      <span class="o">.</span><span class="py">concurrently</span><span class="o">(</span><span class="nv">chunks</span><span class="o">.</span><span class="py">noneTerminate</span><span class="o">.</span><span class="py">covary</span><span class="o">[</span><span class="kt">F2</span><span class="o">].</span><span class="py">through</span><span class="o">(</span><span class="nv">queue</span><span class="o">.</span><span class="py">enqueue</span><span class="o">))</span>
  <span class="o">}</span>
</code></pre></div></div>

<h2 id="approach-5-make-producers-run-in-parallel">Approach 5: Make Producers Run in Parallel</h2>

<p>We’ve made it runs in parallel between consumers, also between consumers and producers. But we haven’t made producers run in parallel yet. With the queue, its very easy to do, just start multiple <code class="language-plaintext highlighter-rouge">IO</code>s for <code class="language-plaintext highlighter-rouge">produceStream.through(queue.enqueue)</code>. <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/ConcurrentProducerQueueApp.scala">ConcurrentProduceQueueApp.scala</a> is an example:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">private</span> <span class="k">val</span> <span class="nv">counter</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">AtomicInteger</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="k">override</span> <span class="k">def</span> <span class="nf">work</span><span class="o">()</span><span class="k">:</span> <span class="kt">IO</span><span class="o">[</span><span class="kt">Unit</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="k">for</span> <span class="o">{</span>
    <span class="n">queue</span> <span class="k">&lt;-</span> <span class="nv">Queue</span><span class="o">.</span><span class="py">bounded</span><span class="o">[</span><span class="kt">IO</span>, <span class="kt">Int</span><span class="o">](</span><span class="nv">config</span><span class="o">.</span><span class="py">batchSize</span> <span class="o">*</span> <span class="mi">2</span><span class="o">)</span>
    <span class="k">_</span> <span class="k">&lt;-</span> <span class="nc">Seq</span><span class="o">(</span>
      <span class="nf">produceStream</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="nv">config</span><span class="o">.</span><span class="py">totalSize</span> <span class="o">/</span> <span class="mi">2</span><span class="o">).</span><span class="py">through</span><span class="o">(</span><span class="nv">queue</span><span class="o">.</span><span class="py">enqueue</span><span class="o">).</span><span class="py">compile</span><span class="o">.</span><span class="py">drain</span><span class="o">,</span>
      <span class="nf">produceStream</span><span class="o">(</span><span class="nv">config</span><span class="o">.</span><span class="py">totalSize</span> <span class="o">/</span> <span class="mi">2</span><span class="o">,</span> <span class="nv">config</span><span class="o">.</span><span class="py">totalSize</span><span class="o">).</span><span class="py">through</span><span class="o">(</span><span class="nv">queue</span><span class="o">.</span><span class="py">enqueue</span><span class="o">).</span><span class="py">compile</span><span class="o">.</span><span class="py">drain</span><span class="o">,</span>
      <span class="nv">queue</span><span class="o">.</span><span class="py">dequeue</span><span class="o">.</span><span class="py">parEvalMap</span><span class="o">(</span><span class="nv">config</span><span class="o">.</span><span class="py">batchSize</span><span class="o">)</span> <span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span>
        <span class="nf">consume</span><span class="o">(</span><span class="n">x</span><span class="o">).</span><span class="py">map</span> <span class="o">{</span> <span class="k">_</span> <span class="k">=&gt;</span>
          <span class="nf">if</span> <span class="o">(</span><span class="nv">counter</span><span class="o">.</span><span class="py">incrementAndGet</span><span class="o">()</span> <span class="o">&gt;=</span> <span class="nv">config</span><span class="o">.</span><span class="py">totalSize</span><span class="o">)</span> <span class="o">{</span>
            <span class="nc">None</span>
          <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
            <span class="nc">Some</span><span class="o">()</span>
          <span class="o">}</span>
        <span class="o">}</span>
      <span class="o">}.</span><span class="py">unNoneTerminate</span><span class="o">.</span><span class="py">compile</span><span class="o">.</span><span class="py">drain</span><span class="o">,</span>
    <span class="o">).</span><span class="py">parSequence</span>
  <span class="o">}</span> <span class="nf">yield</span> <span class="o">()</span>
<span class="o">}</span>
</code></pre></div></div>

<p>It has 2 concurrent producers but in theory you can create as many as you want, just be careful with the parameters of <code class="language-plaintext highlighter-rouge">produceStream</code>.</p>

<p>If you run this with <code class="language-plaintext highlighter-rouge">sbt "run -n ConcurrentProduceQueueApp"</code>, you can find the performance is much better with slower producer. However, with the help of fs2 library, we can make the code cleaner without depends on any queue explicitly. Here is what I did in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/ConcurrentProducerApp.scala">ConcurrentProducerApp.scala</a>:</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">work</span><span class="o">()</span><span class="k">:</span> <span class="kt">IO</span><span class="o">[</span><span class="kt">Unit</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
  <span class="nv">fs2</span><span class="o">.</span><span class="py">Stream</span><span class="o">.</span><span class="py">emits</span><span class="o">(</span><span class="nc">Range</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">produceParallelism</span><span class="o">))</span>
    <span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">batch</span> <span class="k">=&gt;</span> <span class="nf">produceStream</span><span class="o">(</span>
      <span class="n">batch</span> <span class="o">*</span> <span class="nv">config</span><span class="o">.</span><span class="py">totalSize</span> <span class="o">/</span> <span class="n">produceParallelism</span><span class="o">,</span>
      <span class="o">(</span><span class="n">batch</span> <span class="o">+</span> <span class="mi">1</span><span class="o">)</span> <span class="o">*</span> <span class="nv">config</span><span class="o">.</span><span class="py">totalSize</span> <span class="o">/</span> <span class="nv">produceParallelism</span><span class="o">.</span><span class="py">toDouble</span><span class="o">))</span>
    <span class="o">.</span><span class="py">parJoin</span><span class="o">(</span><span class="n">produceParallelism</span><span class="o">)</span>
    <span class="o">.</span><span class="py">prefetch</span>
    <span class="o">.</span><span class="py">parEvalMap</span><span class="o">(</span><span class="nv">config</span><span class="o">.</span><span class="py">batchSize</span><span class="o">)(</span><span class="n">consume</span><span class="o">).</span><span class="py">compile</span><span class="o">.</span><span class="py">drain</span>
<span class="o">}</span>
</code></pre></div></div>

<p>Here we use <code class="language-plaintext highlighter-rouge">parJoin</code> to join multiple producer stream at the same time.</p>

<h2 id="more">More</h2>

<p>All the approaches above other than the first one uses a queue either implicitly or explicitly. However, under high parallelism and load, every job operating on a single queue may makes this queue a bottleneck. In this case, there is a <a href="https://en.wikipedia.org/wiki/Work_stealing">work stealing</a> algorithm that each consumers can has its own queue, and whenever a consumer’s queue is empty, it steal some tasks from another one. But it’s a little bit complex and unnecessary if the load is not so high, so I will not cover it in this article.</p>

<h2 id="test-results">Test Results</h2>

<p>Now let’s run all the approaches and compare the performance with <code class="language-plaintext highlighter-rouge">sbt "run -n"</code>. Here are the results:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>slow producer</th>
      <th>balanced</th>
      <th>slow consumer</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BatchIO</td>
      <td>11086.078637 ms</td>
      <td>29912.377578 ms</td>
      <td>10015.51878 ms</td>
    </tr>
    <tr>
      <td>BlockingQueue</td>
      <td>10190.038753 ms</td>
      <td>14195.228189 ms</td>
      <td>6495.333179 ms</td>
    </tr>
    <tr>
      <td>StreamQueue</td>
      <td>10138.016643 ms</td>
      <td>14458.443122 ms</td>
      <td>6418.078377 ms</td>
    </tr>
    <tr>
      <td>Stream</td>
      <td>10356.178562 ms</td>
      <td>15655.697826 ms</td>
      <td>6560.111697 ms</td>
    </tr>
    <tr>
      <td>PrefetchStream</td>
      <td>10141.110634 ms</td>
      <td>14578.362136 ms</td>
      <td>6376.628036 ms</td>
    </tr>
    <tr>
      <td>ConcurrentProduceresQueue</td>
      <td>5187.442452 ms</td>
      <td>14395.321922 ms</td>
      <td>6576.538821 ms</td>
    </tr>
    <tr>
      <td>ConcurrentProducer</td>
      <td>5198.723825 ms</td>
      <td>14544.247312 ms</td>
      <td>6418.078377 ms</td>
    </tr>
  </tbody>
</table>

<p>We can see approaches that parallelize all the parts win the performance game.</p>]]></content><author><name></name></author><category term="Scala" /><category term="concurrent" /><category term="cats" /><category term="cats-effect" /><category term="fs2" /><category term="queue" /><category term="stream" /><summary type="html"><![CDATA[All the source code mentioned in this blog can be found in my Github repo.]]></summary></entry><entry><title type="html">Upgrade Kubernetes from 1.23 to 1.24</title><link href="https://www.binwang.me/2023-05-22-Upgrade-Kubernetes-from-1.23-to-1.24.html" rel="alternate" type="text/html" title="Upgrade Kubernetes from 1.23 to 1.24" /><published>2023-05-22T00:00:00-04:00</published><updated>2023-05-22T00:00:00-04:00</updated><id>https://www.binwang.me/Upgrade-Kubernetes-from-1.23-to-1.24</id><content type="html" xml:base="https://www.binwang.me/2023-05-22-Upgrade-Kubernetes-from-1.23-to-1.24.html"><![CDATA[<p>In the <a href="/2023-03-13-Infrastructure-Setup-for-High-Availability.html">last blog post</a>, I introduced using Kubernetes to setup high available infrastructure. I had that setup a long time ago. I did the long overdue upgrade for Kubernetes from 1.23 to 1.24 recently. Since GlusterFS is <a href="https://github.com/kubernetes/kubernetes/pull/111485">deprecated</a>(though not removed) in 1.25, I have no plans to continue the upgrade without exploring alternative storage options.</p>

<p>There is a big change from 1.23 to 1.24 as well, namely, <a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">Docker Engine support has been removed</a>. I migrated the container engine to containerd. But the process is not without pain. I need to search different sources to fix the issues. So I list my upgrade steps so that if anyone has the same issue, this may help.</p>

<p>My Kubernetes cluster is set up locally with <code class="language-plaintext highlighter-rouge">kubeadm</code>. There is an <a href="https://v1-24.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">official upgrade guide</a> for kubeadm to upgrade from 1.23 to 1.24, but it doesn’t mention any steps to remove Docker and setup containerd. So here are the steps I took:</p>

<ol>
  <li>Add <code class="language-plaintext highlighter-rouge">--container-runtime-endpoint</code> option to kubelet. The way I did it is adding <code class="language-plaintext highlighter-rouge">KUBELET_ARGS="--container-runtime-endpoint=/run/containerd/containerd.sock"</code> to <code class="language-plaintext highlighter-rouge">/etc/kubernetes/kublet.env</code>. Without this, Kubelet will fail to start.</li>
  <li>Remove <code class="language-plaintext highlighter-rouge">--network-plugin=cni</code> from  <code class="language-plaintext highlighter-rouge">/var/lib/kubelet/kubeadm-flags.env</code>.</li>
  <li>Add the following configuration in <code class="language-plaintext highlighter-rouge">/etc/crictl.yaml</code>, otherwise kubeadm will not be able to pull needed images:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
</code></pre></div>    </div>
  </li>
  <li>Configure <code class="language-plaintext highlighter-rouge">SystemdCgroup</code> permission for containerd. Otherwise kube-apiserver will always be restarted because of “sandbox environment changes” (see more in <a href="https://github.com/kubernetes/kubernetes/issues/110177">Github issue</a>):
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo mkdir -p /etc/containerd/
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml
sudo systemctl restart containerd
</code></pre></div>    </div>
  </li>
  <li>Follow the <a href="https://v1-24.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">official upgrade guide</a>.</li>
  <li>After the upgrade, remember to restart Docker so that the old containers started by Docker will be stopped.</li>
</ol>]]></content><author><name></name></author><category term="Kubernetes" /><category term="container" /><category term="Docker" /><category term="Linux" /><summary type="html"><![CDATA[In the last blog post, I introduced using Kubernetes to setup high available infrastructure. I had that setup a long time ago. I did the long overdue upgrade for Kubernetes from 1.23 to 1.24 recently. Since GlusterFS is deprecated(though not removed) in 1.25, I have no plans to continue the upgrade without exploring alternative storage options.]]></summary></entry><entry><title type="html">Infrastructure Setup for High Availability</title><link href="https://www.binwang.me/2023-03-13-Infrastructure-Setup-for-High-Availability.html" rel="alternate" type="text/html" title="Infrastructure Setup for High Availability" /><published>2023-03-13T00:00:00-04:00</published><updated>2023-03-13T00:00:00-04:00</updated><id>https://www.binwang.me/Infrastructure-Setup-for-High-Availability</id><content type="html" xml:base="https://www.binwang.me/2023-03-13-Infrastructure-Setup-for-High-Availability.html"><![CDATA[<p><em>See <a href="/2023-11-28-Introduce-K3s-CephFS-and-MetalLB-to-My-High-Avaliable-Cluster.html">Introduce K3s, CephFS and MetalLB to My High Avaliable Cluster</a> for updates on this setup.</em></p>

<p>Cloud is popular these days. But sometimes we just want to host something small, maybe just an open source service for family and friends, or some self-built service that we are still experimenting on. In this case, the cloud can be expensive. We can just throw a few nodes at home and run it at a very low cost. But you don’t want the service down when some nodes failed, at least the service should be available when you upgrade and reboot the nodes because it can happen very frequently. In this article, I will talk about how to build high available infrastructure so that the service can be alive even when some nodes are down.</p>

<h2 id="what-is-high-availability">What is High Availability?</h2>

<p>Availability means a service is alive and can serve traffic. High availability (HA) means when some components of the system are down, the service is still alive. The failed components can be in different layers: it can be a region, a DC, a network, or some nodes. In this article, I’ll only talk about things including and above node level, since region and network are usually out of control for a small infrastructure setup. That means you can host the service on multiple machines, and it should still be alive even when some of the machines are down. This is the most useful case of HA in the case anyway since nodes can be down frequently because of OS or software updates.</p>

<p>Before I start with the real setup, I want to clear some myths about HA first. Maybe you’ve heard of the famous CAP theorem, which says only two of the three properties can be met at the same time: consistency, availability, and partition tolerance. Lots of people misinterpret it as a HA system that will sacrifice consistency during a failure. It is not true: the type of partition that makes you must choose between consistency and availability is very rare. In most well-designed HA systems, you can have both as long as more than half of the nodes are alive (alive also means reachable from clients). And HA doesn’t necessarily mean it prioritizes availability over consistency either: it just means it can handle more failure cases when keeping both consistency and availability. When there is a failure it cannot handle, it can choose to keep consistency and make the service unavailable. This is the type of HA I’m going to introduce in this setup.</p>

<p>So to make it clear, the HA goal in this setup is to make the services still alive without sacrificing consistency when we lose less than half of the nodes (either it’s partitioned from the network or actually dead).</p>

<h2 id="ha-setup">HA Setup</h2>

<p>As said before, the HA needs multiple nodes in case of some nodes are down. The setup in this article can tolerate less than half of the node loss. So if you want to have a HA that can handle 1 node loss, the whole system needs at least 3 nodes. There are lots of cheap used machines on the market that have enough power to host many open source services.</p>

<p>The HA setup has multiple layers and we will use different tools for each of the layers:</p>

<ul>
  <li>Compute: Kubernetes</li>
  <li>Storage: GlusterFk</li>
  <li>Database: Cockroach DB</li>
  <li>Network Ingress: Cloudflare Tunnel</li>
</ul>

<p>Here is an overview of the setup:</p>

<p><img src="/static/images/2023-03-13-Infrastructure-Setup-for-High-Availability/HA-self-hosted.png" alt="overview" /></p>

<p><em>Update at 2023-11-28: updated the diagram to include Keepalived.</em></p>

<p>Let’s talk about each of them in detail.</p>

<h2 id="compute-kubernetes">Compute: Kubernetes</h2>

<p><a href="https://kubernetes.io/">Kubernetes</a> is a container orchestration system. Think of it as Docker but across machines. You define what you want to run in the format of YAML or Json, including how much CPU, memory, and storage to use, then Kubernetes will find a node that fits your needs to run your container. It also tries to keep the current system state that meets your definition. For example, if there is a node failed and your service’s container is on it, Kubernetes will try to find another node to start the container so that the state meets the definition. So if the service itself doesn’t have any state between restarts, you get HA for free using Kubernetes.</p>

<p>I must have some warnings about using Kubernetes. It’s a complex project that is used by many big players. It’s not very easy to set up, maintain or upgrade. You need lots of knowledge to make it work. It has so many open issues that your particular needs are most likely not prioritized. While it’s open source so that you can modify the code to meet your use case, and I’ve had good experience contributing code in the early days, the recent experience is not so good anymore: you may need to attend some discussions to push your change instead of async online discussion. That is a lot for a causal contributor. So I end up maintaining a custom branch of Kubernetes with the changes I need locally, which is also a lot for average users.</p>

<p>Even though Kubernetes is heavy, I still think it’s a good tool even for small deployments since it’s already the industry standard. If you want to dedicate the maintenance of the Kubernetes cluster to a third party in the future, you can find lots of providers very easily. And you can just migrate your services to a different cluster without much effort since you’ve already defined the deployment in a language that any Kubernetes cluster can understand.</p>

<p>If you decide Kubernetes is the way to go, Kubernetes can be deployed with Kubeadm. Here is the <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/">official document</a> about how to deploy a Kubernetes cluster. Make sure to finish the <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/">HA setup</a> so that Kubernetes can survive even if some nodes are down.</p>

<p>Because of the nature of multiple nodes, every time the service restarts, the container can end on a different machine. So if your service needs to store anything on a disk, the data can get lost if you use the local disk of the machine. There are a few solutions for this:</p>

<ul>
  <li>Just don’t store data on a disk:
    <ul>
      <li>Store it in a database instead. But this is not an option for a service we don’t write and control.</li>
      <li>Send the files to another system. For example, you can send all the logs to something like Elastic Search so that it’s acceptable to lose logs after the container is restarted.</li>
      <li>Use <a href="https://kubernetes.io/docs/concepts/configuration/configmap/">ConfigMaps</a> for configuration files so that you can access them on any machine.</li>
    </ul>
  </li>
  <li>Bind the service to a specific node so that the container will run on that machine all the time. This needs the service itself to have HA built in so that when that node fails, the containers on the other nodes can still serve traffic. We will see an example of this in Cockroach DB setup.</li>
  <li>The last option is to use a distributed storage system that can be accessed from every machine. We will use GlusterFS for it in this setup.</li>
</ul>

<h2 id="storage-glusterfs">Storage: GlusterFS</h2>

<p><strong>WARNING: Gluster integration for Kubernetes has been removed since Kubernetes 1.26. You can use CephFS instead. Or check <a href="https://github.com/kadalu/kadalu/">Kadulu</a> if you still want to use GlusterFS.</strong></p>

<p><a href="https://www.gluster.org/">Gluster</a> is a distributed storage system. Once you created a GlusterFS volume, you can mount it to a machine just like NFS. The difference is the volume is backed by multiple machines so if even one of the machines fails, the volume is still usable. Kubernetes could mount a GlusterFS volume for containers as well. Sadly, Kubernetes has removed this support since version 1.26. But I’ve had this setup for a while and is still using an older version of Kubernetes, so I’ll still list GlusterFS as a solution here. The documents are still available for older versions. <a href="https://v1-24.docs.kubernetes.io/docs/concepts/storage/volumes/#glusterfs">Here is an example for Kubernetes 1.24</a>. You can select “versions” on the upper right to match your Kubernetes installation. CephFS is another distributed storage system, but it’s less user-friendly than GlusterFS in my opinion since the setup is more complex and it’s harder to mount it locally and explore it like a normal Linux file system. <a href="https://github.com/kadalu/kadalu/">Kadulu</a> seems to be another option if you still want to use GlusterFS, but I’ve never used it and I’m not sure if it’s production ready or not.</p>

<p>See <a href="https://docs.gluster.org/en/latest/Install-Guide/Overview/#what-is-gluster-without-making-me-learn-an-extra-glossary-of-terminology">the official install guide</a> for how to install Gluster and set it up. Most of the Linux distros already have the Gluster in the repo so you can install it by the package manager, and configure it based on the official document. Be aware you need to reserve a separate partition just for Gluster.</p>

<p>When creating a Gluster volume for use with Kubernetes, make sure to create it with at least 3 replicas so that you have HA for this volume. One of the replicas can be “<a href="https://docs.gluster.org/en/v3/Administrator%20Guide/arbiter-volumes-and-quorum/">arbiter</a>”, which means it’s only used for checking consistency and doesn’t store any actual data. So the data is only duplicated across 2 machines instead of 3 to save some space. Here is an example command to create such a volume:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo gluster volume create &lt;volume-name&gt; replica 3 arbiter 1 &lt;host1&gt;:&lt;glusterfs-path&gt; &lt;host2&gt;:&lt;glusterfs-path&gt; &lt;host3&gt;:&lt;glusterfs-path&gt;
</code></pre></div></div>

<h2 id="database-cockroachdb">Database: CockroachDB</h2>

<p>Even though we can make persistent work with distributed storage, it’s better to avoid it if possible because of the setup complexity and performance impact. (This is more of the case of a self-hosted solution, distributed storage from cloud providers is very easy to use, and is also used by the VM so there is no difference in performance). We’ve listed some options above. In this section, we will look at how to create a database for the services to use so that they don’t need to store data on disks.</p>

<p>Here I will use CockroachDB as an example. But this introduction should help you to set up other similar systems like Elastic Search. Cockroach DB is a distributed database that is compatible with PostgreSQL. It’s built with HA in mind, so it has good guarantees and is easy to set up. I’ve checked lots of HA solutions for PostgreSQL and all of them have less guarantee (lots of them have no information about the consistency and availability level they provide, and I found them half-baked with a closer look) while are much harder to set up. I’ve written <a href="/2018-07-29-A-Review-on-Spanner-and-Open-Source-Implementations.html">a blog about Spanner that also talks about Cockroach DB</a> if you are interested in more details. Overall I have a good impression of it: the tech writings are solid, and the support is nice: when I have an issue and report it in the forum, the response is usually very quick and useful even though I’m just a free user.</p>

<p>CockroachDB has <a href="https://www.cockroachlabs.com/docs/stable/kubernetes-overview.html">an official document</a> about how to install it on Kubernetes. It’s using <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a>. Here is <a href="https://raw.githubusercontent.com/cockroachdb/cockroach/master/cloud/kubernetes/cockroachdb-statefulset.yaml">one of the configurations it uses</a>. However, I still find there are too many limitations in StatefulSets so I deployed it in my own way:</p>

<ul>
  <li>Each CockroachDB instance is in its own StatefulSets with only 1 replica.</li>
  <li>Each of the instances is bound to the physical node with <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">PodAffinity</a>. So that each instance will only ever run on a specific host. In this way, we can just use the local disk as the storage because it will never run on a different host.</li>
  <li>Each CockroachDB instance has its own <a href="https://kubernetes.io/docs/concepts/services-networking/service/">service</a> defined so that they can communicate with each other.</li>
  <li>Copy the parameters from the official configuration and adjust them based on your use case.</li>
</ul>

<p>With a setup like this, it’s like installing CockroachDB on physical nodes but managed by Kubernetes. You don’t need to worry about distributed storage. When a node fails, a CockroachDB instance will also fail. But since CockroachDB itself has HA enabled, the whole CockroachDB cluster is still alive. Here is an example of the Kubernetes resources in my setup:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Pods

NAME                READY   STATUS    RESTARTS   AGE
pod/cockroach01-0   1/1     Running   1          4d11h
pod/cockroach02-0   1/1     Running   5          4d11h
pod/cockroach03-0   1/1     Running   0          4d11h

# StatefulSet

NAME          READY   AGE
cockroach01   1/1     298d
cockroach02   1/1     298d
cockroach03   1/1     298d

# Service

NAME                              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                          AGE
service/cockroach01-cockroachdb   ClusterIP   None           &lt;none&gt;        26257/TCP,8080/TCP               298d
service/cockroach02-cockroachdb   ClusterIP   None           &lt;none&gt;        26257/TCP,8080/TCP               298d
service/cockroach03-cockroachdb   ClusterIP   None           &lt;none&gt;        26257/TCP,8080/TCP               298d
service/cockroachdb               ClusterIP   10.96.70.142   &lt;none&gt;        26257/TCP,8080/TCP               269d
service/cockroachdb-public        NodePort    10.108.23.98   &lt;none&gt;        26257:30005/TCP,8080:30006/TCP   269d
</code></pre></div></div>

<p>You can see there is a separate StatefulSet for each of the CockroachDB instances, and a service for each of them for internal communications (with the name pattern <code class="language-plaintext highlighter-rouge">cockroach**-cockroachdb</code>). Service <code class="language-plaintext highlighter-rouge">cockroachdb</code> is for the use in Kubernetes cluster, and service <code class="language-plaintext highlighter-rouge">cockroachdb-public</code> is used by the service outside of the Kubernetes cluster (can be disabled if not needed) so that you can see the dashboard from your browser.</p>

<p>It may seem to have more Kubernetes definitions to write with such a method. But remember, while Kubernetes accepts YAML or Json format, how to prepare the definition can be flexible: you can use your favorite programming language to construct the definition and pass it to Kubernetes with a <a href="https://kubernetes.io/docs/reference/using-api/client-libraries/">client library</a>.</p>

<p>The upgrade of CockroachDB is very easy as well. Make sure to check the official release notes and upgrade guides first, but normally the upgrade is just to patch each of the StatefulSet with a newer version of Docker image, for example:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># run this command for every stateful set

kubectl patch statefulset cockroach01 \
--type='json' \
-p='[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"cockroachdb/cockroach:v22.2.5"}]'
</code></pre></div></div>

<h2 id="network-ingress">Network Ingress</h2>

<p>Once we have everything deployed in the cluster, the last step is to expose our service to the public Internet so that everyone can use it. Here we list two options based on the use case.</p>

<h3 id="cloudflare-tunnel">Cloudflare Tunnel</h3>

<p><a href="https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/">Cloudflare Tunnel</a> is basically a reverse proxy that forwards the traffic from the public Internet to your service. There is a daemon called cloudflared running in Kubernetes. Cloudflare will forward the traffic from clients to cloudflared and cloudflared will forward the traffic to the actual service. Check <a href="https://developers.cloudflare.com/cloudflare-one/tutorials/many-cfd-one-tunnel/">this doc</a> to see how it works with Kubernetes.</p>

<p>The upside of Cloudflare tunnel is that you don’t need to open any port to the public Internet at all. So it’s safer because there is no way to access your service without going to Cloudflare first. Cloudflare also provide some tools to mediate attacks like DDoS.</p>

<p>The downside is it depends on a third-party provider. And it can see all the traffic. It only supports limited protocols. So if you want to avoid Cloudflare seeing your traffic or have a protocol that is not supported, you need a more generic way to do it.</p>

<h3 id="nodeport-with-virtual-ip-and-dynamic-dns">NodePort with Virtual IP and Dynamic DNS</h3>

<p>We need to really open a port to the Internet without something like Cloudflare Tunnel. First, we need to open a port on our nodes, this can be done by defining <a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport">NodePort</a> in Kubernetes’ service.</p>

<p>Once we have the port opened on the nodes, we need to open it to the Internet. How to do it depends on the Internet provider. Usually, you should be able to set up a port mapping from the router to an internal IP for a node. However, to make the setup HA, we shouldn’t map the port just to a single node since that single node can be down, we can set up Keepalived so that there is a virtual IP that always maps to a live node. If you’ve <a href="https://github.com/kubernetes/kubeadm/blob/main/docs/ha-considerations.md#options-for-software-load-balancing">set up HA for Kubernetes with Keepalived and HAProxy</a>, you should be already familiar with how to set it up.</p>

<p>When you open a NodePort, make sure you’ve configured all the protections like authentication and encryption since beyond that it’s public Internet and anyone can access it.
You may want to run Nginx or HAProxy in the Kubernetes cluster, use it as a reverse proxy and only expose it to the Internet so that it’s safer and you have more control over the public traffic.</p>

<p>The client also needs a way to find the IP address of your network. Depending on the Internet provider, the IP address can change from time to time. So we need dynamic DNS to bind the changing IP to a fixed DNS. <a href="https://github.com/ddclient/ddclient">ddclient</a> can do it automatically and supports lots of domain name providers.</p>

<p>After all of this, your service is open to the public Internet and can be accessed by anyone. But if desired, you can still use Cloudflare DNS with proxy enabled, so that the client will send requests to Cloudflare first and you can get protections from Cloudflare. In this case, since the SSL is terminated inside the Kubernetes cluster, Cloudflare will not be able to see the actual payload of the traffic.</p>]]></content><author><name></name></author><category term="Kubernetes" /><category term="GlusterFS" /><category term="CockroachDB" /><category term="tech" /><category term="high availability" /><summary type="html"><![CDATA[See Introduce K3s, CephFS and MetalLB to My High Avaliable Cluster for updates on this setup.]]></summary></entry><entry><title type="html">My 2022 in Review</title><link href="https://www.binwang.me/2023-01-26-My-2022-in-Review.html" rel="alternate" type="text/html" title="My 2022 in Review" /><published>2023-01-26T00:00:00-05:00</published><updated>2023-01-26T00:00:00-05:00</updated><id>https://www.binwang.me/My-2022-in-Review</id><content type="html" xml:base="https://www.binwang.me/2023-01-26-My-2022-in-Review.html"><![CDATA[<p>It’s 2023 now! 2022 has ended. It has been 2 years since I wrote <a href="/2021-01-26-My-2020-in-Review.html">the last yearly review</a>. So this article would be more like 2021 and 2022 in review.</p>

<p>There are lots of things that happened in the last two years, if not more than the two years of 2019 and 2020. That includes both my personal life and work life, both individual level and national level. So let’s talk about them one by one.</p>

<p>My personal life has changed a lot in the past two years. My wife and I obtained permanent resident status in Canada in mid-2021, and bought our first home in Toronto. Those two things are a big relief to me: I lived in Beijing for more than 7 years since graduating from university, but I never saw it as a long-term home. Needless to say, the apartments in Beijing are too expensive. There are also lots of restrictions because of the Hukou system: Hukou basically means “citizenship” for a region in China. Without it, the biggest problem is the education of the children. While the children can enroll in schools in Beijing and be educated there, they still need to take Gaokao (the national college entrance examination) as a resident of my hometown, where their Hukou belongs (no, being born in Beijing wouldn’t get them a Beijing Hukou). The eligible scores for colleges vary by region, since the number of students admitted from each region is different and each region’s population is also different. In this case, good colleges usually admit much fewer students from my hometown province, even though the province has a much larger population. As a result, a much higher score is needed to be accepted by the same college. In my opinion, this puts the children into an unfair game since the education they received is not prepared for such a level of competition. (It’s already an unfair game for the students from regions like my hometown province, but for better or for worse, the education system there is prepared for it). While Gaokao is still the way for most Chinese to enroll in a college, there are some other options like studying aboard, which requires studying in a private school for preparation. But all the other paths need much more financial support. This is only the biggest problem, not to mention other inconveniences without Hukou like retirement or even buying a home or a car. In general, without it, you won’t be treated equally by the government, even though you’ve paid lots of tax and social insurance. You may ask, why not get one if it’s so important? Because it’s even harder than immigrating to another country. The only practical way other than marriage is to join a state-owned company or a government department just after graduating from college. Once you miss that opportunity, it’s nearly impossible for average people to get it.</p>

<p>Anyway, even if I can solve all the problems above, Beijing has changed in the last few years when I was there, to the level that I don’t want to live there. The government started to focus on Beijing’s role as China’s capital, aiming for a place like Washington. There is even a “one thousand years” plan to move non-capital functions to a small town called Xiong’an. However, Beijing is not a city that is built for the pure purpose of political capital from scratch. It has a long history, multiple functions, and diverse residents. But the average people are not a priority in the government’s decision. It started to ban commercial usage at the lower level of resident buildings (such commercial usage was encouraged during Beijing’s 2008 Olympic games to make the city more vibrant), demolish restaurants and shopping malls, ban street foods, and so on. Lots of historical buildings were refurbished with shiny walls and unified shop signs. The policy reached an extreme when it began to banish “low-end” people (the phrase “clean up low-end people” was literally used in official publications and slogans). During the winter of 2017, using a fire incident as an excuse, the government started a movement to demolish “slums”, and ban some of the rental rooms which have the proper licenses. The movement caused lots of people didn’t have a place to live during the cold winter. As a result, the city as a whole seems to be dying. While the parks were grander, the streets were also quieter, and more people were leaving. It’s so sad to see a city I lived and loved transferred to a place like that. But my view doesn’t represent the view of big brother. Cai Qi, the leader of Beijing at the time (the leader of a Chinese city is not the mayor, instead, it’s the head of that city’s communist party), used to be seen as an open-minded official because of his interaction with people on social media (which is very rare in China), a close comrade of Xi, entered the standing committee last year and became one of the most powerful political figures in China.</p>

<p>Another bigger event also happened in the same winter. It made me not only want to leave Beijing but also China itself. It was an evening with a beautiful sunset, I went for a walk with my girlfriend to Dong Zhi Men – East Straight Gate of ancient Beijing, a place with a combination of historical buildings, vibrant shopping malls, and sky crawlers. In front of a shopping mall, I saw a large billboard displaying news of amendments to China’s Constitution. The news was pure text with a large font size, which seemed out of place compared to the modern buildings nearby. It’s only a draft of amendments, but there is an important one that caught my eye: it removes the presidential term limits! I’ve never had high hopes for China’s democracy. When some people talk about the ingenuity of Deng Xiaoping’s design of skip-generation appointments for power transfer, I always think it’s childish. However, I never thought the transfer of power would break so quickly. In my opinion, it’s like the last barrier that prevents China from becoming a pure dictatorship country again. Once that barrier is broken, it would lead to a downfall in the foreseeable future. Even if it’s not the last barrier, the change shows Xi’s ambition to become a dictator in the most obvious way, which will take China in the wrong direction.</p>

<p>It was a cold winter day when the draft was passed. Living in a rented apartment, which is close to a military compound and just less than a 30-minute drive to the power center of China, I wrote down the following sentence after a sleepless night:</p>

<blockquote>
  <p>京城一夜风吹雪，万籁无声国岁寒。</p>
</blockquote>

<blockquote>
  <p>Beijing at night, wind and snow blowing,</p>

  <p>All creatures silent, the nation’s cold in growing.</p>
</blockquote>

<p>It seems weird to write about so many things that didn’t happen in the last two years. But they are important contexts to understand the significance of being a permanent resident of Canada. By no means Canada or Toronto is perfect. It has its own problems, but I’d rather live in a sociality where people can discuss the problems and hold the government accountable. So after moving to Toronto, I feel like this is a place that I can live and call home, and finally made it happen. Now, we’re having a baby on the way, which makes me excited and nervous at the same time.</p>

<p>Also because of the PR status, I was able to change the job without worrying about the visa. I left Amazon mainly because the on-call was too stressful to the point where it affected my personal life. I moved to a smaller company and started to do database related stuff again. The workload is much lighter than at Amazon, and I feel I have recovered a lot of energy because of it.</p>

<p>Outside of work, I continued to work on my side project which I mentioned in 2020’s year-end review. It’s now a usable product called <a href="https://rssbrain.com">RSS Brain</a>. It’s an RSS reader. I’ve mentioned it in past blogs so I will not go into details again. But it has been the app that I use the most every day. And it feels so good to write software that meets my own needs and maybe able to help others at the same time. I will eventually open source it, but before that, I still have some plans that need to be finished first. So open source the code became a lower priority. The development has been slowed down after most of the features have been finished: that’s a pattern of my past projects. But for the past projects it often slowed down before they were actually usable. At least for RSS Brain, it’s a usable app now. Hopefully, I still have the passion to continue developing it and finish all of my planned features.</p>

<p>While developing RSS Brain, I also developed and open sourced a <a href="/2022-05-02-A-Library-to-Make-It-Easier-to-Use-Scala-with-GRPC.html">Scala library</a> that makes writing gRPC service with Scala much easier. I’m very happy about the library because of its non-invasive nature. Even though I can see it’s controversial in some projects, I feel like it’s a very useful library that can help a lot in my future projects.</p>

<p>Another thing I want to review, which I value a lot, is this blog. In my opinion, ideas and the interaction of ideas are the most important factor for the advancement of human beings. The most important platforms that I can use to share ideas are open source projects and articles. Work is valuable, but I don’t think the work I’m doing can inspire lots of people because not a lot of people can see it. So I always treat this blog as an important platform for me to contribute to the world, even though it’s small, it shares ideas directly. Who knows if someone can get some inspiration from it, had some ideas on top of it then inspires someone else, and eventually create something revolutionary. In the past two years, I have posted a few articles. The frequency is neither too high nor too low. Quality is similar: neither too high nor too low overall. On some level, I’ve happy I left something, but at the same time, I feel like there is still a lot of room for improvement. There are some topics I wanted to write about but ended up not doing it. One factor is the RSS Brain project I was working on, but I still think there is still a lot of time I’ve wasted which can be used on writing articles.</p>

<p>Other than the output of knowledge and ideas, there is also the input part. One important source is reading books. My readings decreased a lot in the past two years in terms of both quantity and quality. Part of it may be because of working from home: I’ve read lots of books during my commute. I wasted too much time watching videos that didn’t even bring me relaxation or happiness in the end. I should be more aware of that and remember to find some books to read when I’m bored.</p>

<p>Other than the things that happened to me or around me, the events that happened in China also greatly affected me: the Covid restrictions became more and more extreme, preventing me from visiting my family for more than 3 years and greatly impacting the daily life of Chinese people. People were controlled by massive testing and surveillance in the guise of monitoring Covid cases. Cities implemented absurd lockdowns from time to time. The level of lockdown is so strict that lots of people were not able to get enough food or basic medicine. Some people were dead because of the lack of health care. Some people killed themselves because of depression. People are transferred into quarantine centers. Parents and kids were forcibly separated. The better quarantine centers are built from sports centers that have 24-hour overhead lights, the worse ones can be just outdoor parking lots or even public washrooms. During the transfer, some of them were packed onto trains without enough capacity, leading to people sleeping on luggage racks. There was even an incident that happened on a transfer bus, which killed more than 20 people. Government workers went into people’s homes to sterilize, which often destroys furniture and kills pets. Because of the lack of food and infrastructure, people fled lockdown regions on foot, walking tens of miles to airports and train stations, and in some cases, even hundreds of miles on the highway to their hometowns. Eventually, a fire broke out in Xinjiang and killed ten people, because they were locked into the building and were unable to escape. This tragedy sparked protests all around China. People held blank papers to protest without a word, but everyone, including the police and the government, knew what they wanted to say. So some of them were beaten and arrested. It’s like a dark Soviet Union joke but happened in real life China. Then, suddenly, all the Covid restrictions were lifted without any preparation. This led to a medicine shortage and a surge in Covid cases. Everyone I knew in China got Covid, except for those who stayed at home all the time. mRNA vaccines are still not approved because it’s not produced domestically. The strong government which controlled every aspect of people’s life during lockdowns are missing now, the cases and deaths are not even properly counted anymore. Everyone is left in the dark to fight for themselves.</p>

<p>There are so many tragedies that happened in China in the past two years that it’s impossible to write all of them down in this short article. Almost everyone’s life is affected by the bad economy if not by the Covid policies directly. Most of my friends and family members are in China, and my culture is also from there. Seeing the events unfold during the past two years has made me heartbroken. It also makes me frustrated because there is nothing much I can do. As the new year comes, hope everything can be better. And I’ll think more about what I can do, even if it’s just the smallest help.</p>]]></content><author><name></name></author><category term="life" /><summary type="html"><![CDATA[It’s 2023 now! 2022 has ended. It has been 2 years since I wrote the last yearly review. So this article would be more like 2021 and 2022 in review.]]></summary></entry><entry><title type="html">How RSS Brain Shows Related Articles</title><link href="https://www.binwang.me/2022-12-03-How-RSS-Brain-Shows-Related-Articles.html" rel="alternate" type="text/html" title="How RSS Brain Shows Related Articles" /><published>2022-12-03T00:00:00-05:00</published><updated>2022-12-03T00:00:00-05:00</updated><id>https://www.binwang.me/How-RSS-Brain-Shows-Related-Articles</id><content type="html" xml:base="https://www.binwang.me/2022-12-03-How-RSS-Brain-Shows-Related-Articles.html"><![CDATA[<p>In the new version of <a href="https://rssbrain.com">RSS Brain</a>, I added a new feature to show related articles from folders or feeds of your choice, instead of only show related articles from all feeds.</p>

<p>I have mentioned this feature in the <a href="/2022-10-29-RSS-Brain-Yet-Another-RSS-Reader-With-More-Features.html">previous blog post</a>. I also mentioned the algorithms in RSS Brain will be transparent. So in this article, I will talk about the details of this feature, how it can be used, the algorithm that backs it and how RSS Brain implements it.</p>

<h2 id="what-is-the-feature">What is the Feature</h2>

<p>The feature is very straightforward: when you are reading an article from a feed, RSS Brain will show related articles at the end. It’s a very common feature. What makes RSS Brain different are two things:</p>

<ol>
  <li>You can configure where the related articles come from.</li>
  <li>The implementation is transparent. That includes both the algorithm, which I will introduce in this article, and the code, which I will open source it in the future.</li>
</ol>

<p>How do you configure the related articles? By default, there will be no recommended articles. But there is a button to let you add a recommendation section at the end of an article:</p>

<p><img src="/static/images/2022-11-27-How-RSS-Brain-Show-Related-Articles/screenshot_add_section.png" alt="screenshot-add-section" /></p>

<p>Once you click the “Add More” button, it will show all your folders and feeds, with another “All Subscriptions” option. If you select “All Subscriptions”, this recommendation section will find related articles from all your subscriptions. If you select a folder or feed, it will find them from the folder or feed of your choice.</p>

<p>You can add multiple recommendation sections. After that, each recommendation section will be shown after the article. The screenshot below is an example that has a section that shows related articles from a folder called “local-form”, and another section that shows related articles from all the user’s subscriptions.</p>

<p><img src="https://rssbrain.com/images/screenshot_multi_recommend.png" alt="screenshot-sections" /></p>

<p>The recommendation configuration is attached to the feed that this article belongs. So if you read another article from the same feed, it will show related articles from the same recommendation sections.</p>

<h2 id="why-the-feature-is-useful">Why the Feature is Useful</h2>

<p>The first way I use it is to find more discussions about this article. For example, you can configure it to show related articles from Hacker News, some Twitter account or from a sub Reddit. So that you know what other people think about this article, or about this topic.</p>

<p>Another way to use it is to check the coverage from different sources. For example, you can add a recommendation section that contains left wing media and another section that has right wing media, so that you can compare the coverage and get a whole picture.</p>

<p>Last but not least, the recommendation is useful in its traditional way: just show related articles about the same topic so that you can read more details about the same topic. I often just read folders that has high quality sources, and when I want to know more, I will add a recommendation section that has more sources and find articles to read from there.</p>

<h2 id="how-the-feature-is-implemented">How the Feature is Implemented</h2>

<p><em>Update: see <a href="/2023-11-14-Update-On-RSS-Brain-to-Find-Related-Articles-with-Machine-Learning.html">this blog</a> for updated algorithm.</em></p>

<p>The algorithm to find recommended feature is content based instead of user based. RSS Brain doesn’t collect any user’s information in order to make personalized recommendation. It just find the related articles by how similar they are.</p>

<p>Each of the article can be represented by a term vector. The values in this vector are scores of the terms. For example, if article A has the content of “apple boy cat” and article B has the content of “apple boy dog”, the term vectors for each of the articles can be:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   apple, boy, cat, dog
A: [0.5,  0.5,   1,   0]
B: [0.5,  0.5,   0,   1]
</code></pre></div></div>

<p>The score is computed by <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">tf-idf</a>, which is basically a score considers both the frequent of the term in this article, and the frequent in all the articles: the more frequent it is in this article, the bigger the score since it can better represent the article. However the more frequent it is in all the articles, the score should be smaller since it’s not unique enough to represent the feature of this article. Once we have a term vector for each of the article, we can find the similarity by counting the distance between these vectors.</p>

<p>So we have the algorithm, how RSS Brain implements it in the code? We are using <a href="https://www.elastic.co/">ElasticSearch</a> under the hood. It’s widely used and open source. So for the APIs that RSS Brain is using, you can check the code for implementation details if you want. It has <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-termvectors.html">an API to find the term vector</a> for an article, and we use the scores in the term vector to do a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-boosting-query.html">boosting query</a>, which means do a search with different weights for each query term. For example, in the example above, if we want to find related articles for article A, we would find its term vector first, then convert the term vector into a boosting query that searches related articles by the query <code class="language-plaintext highlighter-rouge">apple^0.5 boy^0.5 cat^1</code>.</p>

<p>There are more details in this implementation, like adding filter on feed or folder in the query, limiting the term vector size and so on. The details can be found in the code once it’s open sourced. But the main idea doesn’t change.</p>

<p>With this simple and content based recommendation algorithm, instead of letting a black box AI decides what content are shown to you, I believe users can understand why an article is recommended, and judge whether the recommendation can benefit them or not.</p>]]></content><author><name></name></author><category term="RSS" /><category term="project" /><category term="RSS Brain" /><category term="digital life" /><summary type="html"><![CDATA[In the new version of RSS Brain, I added a new feature to show related articles from folders or feeds of your choice, instead of only show related articles from all feeds.]]></summary></entry><entry><title type="html">RSS Brain: Yet Another RSS Reader, With More Features</title><link href="https://www.binwang.me/2022-10-29-RSS-Brain-Yet-Another-RSS-Reader-With-More-Features.html" rel="alternate" type="text/html" title="RSS Brain: Yet Another RSS Reader, With More Features" /><published>2022-10-29T00:00:00-04:00</published><updated>2022-10-29T00:00:00-04:00</updated><id>https://www.binwang.me/RSS-Brain-Yet-Another-RSS-Reader-With-More-Features</id><content type="html" xml:base="https://www.binwang.me/2022-10-29-RSS-Brain-Yet-Another-RSS-Reader-With-More-Features.html"><![CDATA[<p>I’ve written yet another RSS Reader called RSS Brain recently. If you just want to take a quick look at the main features and try it, <a href="https://rssbrain.com/">the official website</a> is the best place to start. Though most of the main features are already finished and I’ve used it personally for a few months, be aware it’s still in beta stage and hasn’t been tested by a larger group yet. So please let me know if you run into any issues. I will open source it as well in the future (for non commercial usage) but it’s not ready yet for now.</p>

<p>In this article, I’ll give an introduction to the motivation behind it. You may have a better reason to try it after read this.</p>

<p>RSS is just a protocol to aggregate news. But how to organize them depends on the RSS reader.  While lots of RSS readers have the basic organization features like folders to manage the feeds, it is not enough. Not a lot of them have taken new technologies from fields like information retrieval and machine learning. Even a few them have done that, they are using some complex and black box algorithms which we don’t know what is going on behind the sense. I’ve written <a href="/2020-08-02-What-Is-Wrong-abount-Recommendation-System.html">an article</a> that talks about the harm of letting black box algorithm decide what we read. So in RSS Brain, I use transparent algorithms to help organize the feeds and articles without use clicks or read time as the optimization target, so that we can have a good understanding about why it behaves in that way, in that way we can decide whether it’s good for us or not.</p>

<p>Here are some pain points in the traditional RSS readers and how I solve them in RSS Brain with transparent algorithms.</p>

<h2 id="1-traditional-rss-readers-dont-rank-articles-by-weight">1. Traditional RSS Readers Don’t Rank Articles by Weight</h2>

<p>The mainstream RSS protocol don’t have a field to indicate how important an article is in a feed. It is very different from news papers or news websites, which have head line for the most important news. When it comes to RSS, there is no difference between the importance of the articles. It is a smaller problem when there are not so many articles in a feed, but for feeds that have lots of articles, or forums like Reddit and Hacker News, that is a very big problem.</p>

<p>Reddit and Hacker News sort the articles by both votes and timeline. The algorithm is relatively transparent (although less and less transparent in the case of Reddit). Some people don’t want the rank to be effected by other users at all, but I’m okay for other this kind of ranking for these reasons:</p>

<ol>
  <li>The posts are just too many to read them all without some kind of priority.</li>
  <li>The community is part of the forum. If I like a subreddit, it means I trust the community and mods to promote high quality posts. Otherwise I will just not subscribe to it.</li>
  <li>If you think about it, the traditional media also rank news for you, even khough it is selected by more professional people. But like I said in point 2, if you trust the community of a subreddit, then it doesn’t make a big difference.</li>
</ol>

<h3 id="11-ranking-algorithm">1.1 Ranking Algorithm</h3>

<p>Some readers have ranking algorithms to sort the articles for you, but those are mostly black box algorithms, which is harmful like I said before. In RSS Brain, we will take the votes from the source, and sort it with an algorithm that is similar to Reddit:</p>

\[S_{vote} = log_{10}v\]

\[S_{time} = { time \over C }\]

\[S = S_{vote} + S_{time}\]

<p>Which <code class="language-plaintext highlighter-rouge">v</code> means how many votes this article has. <code class="language-plaintext highlighter-rouge">time</code> means when this article is posted. <code class="language-plaintext highlighter-rouge">C</code> is a constant number that indicate how much wait time contributes to the whole score. I’m using 12.5 hours in my implementation.</p>

<p>For \(S_{vote}\), it means the first 10 votes will get the most weights, the next 100 votes has the same weight as the first 10 votes, and so on. It’s not a perfect algorithm but works good enough, and is easy to implement. Most importantly it’s very easy to be understood.</p>

<p>Once you have the score, you have the option to sort the articles in a folder or feed based on score instead of time.</p>

<p><img src="https://www.rssbrain.com/images/sort_post.png" alt="ranking" /></p>

<h3 id="12-data-source">1.2 Data Source</h3>

<p>Another obvious problem is how to get the votes. The traditional RSS protocols don’t have a specific field for vote count, luckily atom protocol has the ability to extend it with custom tags. So RSS Brain will parse these tags in <code class="language-plaintext highlighter-rouge">&lt;entry&gt;</code> tag if exists: <code class="language-plaintext highlighter-rouge">&lt;*:comments&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;*:upvotes&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;*:downvotes&gt;</code>. <code class="language-plaintext highlighter-rouge">*</code> can be any namespace. I’ve added <a href="https://docs.rsshub.app/en/joinus/quick-start.html#submit-new-rss-rule-code-the-script-produce-rss-feed-interactions">these fields</a> to a very popular RSS generator <a href="https://rsshub.app/">RSSHub</a> in namespace <code class="language-plaintext highlighter-rouge">RSSHub</code>. So if someone include these fields when implement a RSS, RSS Brain will be able to parse it and use <code class="language-plaintext highlighter-rouge">upvotes - downvotes</code> as the votes. If they are not available, RSS Brain will try to use <code class="language-plaintext highlighter-rouge">comments</code> instead.</p>

<p>Reddit and HackerNews are two of my main daily reading websites, and I believe it’s the same for many other people, so I also included an implementation to fetch the posts from Reddit and HackerNews JSON API. You can just input HackerNews or subreddit URL when add a new source, it will has an option to use the JSON feed when it tries to find the feeds. I know it’s not a very standard way, but it’s easier for me to implement rather than do it in RSSHub. Once there is a RSS implementation that contains the fields above, you have no trouble to use the RSS feed.</p>

<p>Since RSS Brain is parsing the tags in the RSS feed, for data sources other than forum, the RSS generator can also try to generate the votes in some way if necessary. For example, some score based on the article position of the website, the font size and so on. I haven’t done any experiment with it yet but I think it’s an interesting idea to explore.</p>

<h2 id="2-filter-articles-with-search-terms">2. Filter Articles With Search Terms</h2>

<p>Ranking the articles is one way to get the interested aritcles pop up. Another way is to filter the articles: sometimes we only care about a topic in a feed. In traditional RSS readers, there is no easy way to filter the topic out if there is no feed for that topic. In RSS Brain, you can define a search term on a folder, so that when you check the articles in this folder, it will only show the articles that matches the search term.</p>

<p>For example, you’ve got a few news feed, but only care reports about the war between Ukraine and Russia, then you can just set the search term as <code class="language-plaintext highlighter-rouge">"Ukraine" AND "Russia"</code> on the folder and enable search filter. After that, when you click on the folder to see articles, it will only show the articles about those news.</p>

<p><img src="https://www.rssbrain.com/images/filter_folder.png" alt="filter folder" /></p>

<p>By the way, you can also just search in a folder or source. This is very useful for me to search some news, since the search quality of search engineers for some news is very bad, especially for Chinese news, which very suspicious news agencies are always shown in the top results.</p>

<h2 id="3-show-related-articles">3. Show Related Articles</h2>

<p>This is a feature I found to be pretty interesting while Google News included it. But Google News selects the news source for you. In RSS Brain, you get get related articles from the feeds you subscribed, so that you can decide which sources are valuable to you instead of letting a big corp decide that.</p>

<p>Currently the implementation only has the ability to select from all your subscribed feeds. But even with this implementation, I find it is very useful in some ways:</p>

<ul>
  <li>When I check the news, the related articles will show a post related to this in forums like Reddit and HackerNews, so that I can check other people’s opinion.</li>
  <li>I can compare the report coverage from both left and right news agencies, to get a whole picture. I don’t use this feature a lot myself, but I know a lot of people like that and there are some popular apps do just that.</li>
  <li>I usually read news from a high quality source. But if I find some news to be interesting, I’d like to read more coverage on that. Those coverage doesn’t need to be high quality but might have more details.</li>
</ul>

<p>I have plan to extend this feature. (<em>Update: this feature has been implemented. Check the <a href="/2022-12-03-How-RSS-Brain-Shows-Related-Articles.html">blog post here</a>.</em>) It will be much better after that: RSS Brain will support related articles groups, so that you can show related articles not only from all the subscriptions, but also from the folders of your choice. So you can configure it to show related forum posts in one group, left/right news coverage on another group, and so on. I’ll also implement a feature to show related articles from a time range, so that it can filter the matches based on time, to make the news more related if you want.</p>

<h2 id="4-just-want-to-write-something">4. Just Want to Write Something</h2>

<p>Last but not least, I just want to write something and feel the happiness of coding. I really enjoy myself a lot while writing this project: coding, choosing the tech stack to use, setting up high availability cluster and database, and so on. Everything is so elegant because I can decide how to do it.</p>

<p>While enjoy myself during the development of the project, I find the product is pretty useful as well. So I decided to share it, and if it benefits more people I’m happier. As I said at the beginning of the article, I’ll open source the whole project and allow non commercial usages when it’s ready. But at the mean time, you can pay a monthly fee to use the software. The reason of this payment mode is two fold: I don’t want this software to be flooded with terrible ads, but I still need some revenue to keep the infrastructure running (for both hardware cost and my time). So I want to set some barriers at first to limit the user base, so that the users can have a better experience. Hope you enjoy this app and let me know if you run into any issues or have any question.</p>]]></content><author><name></name></author><category term="RSS" /><category term="project" /><category term="RSS Brain" /><category term="digital life" /><category term="news" /><summary type="html"><![CDATA[I’ve written yet another RSS Reader called RSS Brain recently. If you just want to take a quick look at the main features and try it, the official website is the best place to start. Though most of the main features are already finished and I’ve used it personally for a few months, be aware it’s still in beta stage and hasn’t been tested by a larger group yet. So please let me know if you run into any issues. I will open source it as well in the future (for non commercial usage) but it’s not ready yet for now.]]></summary></entry><entry><title type="html">Handle Apple In-App-Purchase Server Notification with Scala/Java</title><link href="https://www.binwang.me/2022-08-20-Apple-In-App-Purchase-Server-to-Server-Notification-Handling-with-Scala-Java.html" rel="alternate" type="text/html" title="Handle Apple In-App-Purchase Server Notification with Scala/Java" /><published>2022-08-20T00:00:00-04:00</published><updated>2022-08-20T00:00:00-04:00</updated><id>https://www.binwang.me/Apple-In-App-Purchase-Server-to-Server-Notification-Handling-with-Scala-Java</id><content type="html" xml:base="https://www.binwang.me/2022-08-20-Apple-In-App-Purchase-Server-to-Server-Notification-Handling-with-Scala-Java.html"><![CDATA[<p>When you write an app for iOS, publish it to Apple App Store and want to sell something within it, Apple makes it mandatory to use its own in app purchase framework for non consumable items and subscriptions. If the app has a server, it’s very usual that the server wants to know the payment events and have some followup logic with them. But how to do that? There is always an option to let the app send a request to server, but anyone can use the same endpoint to make false claims. To prevent this, Apple has a server to server notification mechanism: Instead the app itself, a server from Apple will send a request to your server to notify the payment events. Since the message is signed by Apple, you can make sure no one else can fake it by verifying the signature.</p>

<p>While this framework should in theory makes developer’s life easier, the lack of documentation makes it very painful to use. There is also little and often wrong information on the Internet about how to verify the signature, especially for a server written with Java related tech stack. So in this article, I will give an example about how to decode and verify the payment notification messages sent by Apple with Scala. The library we are using is <a href="https://connect2id.com/products/nimbus-jose-jwt">Nimbus JOSE + JWT</a> which is written in Java, so the method applies to other JVM languages as well. Hopefully this can help other developers who are facing the same problem.</p>

<h2 id="1-how-to-trigger-a-server-notification">1. How to Trigger a Server Notification</h2>

<p>Needless to say, to receive a payment notification you must initiate a payment from the app. There are various ways to do it and we will not discuss it in this article. But be aware there are two ways to test the in app payment: create a <a href="https://developer.apple.com/documentation/xcode/setting-up-storekit-testing-in-xcode">StoreKit configuration in Xcode</a> or <a href="https://developer.apple.com/documentation/storekit/in-app_purchase/testing_in-app_purchases_with_sandbox">use a sandbox environment</a>. Only the later one will trigger a server to server notification.</p>

<p>Another requirement is to set up the notification endpoint in Apple Connection settings. The endpoint is a https URL that Apple will send a http post request to. <a href="https://developer.apple.com/documentation/storekit/in-app_purchase/original_api_for_in-app_purchase/subscriptions_and_offers/enabling_app_store_server_notifications">Here</a> is the Apple document about how to do it.</p>

<h2 id="2-server-notification-workflow">2. Server Notification Workflow</h2>

<p>In order to better understand how to handle the notification message, let’s take a look at the server notification workflow first.</p>

<p><img src="/static/images/2022-08-20-Apple-In-App-Purchase-Server-to-Server-Notification-Handling-with-Scala-Java/iap-server-notification.png" alt="iap-server-notification" /></p>

<p>When Apple receives some payment information, whether it’s from the app, or from subscription renew, or subscription expiration or other events, it will try to send the event to your server by sending an http POST request. In order to make sure no other people can fake a request to the same http endpoint, Apple signs the request payload with a private key that only Apple has access, so that when your server received the message, you can verify it by Apple’s public key.</p>

<p>The way Apple signs the message is using a standard called <a href="https://www.rfc-editor.org/rfc/rfc7515.html">JWS</a>. This is a complex standard with multiple implementation options. To fully understand it you also need to know things like <a href="https://www.rfc-editor.org/rfc/rfc7518.html">JWA</a> and <a href="https://www.rfc-editor.org/rfc/rfc7517">JWK</a>. Apple has very little document about how to decode its own message other than throw this RFC page into the document. Even in support forums, their response is like “use your favourite crypto library”, which doesn’t really help anything.</p>

<h3 id="3-jws-overview">3. JWS Overview</h3>

<p>To make it easy, I will give a very simple overview of JWS, JWS has three parts: a header that contains metadata like keys and algorithm to use, the actual payload, and a signature:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>header (metadata)

-----------

payload (actual message we want, base64 encoded JSON)

-----------

signature (generated by applying crypto algrithm on payload with keys in header)
</code></pre></div></div>

<p>So in order to make sure the whole message is actually sent by Apple, we need to verify:</p>

<ul>
  <li>The signature is generated by the keys in header and the payload.</li>
  <li>The keys in header is generated by Apple.</li>
</ul>

<p>Since the payload is only base64 encoded, for a developer that is not familiar with JWS, even with the help of a JWS library, both verification steps can be easily missed since it only affects the verification, not the decoding of the message.</p>

<p>In the section next, we will have an example about how to decode the message while really verify the message is sent by Apple as well.</p>

<h2 id="4-decode-and-verify-notification">4. Decode and Verify Notification</h2>

<p>Here we are using <a href="https://developer.apple.com/documentation/appstoreservernotifications/app_store_server_notifications_v2">App Store server notifications v2</a>. Let’s say you’ve already got the http POST body from your configured endpoint:</p>

<pre><code class="language-Scala">val responseBodyV2Str: String = ... // your code to get POST body from HTTP request
</code></pre>

<p><code class="language-plaintext highlighter-rouge">responseBodyV2Str</code> itself is not a JWS object but a JSON string like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{"signedPayload":"eyJhbGciOiJFUzI1NiIsIng1YyI6WyJNSUlFTU...."}
</code></pre></div></div>

<p>The value of <code class="language-plaintext highlighter-rouge">signedPayload</code> is the encoded JWS string we want to parse. So we need to get that value first. I’m using <a href="https://circe.github.io/circe/">circe</a> to parse the JSON string, but any method that can parse it and get the value is fine. In my case, I defined some classes based on the JSON structures so that we can parse them in a more type safe way.</p>

<pre><code class="language-Scala">import io.circe.generic.extras._
import io.circe.parser


object ApplePaymentService {

  implicit val config: Configuration = Configuration.default.withDefaults

  @ConfiguredJsonCodec case class AppleResponseBodyV2(
    signedPayload: String,
  )

  @ConfiguredJsonCodec case class AppleResponseBodyV2DecodedPayload(
    notificationType: String,
    subtype: Option[String],
    data: ApplePayloadData,
  )

  @ConfiguredJsonCodec case class ApplePayloadData(
    appAppleId: Option[String],
    bundleId: String,
    bundleVersion: String,
    environment: String,
    signedRenewalInfo: String,
    signedTransactionInfo: String,
  )

  @ConfiguredJsonCodec case class AppleJWSTransactionDecodedPayload(
    appAccountToken: String,
    bundleId: String,
    environment: String,
    expiresDate: Long, // timestamp in ms
    inAppOwnershipType: String,
    originalPurchaseDate: Long, // timestamp in ms
    originalTransactionId: String,
    productId: String,
    purchaseDate: Long, // timestamp in ms
    quantity: Int,
    transactionId: String,
    `type`: String,
    webOrderLineItemId: String,
  )
}
</code></pre>

<p>With the help with the classes and JSON parser, we can get the value of <code class="language-plaintext highlighter-rouge">signedPayload</code>. (I removed <code class="language-plaintext highlighter-rouge">\n</code> from the JSON string since it’s not valid to have newlines in JSON string, not sure why Apple’s request body has newline in it):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val responseBodyV2 = parser.parse(responseBodyV2Str.replace("\n", "")).flatMap(_.as[AppleResponseBodyV2]).toTry.get
</code></pre></div></div>

<p>After get the JWS string, we can parse it with <a href="https://connect2id.com/products/nimbus-jose-jwt/examples">Numbus Jose + JWT</a> (follow the document to add this dependency into your project first):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import com.nimbusds.jose.JWSObject
import com.nimbusds.jose.crypto.ECDSAVerifier
import com.nimbusds.jose.crypto.bc.BouncyCastleProviderSingleton
import com.nimbusds.jose.jwk.ECKey
import com.nimbusds.jose.util.X509CertUtils


val jwsObject = JWSObject.parse(responseBodyV2.signedPayload)
val jwsCerts = jwsObject.getHeader.getX509CertChain.asScala.map(c =&gt; X509CertUtils.parse(c.decode()))
</code></pre></div></div>

<h3 id="41-verify-keys-in-jws-header-is-signed-by-apple">4.1 Verify keys in JWS header is signed by Apple</h3>

<p><code class="language-plaintext highlighter-rouge">jwsCerts</code> is a list of <code class="language-plaintext highlighter-rouge">X509Certificate</code>, which is the key chain in JWS header. A cert in the list can be verified by the cert behind it. And the last cert should be verified by Apple’s public key so that we can make sure the whole key chain is signed by Apple.</p>

<p>So let’s first get the root cert of Apple first: download <a href="https://www.apple.com/certificateauthority/AppleRootCA-G3.cer">Apple Root CA - G3 Root</a> from <a href="https://www.apple.com/certificateauthority/">Apple PKI website</a> and put it under your project’s <code class="language-plaintext highlighter-rouge">src/resources/certs</code> (or any where the program can read, we are just using it as an example here). Then we can read the Apple root cert with this code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val appleRootCa = X509CertUtils.parse(getClass.getResourceAsStream("/certs/AppleRootCA-G3.cer").readAllBytes())
</code></pre></div></div>

<p>With both the key chain and root cert, we can verify the key chain is both valid and signed by Apple:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jwsCerts.sliding(2).foreach { x =&gt;
  x.head.verify(x.last.getPublicKey)
}
jwsCerts.last.verify(appleRootCa.getPublicKey)
</code></pre></div></div>

<p>It will throw exception if the verify doesn’t pass.</p>

<h3 id="42-verify-jws-is-signed-by-keys-in-jws-header">4.2 Verify JWS is signed by keys in JWS header</h3>

<p>Once we verified the keys in JWS header is signed by Apple, we need to verify JWS itself is signed by these keys. Since the <code class="language-plaintext highlighter-rouge">alg</code> field in this JWS header is <code class="language-plaintext highlighter-rouge">ES256</code>, we will use <code class="language-plaintext highlighter-rouge">ECDSAVerifier</code> to verify it:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val jwk = ECKey.parse(jwsCerts.head)
val jwsVerifier = new ECDSAVerifier(jwk)
if (!jwsObject.verify(jwsVerifier)) {
  throw new RuntimeException("Apple JWS object cannot be verified")
}
</code></pre></div></div>

<h3 id="43-parse-the-payload">4.3 Parse the payload</h3>

<p>After verify the JWS is valid, we can start to parse the payload. I’m using the JSON parser and the structure I defined above. Please refer to Apple’s document about the actual fields in the payload:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val responseBodyV2Payload = jwsObject.getPayload.toString
val responseBodyV2DecodedPayload = parser.parse(responseBodyV2Payload).flatMap(_.as[AppleResponseBodyV2DecodedPayload]).toTry.get
val transactionPayload = responseBodyV2DecodedPayload.data.signedTransactionInfo
val transactionDecodedPayloadStr = JWSObject.parse(transactionPayload).getPayload.toString
val transactionDecodedPayload = parser.parse(transactionDecodedPayloadStr).flatMap(_.as[AppleJWSTransactionDecodedPayload]).toTry.get
</code></pre></div></div>

<p>Here we have the detailed transaction information in <code class="language-plaintext highlighter-rouge">transactionDecodedPayload</code> and can hand it with our business logic.</p>

<p>There is an interesting thing: <code class="language-plaintext highlighter-rouge">signedRenewInfo</code> and <code class="language-plaintext highlighter-rouge">signedTrasactionInfo</code> are both encoded with JWS again in payload data. I don’t know why: since we’ve already verified the whole payload is signed by Apple, all the content in it should already be valid as well, what’s the point to sign the fields again? I just decoded the fields with <code class="language-plaintext highlighter-rouge">JWSObject.parse</code> but you can always verify it with the same method above just to be safe.</p>

<h2 id="5-other-thoughts">5. Other Thoughts</h2>

<p>As I said about a <a href="/2020-11-08-DNS-Resolving-Bug-in-iOS-14.html">previous blog about an iOS bug</a>, I really hate Apple’s close ecosystem. But Apple’s hardware is good and has a large user space, so we cannot avoid it. Hopefully Android can be better at permission management and other mobile OS can also catch up.</p>]]></content><author><name></name></author><category term="iOS" /><category term="Apple" /><category term="In App Purchase" /><category term="Scala" /><category term="Java" /><category term="Programming" /><summary type="html"><![CDATA[When you write an app for iOS, publish it to Apple App Store and want to sell something within it, Apple makes it mandatory to use its own in app purchase framework for non consumable items and subscriptions. If the app has a server, it’s very usual that the server wants to know the payment events and have some followup logic with them. But how to do that? There is always an option to let the app send a request to server, but anyone can use the same endpoint to make false claims. To prevent this, Apple has a server to server notification mechanism: Instead the app itself, a server from Apple will send a request to your server to notify the payment events. Since the message is signed by Apple, you can make sure no one else can fake it by verifying the signature.]]></summary></entry></feed>