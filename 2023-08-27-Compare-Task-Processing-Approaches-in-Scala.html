<!DOCTYPE html>

<html>
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title> Compare Task Processing Approaches in Scala |  Bin Wang - My Personal Blog</title>
    <link rel="stylesheet" href="/static/css/default.css" type="text/css" />

		<!--
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-52904500-1', 'auto');
      ga('send', 'pageview');
    </script>
		-->

		<!-- Matomo -->
		<script>
		  var _paq = window._paq = window._paq || [];
		  /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
		  _paq.push(['trackPageView']);
		  _paq.push(['enableLinkTracking']);
		  (function() {
		    var u="//matomo.rssbrain.com/";
		    _paq.push(['setTrackerUrl', u+'matomo.php']);
		    _paq.push(['setSiteId', '2']);
		    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
		    g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
		  })();
		</script>
		<!-- End Matomo Code -->


  </head>

  <body>
    <header id="page_header">
      <nav id="page_nav">
        <ul>
             <li><a href="/">Home</a></li>
             <li><a href="/index_page.html">Index</a></li>
             <li><a href="/snippets">Snippets</a></li>
             <li><a href="https://www.goodreads.com/review/list/103708630-bin?shelf=read" target="_blank">Read</a></li>
             <li><a href="/travel.html">Travel</a></li>
             <li><a href="/search.html">Search</a></li>
             <li><a href="/about.html">About</a></li>
             <li><a href="/feed.xml">RSS</a></li>
       </ul>
      </nav>

    </header>

    <section id="page_content">
      <div id="content_table">
  <h3>Table of Contents</h3><ol class="toc"><li><a href="#task-processing">Task Processing</a></li><li><a href="#introducing-cats-effect-and-fs2">Introducing Cats Effect and FS2</a></li><li><a href="#testing-setup">Testing Setup</a></li><li><a href="#approach-1-batch-consuming">Approach 1: Batch Consuming</a></li><li><a href="#approach-2-use-blocking-queue-to-buffer-tasks">Approach 2: Use Blocking Queue to Buffer Tasks</a></li><li><a href="#approach-3-use-cats-effect-friendly-queue">Approach 3: Use Cats Effect Friendly Queue</a></li><li><a href="#approach-4-use-fs2-stream-directly">Approach 4: Use FS2 Stream Directly</a></li><li><a href="#approach-5-make-producers-run-in-parallel">Approach 5: Make Producers Run in Parallel</a></li><li><a href="#more">More</a></li><li><a href="#test-results">Test Results</a></li></ol>
</div>

<div id="article_content" class="">
<article id="post">
  <header>
    <h1>Compare Task Processing Approaches in Scala</h1>
    
      <p class="description">Posted on 27 Aug 2023, tagged <code>Scala</code><code>concurrent</code><code>cats</code><code>cats-effect</code><code>fs2</code><code>queue</code><code>stream</code></p>
    
  </header>

  <p><em>All the source code mentioned in this blog can be found in <a href="https://github.com/wb14123/scala-stream-demo">my Github repo</a>.</em></p>

<h2 id="task-processing">Task Processing</h2>

<p>There is a common problem in computer science and I’ve met it again recently: how to generate and process tasks efficiently? Use my recent project <a href="https://www.rssbrain.com">RSS Brain</a> as an example: it needs to find the RSS feeds that haven’t been updated for a while in a database, and fetch the newest data from network.</p>

<p>The easiest way to do it is producing and consuming the tasks in a sequence, for example:</p>

<div class="language-Scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>val feeds = getPendingFeeds() // produce the tasks
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>feeds.foreach(fetchFromNetwork) // consume the dtasks
</pre></div>
</div>
</div>

<p>However, it is unnecessarily slow. Network request doesn’t take lots of CPU and we can send multiple requests at the same time. Even if <code>fetchFromNetwork</code> is a CPU bound task, it can be parallelized if there are multiple CPU cores on a machine.</p>

<p>In this article, we will explore ways to do it more efficiently with <a href="https://typelevel.org/cats-effect/">Cats Effect</a> and <a href="https://fs2.io">FS2</a> in a functional programming fashion.</p>

<p><em>You may wonder why not using AKKA stream? Other than it’s using a different programming paradigm (not functional programming), it’s also because <a href="https://www.lightbend.com/blog/why-we-are-changing-the-license-for-akka">AKKA has changed its license</a> with a ridiculous price.</em></p>

<h2 id="introducing-cats-effect-and-fs2">Introducing Cats Effect and FS2</h2>

<p>To make <code>processTask</code> async, there is <code>Future</code> in Scala’s standard library. However, the side effect will happen when you create a <code>Future</code> instance. For example:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>def processTask(task: Task): Future[Unit] = Future(println(task))
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>val runTask1 = processTask(task1) // this will start the async task
</pre></div>
</div>
</div>

<p>I assume the readers have a basic understanding of functional programming, so I’ll not explain why we want to avoid side effects. While Scala is not a pure functional language, a popular Scala library <a href="https://typelevel.org/cats-effect/">Cats Effect</a> provides convenient ways to wrap side effects. With the help of its <code>IO</code> type, we can define an async task like this:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>def processTask(task: Task): IO[Unit] = IO(println(task))
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>// this will not start the task, so no side effect
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>val runTask1 = processTask(task1)
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>
<span class="line-numbers"><a href="#n6" name="n6">6</a></span>// out of pure functional world and starts the side effect
<span class="line-numbers"><a href="#n7" name="n7">7</a></span>runTask1.unsafeRunSync()
</pre></div>
</div>
</div>

<p>Then there is <a href="https://fs2.io">fs2</a> that is a stream library that can be used with cats effect. It will be very handy when resolving our problem as we can see later.</p>

<p><em>Cat Effect has some big changes in version 3.x. In this article, we are using version 2.x. But I may upgrade the version in the future.</em></p>

<h2 id="testing-setup">Testing Setup</h2>

<p>In order to test which approach is the best under different scenarios, we need some basic setup. In <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/TestRunner.scala">TestRunner.scala</a>, I defined some functions to generate tasks. Here are their signatures:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>// Produce a sequence of tasks represented by `Int`
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>def produce(start: Int, end: Int): IO[Seq[Int]]
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>// Process a task
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>def consume(x: Int): IO[Unit]
<span class="line-numbers"><a href="#n6" name="n6">6</a></span>
<span class="line-numbers"><a href="#n7" name="n7">7</a></span>// Produce tasks as a stream
<span class="line-numbers"><a href="#n8" name="n8">8</a></span>def def produceStream(start: Int, end: Double): fs2.Stream[IO, Int]
</pre></div>
</div>
</div>

<p><code>produce</code> simply produces tasks as <code>int</code>, and <code>consume</code> just print characters. In each of the functions, I use <code>IO.sleep</code> to create some delay to simulate the real world non-blocking IO. They also print characters <code>P</code> (produce) or <code>C</code> (consume) (based on the width of terminal, some of the <code>C</code> outputs may be skipped to fit the width) when being invoked, so that we can have an intuitive view of how quick tasks are produced and consumed.</p>

<p>Then there is <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/TestConfig.scala">TestConfig.scala</a> for configuring the test:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>trait TestConfig {
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>  val testName: String
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>  val produceDelay: FiniteDuration
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>  val minConsumeDelayMillis: Long
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>  val maxConsumeDelayMillis: Long
<span class="line-numbers"><a href="#n6" name="n6">6</a></span>  val batchSize = 100  // consume batch size
<span class="line-numbers"><a href="#n7" name="n7">7</a></span>  val totalSize = 1000 // how many tasks to generate
<span class="line-numbers"><a href="#n8" name="n8">8</a></span>}
</pre></div>
</div>
</div>

<p>By setting up produce and consume delays, we can test scenarios when producer is slower, consumer is slower, or producer and consumer speed is almost the same. Here are the configurations we are going to use in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/Main.scala">Main.scala</a></p>

<div class="language-Scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>val configs = Seq(
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>  new TestConfig {
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>    override val testName: String = &quot;slow-producer&quot;
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>    override val produceDelay: FiniteDuration = 1000.millis
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>    override val minConsumeDelayMillis: Long = 10
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>    override val maxConsumeDelayMillis: Long = 100
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>  },
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>  new TestConfig {
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>    override val testName: String = &quot;balanced&quot;
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>    override val produceDelay: FiniteDuration = 1005.millis
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>    override val minConsumeDelayMillis: Long = 10
<span class="line-numbers"><a href="#n12" name="n12">12</a></span>    override val maxConsumeDelayMillis: Long = 2000
<span class="line-numbers"><a href="#n13" name="n13">13</a></span>  },
<span class="line-numbers"><a href="#n14" name="n14">14</a></span>  new TestConfig {
<span class="line-numbers"><a href="#n15" name="n15">15</a></span>    override val testName: String = &quot;slow-consumer&quot;
<span class="line-numbers"><a href="#n16" name="n16">16</a></span>    override val produceDelay: FiniteDuration = 10.millis
<span class="line-numbers"><a href="#n17" name="n17">17</a></span>    override val minConsumeDelayMillis: Long = 10
<span class="line-numbers"><a href="#n18" name="n18">18</a></span>    override val maxConsumeDelayMillis: Long = 1000
<span class="line-numbers"><a href="#n19" name="n19">19</a></span>  }
<span class="line-numbers"><strong><a href="#n20" name="n20">20</a></strong></span>)
</pre></div>
</div>
</div>

<h2 id="approach-1-batch-consuming">Approach 1: Batch Consuming</h2>

<p>The first approach is to make the consuming side parallel. We can consume a batch of tasks concurrently, like in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/BatchIOApp.scala">BatchIOApp.scala</a>.</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>def loop(start: Int): IO[Unit] = {
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>  if (start &gt;= config.totalSize) {
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>    IO.unit
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>  } else {
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>    produce(start, start + config.batchSize)
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>      .flatMap{_.map(consume).parSequence}
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>      .flatMap(_ =&gt; loop(start + config.batchSize))
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>  }
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>}
</pre></div>
</div>
</div>

<p>However, this only makes a batch of tasks run in parallel. It needs to wait the whole batch to be finished in order to start next batch. This is very obvious when we run this approach and see the output of characters (download <a href="https://github.com/wb14123/scala-stream-demo">Github repo</a> and run <code>sbt run "-n BatchIOApp"</code>). See how it paused after each batch even when consumer is slower than producer:</p>

<script async="" id="asciicast-P2ZX0r2VYaMXCVjrJCoJ0Y3DS" src="https://asciinema.org/a/P2ZX0r2VYaMXCVjrJCoJ0Y3DS.js" data-rows="10"></script>

<h2 id="approach-2-use-blocking-queue-to-buffer-tasks">Approach 2: Use Blocking Queue to Buffer Tasks</h2>

<p>We need a way to let producers not waiting for consumers, and also let consumers not wait for a batch to finish in order to start next batch. A very common solution is to use a queue between producers and consumers. Producers put tasks into the queue, and consumers get tasks for the queue. If the queue is thread safe, then both producers and consumers can work on their own without care about each other. In order to not let producer put unlimited tasks into the queue to blowup the memory, we need the queue to have a capacity. When the queue is full, the producer should be blocked. And when the queue is empty, the consumers should be blocked as well.</p>

<p>In Java, <code>BlockingQueue</code> meets our requirements. We can use an implementation <code>LinkedBlockingQueue</code>. However, <code>BlockingQueue</code> will block the whole thread instead of a single <code>IO</code>. Let’s not worry about it for now and see how to use a queue to implement producing and consuming in parallel. The implementation is in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/BlockingQueueApp.scala">BlockingQueueApp.scala</a>:</p>

<div class="language-scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>val queue = new LinkedBlockingQueue[Option[Int]](config.batchSize * 2)
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>override def work(): IO[Unit] = {
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>  Seq(
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>    (produceStream(0).map(Some(_)) ++ fs2.Stream.emit(None))
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>                        .evalMap(x =&gt; IO(queue.put(x))).compile.drain,
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>    dequeueStream().unNoneTerminate.parEvalMap(config.batchSize)(consume).compile.drain,
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>  ).parSequence.map(_ =&gt; ())
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>}
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>
<span class="line-numbers"><a href="#n12" name="n12">12</a></span>private def dequeueStream(): fs2.Stream[IO, Option[Int]] = {
<span class="line-numbers"><a href="#n13" name="n13">13</a></span>  fs2.Stream.eval(IO(queue.take())) ++ dequeueStream()
<span class="line-numbers"><a href="#n14" name="n14">14</a></span>}
</pre></div>
</div>
</div>

<p>Here we have two IOs run in parallel with <code>parSequence</code>: the first one creates a task stream by <code>produceStream</code>, and append <code>None</code> at the end so that the consumer knows it should end processing. Another stream <code>dequeueStream</code> gets the tasks from the queue then consumes it in parallel with <code>parEvalmap(config.batchSize)(consume)</code>.</p>

<p>When run it with <code>sbt "run -n BlockingQueueApp"</code>, we can see it’s much faster when the consumer is faster or has the same speed as the producer. Especially when the consumer is slow, it prints multiple <code>P</code> at first, which means the producers doesn’t wait all the consumers to finish in order to produce tasks.</p>

<script async="" id="asciicast-gWC18DjuVT1v6sDaf2HJJerq6" src="https://asciinema.org/a/gWC18DjuVT1v6sDaf2HJJerq6.js" data-rows="10"></script>

<p>Back to the blocking the whole thread problem: it doesn’t seem to be a problem in this case, right? It’s only because we are lucky! In this setup, we are using two fixed threads as the thread pool of running IO in <code>Main.scala</code>:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>private val executor = Executors.newFixedThreadPool(2, (r: Runnable) =&gt; {
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>  val back = new Thread(r)
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>  back.setDaemon(true)
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>  back
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>})
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>implicit override def executionContext: ExecutionContext = ExecutionContext.fromExecutor(executor)
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>implicit override def timer: Timer[IO] = IO.timer(executionContext)
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>implicit override def contextShift: ContextShift[IO] = IO.contextShift(executionContext)
</pre></div>
</div>
</div>

<p>If 2 consumers with empty queue happens to be scheduled on these 2 threads separately, it will block. If we change our <code>BlockingQueueApp</code> to the code in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/RealBlockingQueueApp.scala">RealBlockingQueueApp</a>:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>override def work(): IO[Unit] = {
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>  Seq(
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>    dequeueStream().unNoneTerminate.parEvalMap(config.batchSize)(consume).compile.drain,
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>    dequeueStream().unNoneTerminate.parEvalMap(config.batchSize)(consume).compile.drain,
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>    (produceStream(0).map(Some(_)) ++ fs2.Stream.emit(None)).evalMap(x =&gt; IO(queue.put(x))).compile.drain,
<span class="line-numbers"><a href="#n6" name="n6">6</a></span>  ).parSequence.map(_ =&gt; ())
<span class="line-numbers"><a href="#n7" name="n7">7</a></span>}
</pre></div>
</div>
</div>

<p>Here we started two dequeue stream at first. Now the whole program will block when run it with <code>sbt "run -b"</code> .</p>

<p>The lesson learned here is that there is a big risk if any operation blocks the whole thread in cats effect. Even it doesn’t block the whole program, it may make a whole thread unavailable.</p>

<p>Actually in <a href="https://typelevel.org/cats-effect/docs/thread-model">Cats Effect’s thread model</a>, there is another thread pool for blocking tasks if we mark it explicitly. In <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/AsyncConsole.scala">AsyncConsole.scala</a>, I use this exact block mode to run console output so that it won’t effect other non blocking IO operations:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>def asyncPrintln(s: String)(
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>    implicit cs: ContextShift[IO], blocker: Blocker): IO[Unit] = blocker.blockOn(IO(println(s)))
</pre></div>
</div>
</div>

<p>However, if a thread is blocked in this pool, it will start another thread for the next operation. Based on the document, there is no limit on how many threads will be created. So if the producer is much slower than consumer, there will be more and more consume operations blocked on dequeue, so it will generate a large amount of threads, which is not ideal and eventually even will blow up the memory.</p>

<h2 id="approach-3-use-cats-effect-friendly-queue">Approach 3: Use Cats Effect Friendly Queue</h2>

<p>What if we have a queue that only block the dequeue <code>IO</code> when empty instead of blocking the whole thread? Luckily, FS2 provides such a queue. (Cats Effect 3.x also provides such a queue). The implementation is basically the same as above (code in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/StreamQueueApp.scala">StreamQueueApp.scala</a>):</p>

<div class="language-Scala highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>import fs2.concurrent.Queue
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>def work(): IO[Unit] = {
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>  for {
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>    queue &lt;- Queue.bounded[IO, Option[Int]](config.batchSize * 2)
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>    _ &lt;- Seq(
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>      (produceStream(0).map(Some(_)) ++ fs2.Stream.emit(None)).through(queue.enqueue).compile.drain,
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>      queue.dequeue.unNoneTerminate.parEvalMap(config.batchSize)(consume).compile.drain,
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>    ).parSequence
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>  } yield ()
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>}
</pre></div>
</div>
</div>

<p>Run <code>sbt "run -n StreamAppQueue"</code> to see how it performs.</p>

<h2 id="approach-4-use-fs2-stream-directly">Approach 4: Use FS2 Stream Directly</h2>

<p>FS2 actually provides some advanced stream operations that makes it possible to combine the producing stream and consume stream, like the code in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/StreamApp.scala">StreamApp.scala</a>:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>produceStream(0).parEvalMap(config.batchSize)(consume).compile.drain
</pre></div>
</div>
</div>

<p>Here we map <code>consume</code> in parallel on <code>produce</code> stream. However, if you try to run <code>sbt "run -n StreamApp"</code> vs <code>sbt "run -n StreamQueueApp"</code>, you will find this is slower than before. This is because <code>produceStream</code> will give the next batch when the downstream asks. If we can prepare at least one batch before the downstream is free, we can save more time. Luckily, it’s very easy to do in fs2. As we can see in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/PrefetchStreamApp.scala">PrefetchStreamApp.scala</a>, we can add <code>prefetch</code> after the <code>produceStream</code>:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>produceStream(0).prefetch.parEvalMap(config.batchSize)(consume).compile.drain
</pre></div>
</div>
</div>

<p>It will prefetch a <a href="https://fs2.io/#/guide?id=chunks">chunk</a> of elements. Use <code>prefetchN</code> if you want to prefetch N chunks.</p>

<p>Then run this with <code>sbt "run -n PrefetchStreamApp"</code>, you will find the performance is similar as the queued approach.</p>

<p>Actually if you check the source code of <code>prefetch</code>, you will find the implementation is almost the same as ours:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"><a href="#n1" name="n1">1</a></span>def prefetch[F2[x] &gt;: F[x]: Concurrent]: Stream[F2, O] = prefetchN[F2](1)
<span class="line-numbers"><a href="#n2" name="n2">2</a></span>
<span class="line-numbers"><a href="#n3" name="n3">3</a></span>def prefetchN[F2[x] &gt;: F[x]: Concurrent](n: Int): Stream[F2, O] =
<span class="line-numbers"><a href="#n4" name="n4">4</a></span>  Stream.eval(Queue.bounded[F2, Option[Chunk[O]]](n)).flatMap { queue =&gt;
<span class="line-numbers"><a href="#n5" name="n5">5</a></span>    queue.dequeue.unNoneTerminate
<span class="line-numbers"><a href="#n6" name="n6">6</a></span>      .flatMap(Stream.chunk(_))
<span class="line-numbers"><a href="#n7" name="n7">7</a></span>      .concurrently(chunks.noneTerminate.covary[F2].through(queue.enqueue))
<span class="line-numbers"><a href="#n8" name="n8">8</a></span>  }
</pre></div>
</div>
</div>

<h2 id="approach-5-make-producers-run-in-parallel">Approach 5: Make Producers Run in Parallel</h2>

<p>We’ve made it runs in parallel between consumers, also between consumers and producers. But we haven’t made producers run in parallel yet. With the queue, its very easy to do, just start multiple <code>IO</code>s for <code>produceStream.through(queue.enqueue)</code>. <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/ConcurrentProducerQueueApp.scala">ConcurrentProduceQueueApp.scala</a> is an example:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>private val counter = new AtomicInteger(0)
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>override def work(): IO[Unit] = {
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>  for {
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>    queue &lt;- Queue.bounded[IO, Int](config.batchSize * 2)
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>    _ &lt;- Seq(
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>      produceStream(0, config.totalSize / 2).through(queue.enqueue).compile.drain,
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>      produceStream(config.totalSize / 2, config.totalSize).through(queue.enqueue).compile.drain,
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>      queue.dequeue.parEvalMap(config.batchSize) { x =&gt;
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>        consume(x).map { _ =&gt;
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>          if (counter.incrementAndGet() &gt;= config.totalSize) {
<span class="line-numbers"><a href="#n12" name="n12">12</a></span>            None
<span class="line-numbers"><a href="#n13" name="n13">13</a></span>          } else {
<span class="line-numbers"><a href="#n14" name="n14">14</a></span>            Some()
<span class="line-numbers"><a href="#n15" name="n15">15</a></span>          }
<span class="line-numbers"><a href="#n16" name="n16">16</a></span>        }
<span class="line-numbers"><a href="#n17" name="n17">17</a></span>      }.unNoneTerminate.compile.drain,
<span class="line-numbers"><a href="#n18" name="n18">18</a></span>    ).parSequence
<span class="line-numbers"><a href="#n19" name="n19">19</a></span>  } yield ()
<span class="line-numbers"><strong><a href="#n20" name="n20">20</a></strong></span>}
</pre></div>
</div>
</div>

<p>It has 2 concurrent producers but in theory you can create as many as you want, just be careful with the parameters of <code>produceStream</code>.</p>

<p>If you run this with <code>sbt "run -n ConcurrentProduceQueueApp"</code>, you can find the performance is much better with slower producer. However, with the help of fs2 library, we can make the code cleaner without depends on any queue explicitly. Here is what I did in <a href="https://github.com/wb14123/scala-stream-demo/blob/master/src/main/scala/ConcurrentProducerApp.scala">ConcurrentProducerApp.scala</a>:</p>

<div class="language-plaintext highlighter-coderay"><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>def work(): IO[Unit] = {
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>  fs2.Stream.emits(Range(0, produceParallelism))
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>    .map(batch =&gt; produceStream(
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>      batch * config.totalSize / produceParallelism,
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>      (batch + 1) * config.totalSize / produceParallelism.toDouble))
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>    .parJoin(produceParallelism)
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>    .prefetch
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>    .parEvalMap(config.batchSize)(consume).compile.drain
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>}
</pre></div>
</div>
</div>

<p>Here we use <code>parJoin</code> to join multiple producer stream at the same time.</p>

<h2 id="more">More</h2>

<p>All the approaches above other than the first one uses a queue either implicitly or explicitly. However, under high parallelism and load, every job operating on a single queue may makes this queue a bottleneck. In this case, there is a <a href="https://en.wikipedia.org/wiki/Work_stealing">work stealing</a> algorithm that each consumers can has its own queue, and whenever a consumer’s queue is empty, it steal some tasks from another one. But it’s a little bit complex and unnecessary if the load is not so high, so I will not cover it in this article.</p>

<h2 id="test-results">Test Results</h2>

<p>Now let’s run all the approaches and compare the performance with <code>sbt "run -n"</code>. Here are the results:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>slow producer</th>
      <th>balanced</th>
      <th>slow consumer</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BatchIO</td>
      <td>11086.078637 ms</td>
      <td>29912.377578 ms</td>
      <td>10015.51878 ms</td>
    </tr>
    <tr>
      <td>BlockingQueue</td>
      <td>10190.038753 ms</td>
      <td>14195.228189 ms</td>
      <td>6495.333179 ms</td>
    </tr>
    <tr>
      <td>StreamQueue</td>
      <td>10138.016643 ms</td>
      <td>14458.443122 ms</td>
      <td>6418.078377 ms</td>
    </tr>
    <tr>
      <td>Stream</td>
      <td>10356.178562 ms</td>
      <td>15655.697826 ms</td>
      <td>6560.111697 ms</td>
    </tr>
    <tr>
      <td>PrefetchStream</td>
      <td>10141.110634 ms</td>
      <td>14578.362136 ms</td>
      <td>6376.628036 ms</td>
    </tr>
    <tr>
      <td>ConcurrentProduceresQueue</td>
      <td>5187.442452 ms</td>
      <td>14395.321922 ms</td>
      <td>6576.538821 ms</td>
    </tr>
    <tr>
      <td>ConcurrentProducer</td>
      <td>5198.723825 ms</td>
      <td>14544.247312 ms</td>
      <td>6418.078377 ms</td>
    </tr>
  </tbody>
</table>

<p>We can see approaches that parallelize all the parts win the performance game.</p>

</article>

<footer id="post_footer">
  <table><tr>
    
      <td id="prev"><a href="/2023-05-22-Upgrade-Kubernetes-from-1.23-to-1.24.html">Prev: Upgrade Kubernetes from 1.23 to 1.24</a></td>
    
    
      <td id="next"><a href="/2023-09-01-Build-a-Linux-Virtual-Machine-for-Windows-Apps.html" id="next">Next: Build a Linux Virtual Machine for Windows Apps</a></td>
    
  </tr></table>
</footer>

<section id="comment">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'crazy-hot-ice'; // required: replace example with your forum shortname
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



<!-- MathJax -->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

</div>

    </section>

    <footer id="page_footer">
      Copyright @ 2008 - 2023 Bin Wang
      <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US">Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License</a>.
    </footer>
  </body>
</html>
