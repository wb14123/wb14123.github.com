<!DOCTYPE html>

<html>
  <head>
    <meta charset="UTF-8">
    <title> Backpropagation Algorithm |  Bin Wang</title>
    <link rel="stylesheet" href="/static/css/default.css" type="text/css" />

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-52904500-1', 'auto');
      ga('send', 'pageview');
    </script>

  </head>

  <body>
    <header id="page_header">
      <nav id="page_nav">
        <ul>
             <li><a href="/">Home</a></li>
             <li><a href="/read.html">Read</a></li>
             <li><a href="/travel.html">Travel</a></li>
             <li><a href="/search.html">Search</a></li>
             <li><a href="/about.html">About</a></li>
       </ul>
      </nav>

    </header>

    <section id="page_content">
      <div id="content_table">
  <h3>Table of Contents</h3><ol class="toc"><li><a href="#neural-network-functions">Neural Network Functions</a></li><li><a href="#gradient-descent-algorithm">Gradient Descent Algorithm</a></li><li><a href="#compute-partial-derivative">Compute Partial Derivative</a></li></ol>
</div>

<div id="article_content">
<article id="post">
  <header>
    <h1>Backpropagation Algorithm</h1>
    
      <p class="description">Posted on 17 Jan 2015, tagged <code>algorithm</code><code>deep learning</code></p>
    
  </header>

  <p>These days I start to learn neural networks again and write some Matlab codes from scratch. I try to understand everything I do while I write the code, so I derive the equations in the back propagation while try to keep it clear and easy to understand.</p>

<h2 id="neural-network-functions">Neural Network Functions</h2>

<p>A multi layer neural network could be defined as this:</p>

<div>
\begin{equation}
a_1(x, w, b) = x \\
\end{equation}

\begin{equation}
z_i(x, w, b) = a_{i-1}(x, w, b) \cdot w_{i-1} + b_{i-1}
\end{equation}

\begin{equation}
a_i(x, w, b) = \sigma(z_i(x, w, b))
\end{equation}

</div>

<p>Assume <span><script type="math/tex"> layer_i </script></span> means the number of neural in layer i, then the variables in the equations could be explained as below:</p>

<ul>
  <li><code>x</code> is the input, which is a row vector, it has <span><script type="math/tex"> layer_1 </script></span> elements.</li>
  <li><span><script type="math/tex"> w_i </script></span> meas the weights in layer <code>i</code>, which is a matrix of <span><script type="math/tex"> layer_i </script></span> rows and <span><script type="math/tex"> layer_{i+1} </script></span> columns.</li>
  <li><span><script type="math/tex"> b_i </script></span> means biases in layer i, which is a row vector of <span><script type="math/tex"> layer_{i+1} </script></span> elements.</li>
  <li><span><script type="math/tex"> a_i </script></span> means activation function in the ith layer, which the output is a row vector, it has <span><script type="math/tex"> layer_i </script></span> elements.</li>
  <li><code>l</code> means the last layer. The output of <span><script type="math/tex">a_l</script></span> is the output of the neural network.</li>
</ul>

<p>And <span><script type="math/tex"> \sigma(z) </script></span> may be different in different use cases. This one is an example:</p>

<div>
\begin{equation}
\sigma(z) = {1 \over 1 + e^{-z}}
\end{equation}
</div>

<h2 id="gradient-descent-algorithm">Gradient Descent Algorithm</h2>

<p>We need a cost function to measure how well do we do for now. And the training of the network becomes a optimization problem. The method we use in the problem is gradient descent. Let me try to explain it.</p>

<p>Assume we have a cost function, and it is always non-negative. For example, this function is a good one:</p>

<div>
\begin{equation}
C(x, y, w, b) = {1 \over 2} (y - a_l(x, w, b)) ^ 2
\end{equation}
</div>

<p>Then the goal is try to make the output of the cost function smaller. Since <code>x</code> and <code>y</code> is fixed, the change of cost function while change <code>w</code> and <code>b</code> little could be shown as this:</p>

<div>
\begin{equation}
\Delta C = {\partial C \over \partial w} \Delta w + {\partial C \over \partial b} \Delta b 
\end{equation}
</div>

<p>In order to make the cost function smaller, we need to make <span><script type="math/tex"> \Delta C</script></span> negative. We can make <span><script type="math/tex"> \Delta w = - {\partial C \over \partial w} </script></span> and <span><script type="math/tex"> \Delta b = - {\partial C \over \partial b} </script></span>. So that <span><script type="math/tex"> \Delta C = - ({\partial C \over \partial w}) ^ 2 - ({\partial C \over \partial b}) ^ 2</script></span> which is always negative.</p>

<h2 id="compute-partial-derivative">Compute Partial Derivative</h2>

<p>So the goal is to compute the partial derivative <span><script type="math/tex"> \partial C \over \partial w</script></span> and <span><script type="math/tex"> \partial C \over \partial b</script></span>. Using equations (1), (2), (3), (5) and chain rule, we can get this:</p>

<div>

\begin{equation}
{\partial C \over \partial a_l} = a_l - y
\end{equation}

\begin{equation}
{\partial C \over \partial a_i} = {\partial C \over \partial a_{i+1}} {\partial a_{i+1} \over \partial \sigma} {\partial \sigma \over \partial z_{i+1}} {\partial z_{i+1} \over \partial a_i} = {\partial C \over \partial a_{i+1}} \odot {\sigma^{'}(z_{i+1})} w_i^{'}
\end{equation}

\begin{equation}
{\partial C \over \partial w_i} = {\partial C \over \partial a_{i+1}} {\partial a_{i+1} \over \partial \sigma} {\partial \sigma \over \partial z_{i+1}} {\partial z_{i+1} \over \partial w_i} = a_i^{'} {\partial C \over \partial a_{i+1}} \odot {\sigma^{'}(z_{i+1})}
\end{equation}

\begin{equation}
{\partial C \over \partial b_i} = {\partial C \over \partial a_{i+1}} {\partial a_{i+1} \over \partial \sigma} {\partial \sigma \over \partial z_{i+1}} {\partial z_{i+1} \over \partial b_i} = {\partial C \over \partial a_{i+1}} \odot {\sigma^{'}(z_{i+1})}
\end{equation}

</div>

<p>Note that we use <span><script type="math/tex"> \odot </script></span> before <span><script type="math/tex"> \sigma^{'} </script></span> is because function <span><script type="math/tex"> \sigma(z) </script></span> is element wise.</p>

<p>We can find there are many same parts in these equations: <span><script type="math/tex"> {\partial C \over \partial a_{i+1}} \odot {\sigma^{'}(z_{i+1})} </script></span>. So we can define <span><script type="math/tex"> \delta_i = {\partial C \over \partial a_i} \odot {\sigma^{'}(z_i)} </script></span>, and rewrite these equations like this to avoid compute these parts many times:</p>

<div>
\begin{equation}
\delta_l = (a_l - y) \odot \sigma^{'}(z_l)
\end{equation}

\begin{equation}
\delta_i = \delta_{i+1} w_i^{'} \odot \sigma^{'}(z_i)
\end{equation}

\begin{equation}
{\partial C \over \partial w_i} = a_i^{'} \delta_{i+1}
\end{equation}

\begin{equation}
{\partial C \over \partial b_i} = \delta_{i+1}
\end{equation}
</div>

<p>With these equations, we can write the back propagation algorithm easily.</p>

</article>

<footer id="post_footer">
  <table><tr>
    
      <td id="prev"><a href="/2014-12-28-why-use-reflections-to-write-a-web-framework.html">Prev: Why Use Reflections to Write A Web Framework</a></td>
    
    
      <td id="next"><a href="/2015-04-26-how-about-translate-imap-and-smtp-into-http-api.html" id="next">Next: How About Translate IMAP And SMTP Into HTTP API?</a></td>
    
  </tr></table>
</footer>

<section id="comment">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'crazy-hot-ice'; // required: replace example with your forum shortname
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>
</div>

    </section>

    <footer id="page_footer">
      Copyright @ 2012 - 2015 Bin Wang
      <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US">Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License</a>.
    </footer>
    
    <!-- MathJax -->
    <script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
    </script>
  </body>
</html>
